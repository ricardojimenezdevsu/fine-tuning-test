Chapter 5. Deploying and Releasing APIs

In this chapter we will start to tie together how to move from design, build, and test to running in the target environment. Consider the conference system case study we introduced in the Introduction: we had a single-user interface and server-side application. Deploying an upgrade to the server or user interface would likely mean having some element of downtime. It is likely that the deployment and the release actions are tightly coupled and possibly inseparable. It may also have taken time to roll back the changes if an issue occurred with the deployment. We will explore some options for the legacy conference system, in addition to looking at how a looser coupling between the UI and server components provides more options for deployment and release. The introduction of traffic management provides you options to separate the deployment and release. In this chapter we will explore this in more detail and look at the conference system options available for rolling out changes. You will need to consider how API versioning impacts the options for modeling releases in the conference system. One key consideration for a rollout is to understand whether a change has been successful or not. API architectures are by nature decoupled, and it is important to ensure the right metrics, logs, and traces are available to perform a successful release. We will look at the types of considerations for metrics and how they help in releases and incident management/troubleshooting. Finally, we will touch on how eventual consistency impacts change and where gotchas can creep in at the application level. The introduction of additional layers of infrastructure, such as proxies, requires decisions around caching and header propagation. We will explore these considerations and why you might choose an opinionated platform.
Separating Deployment and Release It’s important for you to understand the difference between deployment and release to get the most out of this chapter. Deployment involves taking a feature all the way into production, because you now have a running process in your system. Although deployed, the new feature is not active or executed by interactions with the production system. There are different ways to achieve this separation and you will explore these shortly. The release involves activating the new feature in a controlled manner, allowing you to control the risk of introducing the new feature. Thoughtworks Technology Radar has a great explanation of the difference between deployment and release: Implementing Continuous Delivery continues to be a challenge for many organizations, and it remains important to highlight useful techniques such as decoupling deployment from release. We recommend strictly using the term Deployment when referring to the act of deploying a change to application components or infrastructure. The term Release should be used when a feature change is released to end users, with a business impact. Using techniques such as feature toggles and dark launches, we can deploy changes to production systems more frequently without releasing features. More-frequent deployments reduce the risk associated with change, while business stakeholders retain control over when features are released to end users. Thoughtworks Technology Radar 2016One advantage of moving toward an API-based architecture is that the decoupled nature enables teams to rapidly release change. To realize this benefit, it is important to consider that the mechanisms for ensuring the coupling between systems remains low and the risk of releases resulting in a failure is minimized.
Warning We have seen teams that move to an API-based architecture without separating deployment and release. This can work for highly coupled services, but quickly puts pressure and downtime on multiple services if releases have to be choreographed across many teams. We will explore further in this chapter how you can use API versioning and lifecycles to help prevent this.
Case Study: Feature Flagging To effectively consider how to separate deployment and release, we will start with the legacy conference system case study. This is a useful place to start as it will allow us to model an evolutionary architecture nicely, where we can control the rate of change to the new infrastructure we have presented so far in the book. Figure 5-1 shows how the the legacy system for attendees and the database will live side-by-side with the modernized API-based service. Using feature flags, the controller can now make a code-level decision about whether to execute the query against the internal or external API service.
Figure 5-1. Conference application container diagram for attendees and feature flags
Feature flags are typically hosted in a configuration store outside of the running application and allow code to be deployed with the feature off. Once the team (or product owner) is ready to enable the feature, they can toggle the feature on, which causes the application to execute a different branch of code. The granularity could be on a per-user level, or more coarse, as simply enabling a specific option globally. The following is example pseudocode from the popular Java feature-flagging tool LaunchDarkly, where the user’s details are in the modern store: LDUser user = new LDUser("jim@masteringapi.com"); boolean newAttendeesService = launchDarklyClient.boolVariation("user.enabled.modern", user, false); if (newAttendeesService) { // Retrieves the attendee from the modern store } else { // Retrieves the attendee from the legacy store } Following this approach would allow you to migrate a small batch of users over to the new system and test that the functionality continues to work for the set of users. If anything goes wrong as part of the migration, the switch can simply be toggled back, or else the rollout continues until reaching 100% migration. Like other services that cross-cut the application, a failure in the feature-flagging service would be catastrophic if not managed correctly and be a potential single point of failure. Degrading gracefully using the last known value in a cache or providing default values can help mitigate this effect.
Warning Feature flags help facilitate the separation between code deployment and release. You must clean up feature flags and always create feature flags with unique names. Once the migration is complete, the feature flag code should be removed completely. For an example of feature flags going wrong because of this issue, you only have to look at Knight Capital, where reusing a feature flag and a failed deployment ended up costing thousands of dollars per second, up to an eventual loss of $460 million.
Traffic Management One benefit of moving to an API-based architecture is that we can iterate quickly and deploy new changes to our Attendee service. We also have the concept of traffic and routing established for the modernized part of the architecture. This makes it possible to manipulate the traffic in two places: at the API gateway ingress or within the constructs defined within the service mesh for shaping traffic. For Kubernetes and service mesh–based systems, deployment looks roughly like the following steps: Create a pull request of the changes required to the application, and once the pull request is approved and merged, automatically kick off the deployment build.The build pipeline creates a new image using Docker or the Open Container Initiative.Push the new image to the container registry.Trigger a new deployment of the image into the target environment.
By default, Kubernetes will replace the running deployment with a new deployment. Later in this chapter we will look at techniques to phase in the release of a new pod to actively separate deployment and release. Once the deployment is in place, the job of deploying code is now complete, and a different set of instructions follow for the release configuration of the running system. The configuration may also have multiple stages, and this is a mechanism we can use to set up different release strategies. Before we dive into exploring how to structure the release for traffic management, it is worth exploring the types of releases that you can have within an API system.
Case Study: Modeling Releases in the Conference System In “Semantic Versioning” (semver), we discussed the idea of different version strategies associated with APIs. When considering releases, it can be helpful to couple the ideas in semver with an API Lifecycle.
API Lifecycle The API space is moving quickly, but one of the clearest representations of version lifecycle comes from the now archived PayPal API standards. An approach to modeling lifecycle is presented in Table 5-1. Table 5-1. API Lifecycle (adapted from PayPal API standards) Planned Exposing an API from a technology perspective is quite straightforward, however once it is exposed and live in production you have multiple API consumers that need to be managed. The planning stage is about advertising that you are building an API and gathering initial feedback on the design and shape of the API from consumers. This allows a discussion about the API and the scope, allowing any early design decisions to be included.Beta Involves releasing a version of our API for users to start to integrate with; however, this is generally for the purpose of feedback and improving the API. At this stage the producer reserves the right to break compatibility, because it is not a versioned API. This helps to get rapid feedback from consumers about the design of the API before settling on a structure. A round of feedback and changes enables the producer to avoid having many major versions at the start of the API’s lifetime.Live The API is now versioned and live in production. Any changes from this point onward would be versioned changes. There should only ever be one live API, which marks the most recent major/minor version combination. Whenever a new version is released, the current live API moves to deprecated.Deprecated When an API is deprecated, it is still available for use, but significant new development should not be carried out against it. When a minor version of a new API is released, an API will only be deprecated for a short time, until validation of the new API in production is complete. After the new version is successfully validated, a minor version moves to retired, as the new version is backward compatible and can handle the same features as the previous API. When a major version of the API is released, the older version becomes deprecated. It is likely that will be for weeks or months, as an opportunity must be given to consumers to migrate to the new version. There is likely going to be communication with the consumers, a migration guide, and tracking of metrics and usage of the deprecated API.Retired The API is retired from production and is no longer accessible.
The lifecycle helps the consumer fully understand what to expect from an API change. With semantic versioning combined with the API Lifecycle, the consumer only needs to be aware of the major version of the API. Minor and patch versions will be received without updates required on the consumer’s side and won’t break compatibility. By considering the API Lifecycle and what you can control through traffic management, you can start to look at the types of changes and consider the most appropriate way to release new versions of APIs.
Mapping Release Strategies to Lifecycle Major changes are the most impactful for API consumers. In order to use the new version of software, consumers must actively upgrade their software that interacts with the API. As defined in the lifecycle, this means that we need to simultaneously run a live and deprecated version of the API for a significant amount of time to allow consumers to upgrade and migrate. This allows the consumer to make an explicit choice as to when they upgrade. One way to do this is to add the version in the URL: GET /v1/attendees Adding in the version is practical and easily visible to the consumer. However, it is not part of the resource and in some groups is considered not RESTful. An alternative approach is to have a header describing the major version that will impact the routing at the ingress to the cluster: GET /attendees Version: v1 Minor changes are free from the constraints imposed by major changes. For these types of changes, it is possible to deploy a new minor version of the API without accepting production traffic and then use a release strategy to introduce the new version. This type of change would not require any code changes from the consumer. Patch changes follow a similar pattern, as they do not change the shape of the API specification at all. For this type of transparent release to be possible, it is worth considering adding extra controls into the build process to assure that breaking changes are not accidentally introduced. In “OpenAPI Specification and Versioning”, we took a look at using openapi-diff to highlight changes between specifications. In the event that the specification is not backward compatible, the build should fail and prevent a breaking change from entering the architecture without a conscious override in place. The majority of releases against an API will be minor changes or patch changes where loose coupling is a primary concern for the producer and consumer. If the consumer and producer are tightly coupled, owned by the same team, and always move together, API versioning and lifecycle will not be critical. In this situation it is important to consider a release strategy that allows the release of both components together and traffic is controlled at ingress. Typically blue-green models work well for this scenario, and we will review this further in “Release Strategies”.
ADR Guideline: Separating Release from Deployment with Traffic Management and Feature Flags The ADR Guideline in Table 5-2 is helpful when considering how to create an ADR to separate release from deployment. Table 5-2. ADR Guideline: Separating release from deployment with traffic management and feature flags guideline Decision How do you go about separating release from deployment?Discussion Points Is it possible to separate deployment and release in the existing systems that are live today? What is the degree of coupling between the consumer and producer in the system? Do you have a build pipeline where it is possible to enforce the loose coupling requirements of traffic managed APIs, ensuring that compatibility is tested?Recommendations Start by working on separating the deploy and release of existing software. This will help to enable an evolutionary architecture and a simplification of the existing system. Feature flags are a good way of creating this separation. If the company hasn’t used feature flagging before, be sure that you review recommended practices and avoid pitfalls associated with flags. Without careful consideration, feature flags have the potential to become a single point of failure. Review the type of coupling between APIs in the architecture and decide the correct release strategy for the situation.
In the next section we will explore the different types of release strategies available.
Release Strategies Once you have adequately separated deployment and release, you can now consider mechanisms for controlling the progressive release of features. It is important to choose a release strategy that allows you to reduce risk in production. Reduction in risk is achieved by performing a test or experiment with a small fraction of traffic and verifying the result. When the result is successful, the release to all traffic triggers. Certain strategies suit scenarios better than others and require varying degrees of additional services and infrastructure. Let’s explore a couple of options that are popular with API-based infrastructure.
Canary Releases A canary release1 introduces a new version of the software and flows a small percentage of the traffic to the canary. In Figure 5-2, the before stage shows the gateway, the legacy conference at version 1.0, and the Attendee service at v1.0. The concept of a traffic split between the legacy conference service and Attendee service is in place, which will be different depending on the target platform. At the deployment stage a new v1.1 of the Attendee service is deployed, and at release time we can start to flow some of the traffic toward the v1.1 service.
Figure 5-2. Deploying the Attendee service using a canary approach
In Kubernetes, traffic splitting can be achieved by introducing a new pod against a service; you will explore this idea further in “Case Study: Performing Rollouts with Argo Rollouts”. It’s quite difficult to control a small percentage—that is, if you would like 1%, you need to run 99 v1 pods and 1 v2 pod. For most situations this would be impractical. In service mesh and API gateways, traffic shifting makes it possible to gradually shift or migrate traffic from one version of a target service to another. For example, a new version, v1.1, of a service can be deployed alongside the original, v1.0. Traffic shifting enables you to canary test or canary release your new service by at first only routing a small percentage of user traffic, say 1%, to v1.1, and then over time shifting all of your traffic to the new service. This allows you to monitor the new service and look for technical problems, such as increased latency or error rates, and also look for a desired business impact, such as an increase in key performance indicators like customer conversion ratio or average shopping checkout value. Traffic splitting enables you to run A/B or multivariate tests by dividing traffic destined to a target service between multiple versions of the service. For example, you can split traffic 50/50 across your v1.0 and v1.1 of the target service and see which performs better over a specific period of time. As a service mesh is involved with all service-to-service communication, you can implement these release and experimentation techniques on any service within your application. For example, you could canary release a new version of the Session service that implements internal caching of an attendees conference session schedule. You would monitor for both business KPIs, such as how often a user views and interacts with their session schedule, and also operational SLIs, such as a decrease in CPU usages within the service.
Separating Deploy and Release: Canary All-the-Things With the rise in Progressive Delivery, and also advanced requirements within Continuous Delivery before this, having the ability to separate the deployment and release of a service (and corresponding API) is a powerful technique. The ability to canary release services or run A/B tests can provide a competitive advantage to your business in both mitigating risks of a bad release and also understanding your customer’s requirements more effectively. You will learn more about this in Chapter 9.Where appropriate, canary releases are an excellent option, as the percentage of traffic exposed to the canary is highly controlled. The trade-off is that the system must have good monitoring in place to be able to quickly identify an issue and roll back if necessary (which can be automated). Canaries have the added advantage that only a single new instance is spun up; in strategies like blue-green, a complete second stack of services is needed. This can save cost and the operational complexity of running two environments in parallel.
Traffic Mirroring In addition to using traffic splitting to run experiments, you can also use traffic mirroring to copy or duplicate traffic and send this to an additional location or series of locations. Frequently with traffic mirroring, the results of the duplicated requests are not returned to the calling service or end user. Instead, the responses are evaluated out-of-band for correctness, such as comparing the results generated by a refactored and existing service, or a selection of operational properties are observed as a new service version handles the request, such as response latency or CPU required. Using traffic mirroring enables you to “dark launch” or “dark release” services, where a user is kept in the dark about the new release but you can observe internally for the required effect. The main difference is the ability to mirror traffic, which during the experiment/release phase duplicates the request to the attendees v1.1 service. In Figure 5-3 the before and deployment stages are identical to the canary release; often dark deployment is referred to as a specialized canary.
Figure 5-3. Deploying the Attendee service using a traffic mirroring approach
Implementing traffic mirroring at the edge of systems has become increasingly popular over the years, and now a service mesh enables this to be implemented effectively and consistently across internal services. Continuing the example of releasing a new version of the Attendee service that implements internal caching, dark launching this service would allow you to assess the operational performance of the release but not the business impact.
Blue-Green Blue-green is usually implemented at a point in the architecture that uses a router, gateway, or load balancer, behind which sits a complete blue environment and a green environment. The current blue environment represents the current live environment, and the green environment represents the next version of the stack. The green environment is checked prior to switching to live traffic, and at go live the traffic is flipped over from blue to green. The blue environment is now “off,” but if a problem is spotted it is a quick rollback. The next change would go from green to blue, oscillating from first release onward. In Figure 5-4 the legacy conference service and Attendee service are both at v1.0 and represent our blue model. During deployment we are looking to deploy the legacy conference service v1.1 and attendees v1.1 together, creating a green environment. During the release step, the configuration is updated to target the gateway to point at the green environment.
Figure 5-4. Deploying the Attendee service using a blue-green approach
Blue-green works well due to its simplicity and for coupled services is one of the better deployment options. It is also easier to manage persisting services, though you still need to be careful in the event of a rollback. It also requires double the number of resources to be able to run cold in parallel to the current active environment.
Case Study: Performing Rollouts with Argo Rollouts The strategies discussed add a lot of value, but the rollout itself is a task that you would not want to have to manage manually. This is where a tool such as Argo Rollouts is valuable for demonstrating practically some of the concerns discussed. Using Argo, it is possible to define a Rollout CRD that represents the strategy you can take for rolling out a new canary of your v1.2 of the Attendee API. A Custom Resource Definition (CRD) allows Argo to extend the Kubernetes API to support rollout behavior. CRDs are a popular pattern with Kubernetes, and they allow the user to interact with one API with the extension to support different features. The Rollout CRD is a combination of the standard Kubernetes Deployment CRD with a strategy on how to roll out new features. In the following configuration YAML, we are running five pods of the Attendee API, and specifying a canary approach to rolling out the new feature. On triggering the rollout, 20% of the pods will be swapped for the new version. The {} syntax in the pause tells Argo to await confirmation from the user before proceeding: apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: attendees spec: replicas: 5 strategy: canary: steps: - setWeight: 20 - pause: {} - setWeight: 40 - pause: {duration: 10} - setWeight: 60 - pause: {duration: 10} - setWeight: 80 - pause: {duration: 10} revisionHistoryLimit: 2 selector: matchLabels: app: attendees-api template: metadata: labels: app: attendees-api spec: containers: - name: attendees image: jpgough/attendees:v1 After installing Argo to our cluster and applying the preceding configuration, the cluster will have five pods of the version 1 Attendee service running. A really nice feature of Argo is that the dashboard helps to clearly visualize the current status of the rollout. Figure 5-5 shows the starting point of the rollout, running five pods of the attendees v1 service.
Figure 5-5. Argo Rollouts starting point
Executing the following command introduces the v1.2 canary into the platform: kubectl argo rollouts set image attendees attendees=jpgough/attendees:v1.2 Figure 5-6 shows that the release is on the first step of the strategy with a 20% weight now set to the canary for attendees v1.2. As the UI demonstrates, the rollout is now in the Pause step, waiting for the manual promotion to be triggered to continue the rollout, either from the UI or command line. It is also possible to quickly roll back the canary release if an issue is encountered.
Figure 5-6. Argo Rollouts canary
In this simple example, you have explored the Kubernetes level only; however, it is possible to fully integrate with features of service mesh to control rollouts. It is also possible to integrate with ingress gateways such as NGINX and Ambassador to coordinate traffic management with the release. Tools like Argo make rollouts and traffic-based releases quite compelling. In addition to manual promotion steps explored in this walkthrough, it is also possible to drive promotion based on the analysis of metrics. The following is an example of an AnalysisTemplate that uses Prometheus metrics to observe the success rate of deploying a canary. This analysis stage can be represented in the Rollout definition, allowing the rollout to progress if the success criteria are met: apiVersion: argoproj.io/v1alpha1 kind: AnalysisTemplate metadata: name: success-rate spec: args: - name: service-name - name: prometheus-port value: 9090 metrics: - name: success-rate successCondition: result[0] >= 0.95 provider: prometheus: address: "http://prometheus.example.com:{{args.prometheus-port}}" Success rate is a fairly simple metric; however, there are situations where APIs fail that are not indicative of a fault in infrastructure but rather the client request. Let’s explore some of the key principles that are important from an API perspective that you can use to both operate the plant and also inform your rollout strategies.
Monitoring for Success and Identifying Failure Consider the legacy conference system case study in the Introduction and how you would go about investigating an issue in a single application. A single service has a single logfile to trace requests and processing by the application. There is only one application to look at for the overall health of the process on the server. Separating out multiple services, such as the Attendee service, results in an increase in operational complexity. The more hops between services introduces a potential for failure and manually finding what has gone wrong soon becomes difficult.
Three Pillars of Observability API-driven architectures are decoupled and, without appropriate support, are complex to reason about and troubleshoot. Observability provides transparency into your system, providing a full understanding of what is happening at all times. Observability is best described by the three pillars, an operational minimum required to reason about distributed architecture: Metrics are a measurement captured at regular intervals that represent an important element to the overall platform health. Metrics can be at different levels across the platform, and the freedom of structure means that a platform can determine what metrics are important to capture. For example, a Java platform may choose to capture CPU utilization, current heap size, and garbage collection pause time (to name a few).Logs are granular details of processing from a given component, and often the quality of logs is closely tied to the application or infrastructure component emitting them. Log format influences the utility of searching and processing of log data significantly, with structured logging facilitating a better search and retrieval of relevant data. In a distributed system, logs alone are usually not enough, and they are often better explored with the addition of context provided by traces and metrics.Traces are essential when moving to a distributed architecture, enabling the tracking of each request through all the components interacted with in the architecture. For example, if a request fails, tracing will enable you to quickly locate the exact component in the architecture that is failing. Tracing works by adding a unique header as close to the origination of the request as possible; this header is propagated in all subsequent processing of a given request. If the context of the request moves over to a different type of infrastructure (e.g., a queue), the unique header will be recorded in the message envelope.
You can find a more detailed introduction in Distributed Systems Observability (O’Reilly) by Cindy Sridharan.
Warning Implementing the three pillars across the platform is not enough. In “Reading the Signals”, we will cover how to make use of the three pillars of observability to operate infrastructure involved with an API platform.For the three pillars of observability, the OpenTelemetry project is the best place to start. The project provides an open standard in the Cloud Native Computing Foundation (CNCF), preventing vendor lock-in and facilitating the widest possible compatibility. Although metrics and tracing standards have been created and are stable, logging is a slightly more difficult problem to solve (due to the vast array of different possible emitters), but it is also covered in the OpenTelemetry project.
Important Metrics for APIs Considering which metrics are important for an API platform is a key decision that will help discover outages early, and possibly even prevent them. You could measure and gather a wide range of different metrics, but some metrics will also be dependent upon your platform. Rate, Error, Duration (RED) metrics are often noted as one approach to measuring traffic-based service architectures. Part of the appeal is that these metrics provide a good overview of what is going on at a point in time. Rate shows how many requests per second a service is processing (or throughput), what errors are returned, and the duration (or latency) of each request. In the Site Reliability Engineering (SRE) world, these metrics help us to derive The Four Golden Signals—latency, traffic, errors, and saturation. Perhaps one of the biggest drawbacks of RED/golden signals is that it is easy to apply the rules and miss out on the wider context (or understanding) of the system. For example, is every error from an API caused by a service in the request chain? For APIs the context of the error is really important—for example, a 5xx range error is important as it highlights a failure caused by an infrastructure component or service. A 4xx error isn’t a service problem and is more the problem of the client, but can you simply ignore this error code? A series of 403 Forbidden errors could indicate that a malicious actor is attempting to access data that they are not entitled to. This is one example of why context is critical, and time spent investigating what metrics are important takes API reasoning beyond RED metrics. Important metrics should be tied to alerting in order to ensure that you can swiftly deal with problems (or upcoming problems). You have to be careful when setting alerts to avoid false positives. For example, if an alert is generated on low or no activity this could be triggered on bank holidays or weekends. Only having this type of alert scheduled for core business hours could help, or perhaps tying it to the current number of website logins. In our conference system case study, the following would be considered important example metrics to capture: The number of requests per minute for attendees.The service-level objective (SLO) for attendees is average latency for responses. If the latency starts to significantly deviate, it could be the early signs of an issue.Number of 401s from the CFP system could indicate a vendor compromise or a stolen token.Measure of availability and uptime of the Attendee service.Memory and CPU usage of the applications.The total number of attendees in the system.
Reading the Signals So far we have discussed observability and why this is important, along with the purpose of each pillar. We have looked at some key metrics for APIs, but also added a caution that implementation alone or metrics without context is not enough. We mentioned the idea of capturing metrics from the running application such as garbage collection time. Increasing time spent garbage collecting might be an early symptom that your application is about to fail. Garbage collections typically spend time pausing applications, which in turn results in requests being delayed and can impact latency. Spotting this early is the equivalent of a car engine making a strange noise; it still works but something isn’t quite right. In order to read the signals, establishing an expectation or baseline is really helpful, and then measuring within this range and alerting when outside the range can help spot a problem. The next step would be observing the actual metric for API latency being impacted—the equivalent of the check engine light now showing. The sooner you can read the signals of a potential issue, the less the likelihood of there being a client-impacting problem. If both measures are ignored, the application eventually falls over, leaving the team scrambling to repair. Understanding the software and the link into key metrics helps lead to a mature operational platform with early identification and hopefully resolution of problems. In distributed architectures, failure is inevitable, and in an outage scenario, traces would be the first port of call to narrow down the root cause. We have seen it take hours to dig into the cause of a problem without tools like tracing, with developers turned detectives poring over logs trying to find clues as to “whodunnit.” Another key consideration is how quickly the team responds to various events. If the first time is when all API traffic is not working, it’s going to be stressful (and possibly business impacting).
Application Decisions for Effective Software Releases Distributed architectures introduce new challenges and considerations for releasing software and require changes at the application level. In this section you will explore some of the gotchas when releasing in a distributed architecture and how to resolve them.
Response Caching Response caching can be a real issue when it comes to application components in particular gateways and proxies. Consider the following scenario. We try to perform a canary release of the Attendee service, and everything looks to be going nicely, so we proceed with the rollout of all new services. However, the service calling GET /attendees was using a proxy, which now bounces, producing 500s everywhere. It turns out the cached result was masking the fact that our new software was broken. To avoid caching results it is important to set a header on the client making the GET request, i.e., Cache-Control: no-cache, no-store. Eventually the cache will expire and we achieve a consistent state.
Application-Level Header Propagation Any API services that terminate an API request and create a request to another service need to copy headers across from the terminated request to the new request. For example, any tracing- or observability-related headers need to be added on to the downstream request to ensure distributed tracing is observed. For authentication and authorization headers, it is important to have an opinion on what can safely be sent downstream. For example, forwarding an authentication header can end up with a service being able to impersonate another service or user, causing issues. An OAuth2 bearer token, however, is safe to send downstream (as long as the transport is secure).
Logging to Assist Debugging Things are going to go wrong in a distributed architecture! Often, being able to see that a request made it to a service is really valuable, especially if you don’t consider caching. It is useful to think of logs in two different types: journal and diagnostics. A journal allows the capturing of important transactions/events within the system and is used sparingly. An example of a journal event is the receipt of a new message to process and the outcome of that event. Diagnostics are more concerned with failures in the processing and any unexpected errors outside of a journal-based event. As part of the structured log, you can add a field to represent the log type, allowing quick access to either only journals or full diagnostics.
Considering an Opinionated Platform Often there is no conscious decision about what approach to take with the decisions we have covered in this section, which can lead to repeated work or inconsistent approaches. One option to solve this problem is to create a platform team and develop an opinionated platform. The opinionated platform would make key decisions on how to solve problems as part of the technical platform, avoiding the need for every developer to implement the same platform features. For opinionated platforms to be successful, they need to enhance the path to production, taking into account DevOps and other key factors required to operate on the platform. This is often referred to as the paved path or golden path to production. Creating a platform that development teams want to use and that makes solving business problems easier will have a far greater chance of adoption. It is important to remember that creating opinions creates constraints, so there is a trade-off between developer freedom and applications that work as expected within an organization.
ADR Guideline: Opinionated Platforms Choosing to create an opinionated platform is most successful when the developers of the platform are involved in the design process. In the guideline in Table 5-3, you will explore what points to consider and the importance of involving developers to create a successful opinionated platform. Table 5-3. ADR Guideline: Opinionated platforms Decision Should you adopt an opinionated platform for your deployments and releases?Discussion Points What are our languages for developing software in the organization? Is it possible to center around a few to live within the opinionated platform? Is the organization set up in a way where you can empower developers as customers and run the opinionated platform as an internal product? What are the constraints or features that are going to add benefit to introducing a platform? For example, should monitoring and observability be features supplied out of the box to developers? How do you update the platform recommendations and help provide changes to teams already using the platform?Recommendations Consider developers as customers of the platform product and create a mechanism that supports developers providing input. The key features should be as transparent as possible to developers (e.g., configures a library to introduce open telemetry). New applications always get the latest features in the stack. However, how do you ensure that existing platform users can easily get access to the latest features?
Summary In this chapter we have provided an introduction to deploying and distributing software in an API architecture: A valuable starting point is to understand the importance of separating deployment and release. In existing applications, feature flagging is one approach to configuring and enabling new features at a code level.Traffic management provides a new opportunity to use the routing of traffic to model releases.Major, minor, and patch releases help to separate the style of release options. Applications that have a tightly coupled API may use a different strategy.You have reviewed the release strategies and the situations in which they apply, and you saw how tools like Argo can help to facilitate rollouts effectively.Monitoring and metrics are an important measure of success in an API platform. You have reviewed why some metrics can be gotchas and could suggest a problem where there isn’t one. You have learned a primer to observability and why applying these technologies is critical to successfully operating an API platform.Finally, you explored application decisions to support effective rollouts and what platform owners may wish to consider when aiming for consistency across the plant.
Deploying and releasing APIs effectively is critical to a successful API-driven architecture. However, it is important to think about security threats API systems will face and consider how to effectively mitigate the risk. This is the focus of Chapter 6.
1 Named after canaries that went into the coal mines first to fatally identify the presence of any dangerous gases. 