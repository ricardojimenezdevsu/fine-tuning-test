Chapter 8. Redesigning Applications to API-Driven Architectures

Now that you have a solid grasp of API operations and security, you will explore how APIs can be used to evolve and augment existing applications. In Building Evolutionary Architectures (O’Reilly), the authors discuss how an evolutionary architecture supports guided, incremental changes, across multiple dimensions. Whether you want to adopt an evolutionary architecture as defined in that book or not, the reality is that almost every successful system will have to evolve over time to meet new user requirements or to react to a changing environment. It is rare for a business or organization not to change its products based on customer feedback or changing market conditions. Equally, it is uncommon for a long-running system not to be impacted by changes in infrastructure (e.g., hardware failing and becoming obsolete), the underlying application frameworks, or a third-party service. APIs are the natural interfaces, abstractions, and (encapsulated) entry points to and within a system, and as such can be instrumental in supporting an evolutionary architecture. In this chapter you will learn about why change is needed, how to design for this, and where to implement useful patterns. Although you may not have realized it, you have been applying many of the skills discussed in this chapter throughout the conference system case study. We recommend that you think about the evolution of the case study as you read this chapter, and you will review the final end state of the conference system architecture in “Case Study: A Look Back on Your Journey”.
Why Use APIs to Evolve a System? Changing software in a safe manner can be difficult. This challenge is further compounded if the software has any of these three characteristics: a large number of users, inherent complexity in the design, or tight integration with a number of other systems. And here’s the kicker: these characteristics are almost inevitable within a software system that is well adopted, meets user needs, and is a key part of an organization’s workflow. The majority of “legacy” systems have also evolved in a somewhat ad hoc manner, with many temporary workarounds, quick fixes, or shortcuts becoming an embedded part of the system design. As an architect, APIs can help you evolve a system. An API can be a boundary to a module or component, and this makes an API a natural point of leverage when trying to ensure a system is highly cohesive and loosely coupled.
Creating Useful Abstractions: Increasing Cohesion Cohesion refers to the degree to which the elements inside a system belong together. Implementing APIs and systems with high cohesion enables the easier evolution of both the API provider and consumer. As a provider, you can alter the internals of your service, such as changing algorithms, refactoring code to improve performance, or changing datastores, and you only have to avoid modifying the external interface in a way that breaks backward compatibility. As a consumer, you can be more confident in modifying and scaling your service, with clear and understandable integration points into the existing API. Closely related to designing cohesive APIs is thinking critically about the abstractions you are creating. We can all understand and appreciate the differences in abstractions for controlling different vehicles. With a car, you typically interact with the dashboard, pedals, and steering wheel. When operating a space shuttle, the control panel contains many more fine-grained dashboards, control sticks, and buttons. The space shuttle controls are cohesive to the task at hand, but the level of control and complexity offered here would not be appropriate when designing a car. Hopefully, you can see the analogy with API design. It can be tempting to design the equivalent of a space shuttle control panel—particularly in relation to the goal of “future-proofing” APIs—when in reality, your underlying business service is analogous to driving a car.
Aim for APIs with High Cohesion APIs that are highly cohesive are easier for an architect to understand, build mental models of, and reason about. Cohesive APIs also don’t violate the principle of least surprise. Highly cohesive APIs can also become focused points of change within a system. For example, a series of related changes may only require the modification of a single API, versus the modification of a series of APIs required when modifying a system with low cohesion. Always strive for and evolve toward highly cohesive systems.As a counterexample of high cohesion within the case study, imagine if you created a “utils” API that exposed a collection of convenience functions that could be used across all of the conference entities. This could easily lead to a situation where a change in the code behind one API, such as the Attendee API, requires that another utility API also be updated. Unless you are the original author of the APIs, or you have very good documentation and tests, it could be easy to miss this and leave the system with inconsistent or incompatible behavior.
There Are Many Types of Cohesion to Consider! Although cohesion often gets talked about as if this could be measured in one dimension, there are several types of cohesion that architects should be aware of. For example, systems can be coupled in a number of ways: Functional cohesionSequential cohesionCommunicational cohesionProcedural cohesionTemporal cohesionLogical cohesionCoincidental cohesion
Joseph Ingeno’s Software Architect’s Handbook (Packt Publishing) provides a more comprehensive overview for readers who want to know more.
Of course, cohesion is only one property to strive for when designing and evolving systems. Let’s now look at cohesion’s close relation in software: coupling.
Clarifying Domain Boundaries: Promoting Loose Coupling A loosely coupled system has two properties. First, components are weakly associated (have breakable relationships) with each other, which means that changes in one component do not affect the functionality or performance of another component. And second, each of the system’s components has little or no knowledge of the definitions of other separate components. Components in a loosely coupled system can be replaced with alternative implementations that provide the same services and are less constrained to the same platform, language, operating system, or build environment. Designing or refactoring toward a loosely coupled API will enable providers and consumers to evolve their systems more effectively. As a provider, a loosely coupled API will enable the maximum adoption of your service across the organization, both from the perspectives of ease of integration and supporting ease of change. And for consumers, an API that is loosely coupled will support easier swapping of components (potentially even at runtime), allow easier testing, and reduce the cost of managing dependencies.
Loose Coupling Enables Easier Mocking and Virtualizing When Testing! An API designed with loose coupling in mind will typically be much easier to mock or virtualize when performing integration and end-to-end testing. A loosely coupled API enables the provider implementation to be easily swapped. When testing a consumer, the provider API implementation can be swapped for a simple stub or virtual service that returns the required response. In comparison, it is often not possible to mock or stub an API that is highly coupled. Instead you will find yourself running the API provider as part of your test set or attempting to use a lightweight (less functional) or embedded version of the service.
Case Study: Establishing Attendee Domain Boundaries As an example with the conference system use case, imagine if our Attendee service was highly coupled with the underlying datastore and exposed data in the format of the underlying data schema. If, as the service provider, you wanted to swap out the datastore to something different, you have two choices. You can implement a new system to adapt any data created or retrieved between the old and new format, which will likely require complicated and error-prone translation code. Or, you can modify your external API and get all of your consumers to adopt this—and don’t underestimate the difficulty in doing this with a widely adopted service!
The Power of Information Hiding When you design an API to be both highly cohesive and loosely coupled, you benefit from the principle of information hiding. This is the principle of segregation of implementation decisions that are most likely to change. If you get this right, you can protect other parts of the system from extensive modification if the design decision is changed. The protection involves providing a stable interface that protects the remainder of the system from the underlying (changeable) implementation. In regards to APIs, information hiding is the ability to prevent certain aspects of a provider being accessible to its consumers This can be achieved by using only business- or domain-focused API endpoints and by not leaking any of the internal abstractions or the implementation-specific data model or schema.
End State Architecture Options As you evolve and redesign your monolithic applications and APIs, you should have a clear vision of what you want your system to be able to do as a result of the changes being made. Otherwise, this now infamous scene in Alice in Wonderland will become all too true: “Would you tell me, please, which way I ought to go from here?” “That depends a good deal on where you want to get to,” said the Cat. “I don’t much care where—” said Alice. “Then it doesn’t matter which way you go,” said the Cat. “—so long as I get SOMEWHERE,” Alice added as an explanation. You will learn more about the approach to determining your overall goals for evolving systems in the next section of this chapter, but for the moment let’s take a tour of the potential options for your architecture and how they impact API design.
Monolith Over the last several years, the monolithic architectural style has gotten a bad rap. However, this is mostly because the word “monolith” has become synonymous with “big ball of mud.”1 In reality, a monolith is just a software system that is composed all in one piece and runs as a single-process, self-contained application. There is nothing fundamentally wrong with a monolithic architecture. For many systems, particularly proof of concept applications, or systems that are being created as the underlying business product market fit is being found, this architectural style will allow you to move the fastest at the beginning of the project. This is because it is easy to understand and modify as there is only one thing to look at, reason about, and work on. The challenge when implementing APIs within a monolithic application is that it is easier to accidentally create a highly coupled design, which will only become apparent when you are making modifications in the future. Following best practices, such as using domain-driven design (DDD) and potentially using a hexagonal architecture, will pay dividends later.
Service-Oriented Architecture (SOA) Service-oriented architecture (SOA) is a style of software design where services are provided to the other components by applications or services that communicate over a network. The first use of SOA, often referred to as “classic SOA,” also has somewhat of a bad rap. This is mostly due to the use of heavyweight technologies with early SOA, such as SOAP, WSDL, and XML, and vendor-driven middleware such as ESBs and message queues. There was a focus on the network using “smart pipes” for communication, and business logic was incorporated into middleware. Evolving your applications toward SOA can be beneficial, but care should be taken to avoid the use of frameworks or vendor middleware that promotes high coupling or low cohesions. For example, always avoid adding business logic to an API gateway or enterprise service bus (ESB). One of the biggest challenges with designing SOA-based systems is getting the size and ownership of services “correct”—i.e., striking a good balance with the cohesiveness of the API, having clear ownership of the code across the organization, and the design and runtime cost of having many services.
Microservices Microservices are the latest implementation of SOA, where software is composed of small independent services that communicate over well-defined APIs. There are several differences with classic SOA—i.e., the use of “smart endpoints and dumb pipes” and avoiding the use of heavyweight middleware that can become highly coupled to your services. Many books have been written about microservices,2 and so we will refer you to these if you are looking to dive deep into the topic. However, the core principles with evolving microservices include creating loosely coupled and highly cohesive API-driven services. As with classic SOA, one of the biggest challenges when designing APIs using a microservices architecture is getting the boundaries (and cohesiveness) of an API and the underlying services correct. Using techniques like context mapping and event storming from the world of DDD, before building or evolving towards microservices, will often greatly reward your future efforts. Microservice APIs should ideally use lightweight technologies that encourage loose coupling. This includes technologies that you have already explored within this book, such as REST, gRPC, and lightweight event-driven or message-based technologies like AMQP, STOMP, or WebSockets.
Functions Although an initial promise of functions being the next evolution of microservices hasn’t quite come to fruition, there is good adoption of this architecture across a range of organizations. This architectural style can be a useful target to aim for if you have a highly event-driven system—for example, a market-based trading system that is highly reactive to news and market events, or an image-processing system with a pipeline of standardized transformations to apply and reports to be generated. The biggest challenge with designing a function-based system and the corresponding APIs is typically related to getting coupling correct. It’s all too easy to design functions or services that are so simplistic that many of them have to be orchestrated together to provide any business value. These services and their APIs tend then to become highly coupled. The balance here between reusability and maintainability can be difficult, and therefore this architecture style should not be chosen without recognizing that you and your team may take some time to adjust to it.
Managing the Evolutionary Process Evolving a system must be a consciously managed activity. Let’s look at the things you need to keep an eye on when making changes to your API.
Determine Your Goals Before attempting to evolve a system, you should be clear about the motivation behind the changes. The goals should be catalogued and clearly communicated with your team and organization. Identifying incorrect assumptions and targets early in the change process is less costly than at the point where coding begins. The goals largely fall into two categories: functional and cross-functional. Functional evolutionary goals are feature or functionality change requests. They are typically driven by end users or business stakeholders. There may be refactoring required, but these types of goals focus on writing more code or integrating more systems. Cross-functional goals, also referred to as nonfunctional goals, focus on the “ilities,” such as maintainability, scalability, and reliability. For example, maintainability changes are often driven by the technical leadership team wanting to reduce the time taken by engineers on understanding, fixing, or changing a system. Scalability changes are often driven by business stakeholders that are forecasting increased usage or more demand for the system. Reliability work often focuses on attempting to reduce the number and impact of failures within a system. These types of goals typically focus on refactoring existing systems or introducing new platform or infrastructure components. Creating cross-functional requirements makes total sense, but how do we set clear goals around the changes we are looking to make to our system, and how will we know if we’ve succeeded? This is where fitness functions can help.
Using Fitness Functions Aiming for an architecture that avoids rapidly becoming legacy requires active decisions that prevent degradation over time. Defining fitness functions is a mechanism that provides a constant interrogation of the system architecture and code artifacts that make up the system. Think of a function as a sort of unit/integration test for architecture, assessing the “ilities” of the architecture in a quantifiable metric. A fitness function is included into the build pipeline to help provide a constant assurance of the goals for the system. In the Thoughtworks blog on fitness functions, several categories of focus are suggested, as outlined in Table 8-1. Table 8-1. Categories of fitness functions Code Quality This is a fitness function category that many teams likely already have in place to a certain extent. Executing tests allow you to measure the quality of code ahead of releasing into production. Additional metrics are also worth considering—for example, ensuring that cyclic complexity is minimized.Resiliency An initial test for resiliency is to deploy a system into a preproduction environment, run sample (or synthetic) traffic into this, and observe that the error rate is below a certain threshold. An API gateway or service mesh can often be used to inject faults into the system and facilitate the testing of resiliency and availability to certain scenarios.Observability Ensuring that services conform (and do not regress) and publishing the types of metrics that are required by the observability platform is critical. In “Important Metrics for APIs”, you reviewed what would be a good set of API metrics to publish; this could be measured and enforced by an ongoing fitness function.Performance Performance tests are often an afterthought; however, if you can set out latency and throughput targets, these can be measured in the build pipeline. Perhaps one of the hardest parts to this objective is getting production-like data to make the type of performance tests you’d need to run be meaningful. We will consider this further in “Performance Issues”.Compliance This section is very business/organization-specific in terms of assessing what is critical to monitor. It could include audit or data requirements that are key to continuing to provide evidence that a business is running as expected.Security Security has many different aspects to it and you explored some of the considerations in Chapters 6 and 7. One possible fitness function might be to analyze the library dependencies in the project and check for any known vulnerabilities. Another could be to run an automated scan over the codebase to ensure that there are no OWASP-style vulnerabilities present.Operability Many applications are built, put into production, and then start to evolve; users onboard and then problems start. Deciding on a minimum set of requirements for operating the platform is key to ensuring the plant remains operational. Assessing whether monitoring and alerting are in place is a good place to start.
Creating ADRs around the fitness functions that you would like to introduce is a good place to start. It might be tricky to immediately implement everything presented in the preceding table. Some decisions are difficult to reverse and it is important that, where possible, these types of decisions are identified.
Note An irreversible decision is not a bad thing! However, an irreversible decision without careful thought and consideration is. An ADR helps with the common “What were they thinking…​?” style question and shares historical context. Decisions made collectively through the use of ADRs and open discussion will lead to an architecture that has longevity.
Decomposing a System into Modules Have you ever worked on a codebase that is bashed for being a monolith (perhaps by yourself or others)? One of the authors has worked on a four-million-line codebase that had 24 years of technical debt (according to SonarQube). The code structure connected ad hoc from many different classes, creating a high degree of uncontrolled coupling across the application. Refactoring any part of the application was difficult—often fixing one bug created other unexpected bugs. None of the problems were due to the monolithic nature of the system but instead were due to a lack of code organization and design. One approach to prevent the spaghetti code/ball of mud is breaking up software applications using modules. Designing modular components within a codebase helps to define clear boundaries and logical grouping based on cohesion of functionality. Modules aim to form well-defined boundaries that hide implementation details. Where you set the definition of the module can be a complicated issue; in languages such as Java, there are options such as method, class, package, and module. Each of those constructs allows a different degree of information hiding that overlaps with object-oriented encapsulation constructs. For the purpose of our discussion, we will consider a module to define an architectural partitioning on a larger scale than methods and individual classes. Sam Newman has some excellent advice on limiting what you expose to start with when designing modules. Sam’s book Monolith to Microservices (O’Reilly) is a fantastic deep-dive into the subject of modularity and migrations from monolith to microservices: Personally, I adopt the approach of exposing as little as possible from a module (or microservice) boundary. Once something becomes part of a module interface, it’s hard to walk that back. But if you hide it now, you can always decide to share it later. Let’s consider what modules we could potentially have introduced in the conference system case study that we started with in the Introduction. Figure 8-1 introduces a module to represent the controllers, services, and data access object (DAO) patterns. Each controller exposes the RESTful endpoints and will be exposed by the web server hosting the application. The service module is where the business logic lives behind the controller, exposing a clear interface to the controllers. The DAO module is where the data access object lives behind the services, exposing clear interfaces to the services. Module layers are quite common, and getting to the point where there is a clear single directional dependency between modules is a good application of modularity.
Figure 8-1. Proposal for module breakdown in the conference case study
Now that we have clear separation, each module can apply a strategy for testing the subject in isolation. Another advantage of a modular approach is the ability for developers to reason about and test within a module. In a recent project, one of the authors created a DAO pattern for interacting with the database as a module of the application. An interface exposed the functionality to the other modules of the application, making interactions with the module clear. A decision was taken later to split the business logic into three modules that utilized the DAOs into independent services—a monolith first evolution. The three new modules nicely separated into their own services, using the DAO module as a library. Designing well-defined modules enabled independent evolution and evolutionary decisions to be made about the system. Using C4 diagrams to express software is a lightweight approach at the component level for defining relationships between components in the system. The component diagram, first discussed in the Introduction, helps provide a mechanism for reviewing relationships and helps in defining modular structures. Defining modules within your applications is a good design step, though there are many options for how modularity is achieved. Look to use language-level support to help enforce modules and agree as a team on the approach that will work best for your technical stack.
Creating APIs as “Seams” for Extension The concept of “seams” was first introduced in Michael Feather’s 2004 book Working Effectively with Legacy Code (Pearson). A seam is a point where functionality is stitched together—it can be considered as the point where one subject under consideration3 interacts with another. This is usually achieved by techniques like dependency injection, injecting the collaborator and executing against an interface allowing for substitutability. The substitutability consideration is important; this allows for effective testing without the requirement to run the whole system (e.g., using mocks or test doubles). If the application was built without a good design, the definition of a seam may be complex and make it hard to understand the full range of behavior. When working with legacy code, it can make it difficult to break apart and refactor code to work in a more modular way. Nicolas Carlo presents a useful recipe for breaking apart seams of legacy code, assuming that tests do not already exist: Identify change points (seams)Break dependenciesWrite the testsMake your changesRefactor
When designing changes, consider creating an API design for how two (or potentially more) collaborators connect together. If there is potential that the definition of the seam could be used outside of the subject under consideration, an interservice API could be a great choice. For example, if the seam is a similar execution across many different parts of the codebase, and the goal is to break up a service into smaller service-based architectures, this is an opportunity to define cross-service reuse.
Identifying Change Leverage Points within a System Sometimes it is easy for architects or developers to identify “change leverage points,” or code and services that are obvious candidates to refactor and change in order to make a system “better” in some way—for example, more performant, extensible, secure. If you’ve worked in the industry for more than a few years, I’m sure you’ve worked on a system with a particularly challenging area of the codebase, or a module that is constantly changing and churning (and often the two issues are correlated!), and you’ve thought to yourself that you would like to spend time properly addressing the issue. However, these leverage points are not always obvious, particularly if you have inherited a codebase or system. For this situation, books such as Adam Tornhill’s Your Code as a Crime Scene (Pragmatic Bookshelf) will prove useful for understanding your code and applications. Related tooling can be very useful, such as version control system churn detection utilities that locate constantly changing parts of a codebase, or software complexity measuring tooling that analyzes a codebase or service with each build pipeline run.
Continuous Delivery and Verification In Chapter 5 you reviewed the importance of automating the deployment and releases of loosely coupled systems. The need to continually verify systems as we deploy more is critical to enabling an evolutionary architecture.
Architectural Patterns for Evolving Systems with APIs APIs provide a powerful abstraction for evolving systems toward a modern architecture and also to introduce new features and change. As you discovered in Chapters 3 and 4, gateways and service mesh–based constructs allow an operational migration using gateways. When creating an evolutionary change, a primary consideration during the evolutionary period is to mitigate the risks of evolution and maximize the benefits as soon as possible. Let’s review some architectural patterns that can assist with migrating to APIs.
Strangler Fig A strangler fig is any of the numerous species of tropical figs that grows around an existing tree. Although a strangler fig often smothers and outcompetes its host, there is some evidence that trees encased in strangler figs are more likely to survive tropical storms, suggesting that the relationship can be somewhat mutualistic. Ultimately this is the goal of an evolutionary architecture—to support a changing system, which may result in completely removing what was there before. This is achieved by introducing new components of the application while the old mechanism is still in place. The goal is to gradually migrate over to the new API-based approach. In “Case Study: Feature Flagging”, you reviewed how a feature flag could be used to query the legacy service or invoke a new API-based service. Figure 8-2 shows a C4 diagram of the use of feature flags. This works well for seams that previously existed as in-process interactions for the introduction of new APIs into a service. However, if there are many consumers already interacting with the service out of process it would be unrealistic to expect all to implement and control a feature flag.
Figure 8-2. Conference application container diagram for attendees and feature flags
Another model is to use a proxy or gateway to front the API interaction, routing to the legacy implementation or to the new implementation. This is a type of facade using a proxy, meaning that the API consumer uses the same API and is unaware of the migration from one service to another that is happening behind the scenes. Managing the strangler fig pattern behind the scenes can be tricky, and the introduction of a new component can be a single point of failure or bottleneck unless this is mitigated. The proxy should not take on business logic or this will make it difficult to remove at the end of the migration. Managing the legacy and modern process side-by-side is a challenge to ensure data coherency between the two services. You can find more guidance for overcoming these challenges in Monolith to Microservices.
Facade and Adapter Facade and adapter patterns are well-known patterns that can assist with migrating to modern services. The strangler fig pattern is a type of facade, intercepting API calls and hiding the complexity behind the scenes. A common situation we have encountered is present in existing large-scale distributed applications already using a form of API. Perhaps interservice communication is driven over SOAP-RCP or another vintage protocol. Adapters can help evolve the architecture by introducing a component that converts a given SOAP request into a new RESTful API call. However, protocol rewriting can be challenging to implement correctly. Care should be taken here to avoid reducing cohesion or introducing coupling. It’s not just legacy situations that can benefit from the use of the adapter pattern. In Chapter 1 we explored the use of gRPC as a technology that is popular and effective for east–west communication. Using the grpc-gateway project, it is possible to present a RESTful JSON endpoint that is converted to the gRPC representation in the background. Facade and Adapter patterns are very similar, in the sense that they get in the way. A facade is usually less complex than an adapter; a traditional API gateway simply routes API requests, whereas an adapter will be responsible for converting into a representation understood by the target application.
Warning If an API gateway steps over the line from acting as a facade pattern to the adapter pattern, the coupling immediately increases. Be sure to ask if you are still using the right component for the task!
API Layer Cake An API migration pattern that is talked about a lot within enterprise contexts is the “Layered APIs” or “API Layer Cake,” which builds upon the “separation of concerns” layering pattern seen within traditional enterprise monolithic applications. During the 2000s, it was considered a best practice in Java or .NET enterprise applications to implement application functionality in a series of layered tiers—for example, presentation, application, domain, and datastore tiers. The core idea was that each user request entering an application flowed sequentially down and then up of each tier/layer. This pattern allowed for the abstraction and reuse of functionality specific to each tier, with the trade-off being that an end-to-end slice of functionality often required modifying many layers—i.e., cohesion within each tier’s layer came at the expense of high coupling of tiers to provide a unit of business functionality. The modern API-based approach to this pattern is seen in Gartner’s Pace-Layered Application Strategy.4 New names are used for each API or microservice layer, with presentation being translated approximately to systems of engagement (SoE), application to systems of differentiation (SoD), and datastore to systems of record (SoR). Over time, this pattern has earned a bad reputation, particularly as the legacy systems that implemented it became more and more challenging to evolve. This pattern encourages architects and developers to take shortcuts, like duplicating functionality between many layers to avoid calling an additional layer, or circumventing layers when dealing with requests, such as the presentation tier directly communicating with the datastore tier. We generally recommend avoiding the use of this pattern.
Identifying Pain Points and Opportunities It is often tempting to avoid working on parts of a system or codebase that have a bad reputation, whether this is for poor code quality, high complexity, or just frequent failure. Some pain points are not always obvious until you have severe outages to resolve. However, never let a good crisis go to waste—identification and cataloguing problematic components within the system can help to track and improve known issues. Let’s explore some of the common issues that occur within a distributed API-based system and how to approach this as an opportunity for change.
Upgrade and Maintenance Issues Identification of where upgrades and reported bugs occur in the overall system can help create a “hit list.” Watch for the following issues that occur within the system: High change fail percentage for a specific subsystemHigh volume of support issues raised for a systemLarge amounts churn of a specific part of the system or codebaseHigh complexity (identified via static analysis and cyclomatic complexity)Low level of confidence provided from development teams when asked about the ease of a required change
Consider adding code quality metrics as this can provide a rough guide to potential underlying issues. A maintenance issue or subsystem issue could be a good opportunity to introduce an API abstraction to pull functionality out and use strangler fig to drive improvement. It might also be a code smell that good coding principles are not followed. In Chapter 9 we will also consider how we approach applications as we migrate toward API-based architectures on new infrastructure.
Performance Issues Service-level agreements (SLAs) are an excellent upper bound to track and monitor performance against. In Chapter 5 you reviewed monitoring and metrics that help to signal an issue with an API service. The reality is that many applications do not build in proactive protections against the introduction of problems. If the first time the team hears about a performance problem is from direct customer feedback or from production monitoring—e.g., an edge system that has exhausted a user request-response latency budget—the team is immediately at a disadvantage when reacting to the problem. Performance issues can be architectural; for example, do you have a service calling a service located in a different region or across the internet? When it comes to performance issues, measuring and creating an objective plan is critical. Take a measurement of the existing system, create a hypothesis on where performance could be improved, and then test and verify. It is important to consider the system as a whole, rather than attempting to optimize a specific component in isolation. Automating the measurement process allows you to introduce the performance measurement as part of the build process.
Breaking Dependencies: Highly Coupled APIs Moving toward a distributed architecture is only going to pay dividends if parts of the architecture can evolve independently. A particular antipattern to watch out for is the synchronized coordination of releases across different parts of the system. This is a sign that APIs are potentially highly coupled, and breaking this could be an opportunity to further reuse and reduce release friction. One skill that is often overlooked in training courses and in teams is working effectively with legacy code. Often developers are unsure about how to introduce the types of changes that help to break up dependencies. There are two techniques that are useful to consider when looking to break dependencies. The sprout technique is covered in Michael Feather’s 2004 Working Effectively with Legacy Code. It is often very difficult to unit test code that wasn’t built with testing in mind. Sprout involves creating the new functionality elsewhere, testing it and adding it to a legacy method known as the insertion point. Another technique is to wrap the existing functionality by creating a new method with the same name and signature as the old method. The old method is renamed and called from the new method, with any additional logic before the legacy method is called. If a service is predominantly legacy, working with legacy code is a critical skill to develop. Working on coding katas or pair programming on complex areas of the codebase will help promote understanding across the team. Sandro Mancuso created an excellent video on YouTube that several of us have used to understand practical approaches to working with legacy code.
Summary In this chapter you have learned how to use an API-driven approach to evolve vintage monolithic applications toward a service-based architecture: APIs often provide a natural abstraction or “seam” within systems, supporting decomposition of services and facades to support gradual change. As such, they are a powerful force in any architect’s toolbox when evolving a system.Key concepts to understand when evolving a system using APIs include coupling and cohesion. Designing and building systems with these universal architectural concepts in mind will make evolution, testing, and deploying of systems easier and safer.When evolving a system, you should always be clear about your current goals and constraints. Without clearly establishing and sharing these, migrations can become open-ended, which drains resources while providing little value and can affect morale.Well-established patterns, such as the strangler fig, can increase the speed and safety of evolving a system and prevent the need to reinvent the wheel.
The next chapter builds on the focus of this chapter and extends the scope of evolutionary architecture to also include migrating to cloud infrastructure.
1 For more insight and the history of the big ball of mud, check out these articles from Wikipedia and InfoQ. 2 Sam Newman’s Building Microservices, 2nd Edition (O’Reilly) is our personal favorite. 3 In this context “subject under consideration” is a class or set of classes. 4 For more information on Pace, see “Accelerating Innovation by Adopting a Pace-Layered Application Strategy” by Gartner and “A Pace-Layered Integration Architecture” by Dan Toomey. 