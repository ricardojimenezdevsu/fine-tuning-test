{"prompt": "Context: Mastering API Architecture   Design, Operate, and Evolve API-Based Systems   James Gough, Daniel Bryant, and Matthew Auburn      Mastering API Architecture   by James Gough, Daniel Bryant, and Matthew Auburn  Copyright \u00a9 2023 James Gough Ltd, Big Picture Tech Ltd, and Matthew Auburn Ltd. All rights reserved.  Printed in the United States of America.  Published by O\u2019Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.  O\u2019Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.          Acquisitions Editor: Melissa Duffield          Development Editor: Virginia Wilson          Production Editor: Clare Laylock          Copyeditor: Kim Cofer          Proofreader: Amnet Systems LLC          Indexer: nSight, Inc.          Interior Designer: David Futato          Cover Designer: Karen Montgomery          Illustrator: Kate Dullea            October 2022: First Edition      Revision History for the First Edition           2022-10-17: First Release      See http://oreilly.com/catalog/errata.csp?isbn=9781492090632 for release details.  The O\u2019Reilly logo is a registered trademark of O\u2019Reilly Media, Inc. Mastering API Architecture, the cover image, and related trade dress are trademarks of O\u2019Reilly Media, Inc.  While the publisher and the authors have used\n\nQuestions:", "completion": "Q1. What is the publication date of the first edition of the book Mastering API Architecture?\n\nA1. October 2022 (Wrong Answers: 2020, 2021, 2023)\n\nQ2. How many authors are listed on the book Mastering API Architecture?\n\nA2. Three (Wrong Answers: Two, Four, Five)"}
{"prompt": "Context: good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.    978-1-492-09063-2  [LSI]      Dedication   This book is dedicated to Alex Blewitt, who unfortunately passed away before publication. We would like to thank Alex for his candid feedback, constant support, and warm friendship over the years.  \u2014The Authors      Foreword   When I built my first APIs at the Financial Times, over a decade ago, there weren\u2019t too many of them. We were building on a monolithic architecture, and the APIs were solely there for external third parties to get access to our content.  Now though, APIs are everywhere and they are core to your success when building a system.  That\u2019s because, over the last decade, a couple of things have combined to change the way many of us do software development.  Firstly, the technology available for us changed. The rise of cloud computing gave us self-service, on-demand provisioning. Automated\n\nQuestions:", "completion": "Q1. What is the purpose of APIs? \nA1. The purpose of APIs is to enable communication between two applications. \nWrong Answers: \n1. To provide access to content. \n2. To provide a monolithic architecture.\n3. To provide self-service, on-demand provisioning. \n\nQ2. What technology enabled the rise of cloud computing?\nA2. Automation enabled the rise of cloud computing. \nWrong Answers: \n1. Manual provisioning. \n2. On-demand provisioning. \n3. Self-service provisioning. \n\nQ3. What is the goal of software development? \nA3. The goal of software development is to create applications that are efficient, effective, and user friendly. \nWrong Answers: \n1. To create applications that are complex and difficult to use. \n2. To build a monolithic architecture. \n3. To provide access to content. \n\nQ4. What is the main focus of this book? \nA4. The main focus of this book is to provide readers with an understanding of how to design, build, and deploy effective APIs. \nWrong Answers: \n1. To provide readers with an understanding of cloud computing. \n2. To provide readers with an understanding of how to design, build, and deploy a monolithic"}
{"prompt": "Context: build and deployment pipelines allowed us to do continuous integration and deployment, and containers and associated technologies like orchestration let us run large numbers of small, independent services as a distributed system.  Why are we doing that? Because of the second thing: the research showing that successful software development organizations have loosely coupled architectures and autonomous, empowered teams. Successful here is defined in terms of a positive impact on the business: increased market share, productivity, and profitability.  Our architectures now tend to be more loosely coupled, distributed, and built around APIs. You want your APIs to be discoverable, consistent, and unlikely to cause problems to the consumers even if they change unexpectedly or disappear. Anything else will couple work together and slow down your teams.  In this book, James, Daniel and Matthew provide a comprehensive and practical guide to building effective API architectures. They cover a lot of ground, from how to build and test an individual API, through the ecosystem you deploy them into, the ways to release and operate them effectively, and perhaps most importantly, how to use APIs to evolve your architecture. Those first APIs I built at the Financial Times don\u2019t exist anymore, and we built those systems again from scratch. That\u2019s costly. James, Daniel, and Matthew provide a template for how to deal with inevitable change and evolve your systems, using APIs as a key tool.\n\nQuestions:", "completion": "Question 1: What are the benefits of using continuous integration and deployment pipelines? \nCorrect Answer: Continuous integration and deployment pipelines allow for faster, more reliable, and more cost-effective development, testing, and deployment of software. \nWrong Answers: \na) Continuous integration and deployment pipelines are not necessary for successful software development. \nb) Continuous integration and deployment pipelines do not reduce the risk of software bugs. \nc) Continuous integration and deployment pipelines do not improve the scalability of software. \n\nQuestion 2: What is the purpose of using containers and orchestration technologies? \nCorrect Answer: Containers and orchestration technologies are used to run large numbers of small, independent services as a distributed system.\nWrong Answers:\na) Containers and orchestration technologies are not used for scalability and reliability. \nb) Containers and orchestration technologies are not used to improve the speed of software development. \nc) Containers and orchestration technologies are not used to ensure secure software systems. \n\nQuestion 3: What are the benefits of using APIs in software development? \nCorrect Answer: APIs allow for more discoverable, consistent, and reliable systems that are less likely to cause problems to consumers if they change unexpectedly or disappear.\nWrong Answers: \na) APIs do not provide scalability and reliability. \nb) APIs are not used for faster development. \nc) APIs"}
{"prompt": "Context: Software architecture has been defined as those decisions that are both important and hard to change. These are the decisions that will make your project succeed\u2014or fail.  The authors\u2019 focus is not on architecture in the abstract, but on how you apply architecture within your own organizations. Deciding to adopt an API gateway or a service mesh, and which one, is exactly the kind of hard-to-undo decision that you should approach with caution and evaluate carefully. James, Daniel, and Matthew give strong, opinionated guidance where they feel it is appropriate, and where the options are less clearcut, they provide a framework to help you make the best choice for your circumstances.  They illustrate throughout with a practical and realistic case study that takes the concepts and shows how you actually make them work in practice. Their case study evolves throughout the book, in the same way real systems do. The authors show that you don\u2019t have to do everything upfront; you can evolve your architecture piece by piece, extracting services and adding tools like API gateways and service meshes as you find you need them.  When I built my first APIs, I made a lot of mistakes. I wish I\u2019d had a book like this, to help me understand where I might trip up, and to guide me towards sensible decisions.  I recommend this book to anyone leading work on systems where APIs play a major role. With it, you should be able to develop a consistent set of tools and standards to support every team\n\nQuestions:", "completion": "Q1. What is software architecture?\nA1. Software architecture is the set of important, hard-to-change decisions that will determine a project's success or failure. \nWrong Answers:\n-Software architecture is the design of a system's code.\n-Software architecture is the process of writing code.\n-Software architecture is the process of gathering user requirements.\n\nQ2. What is the focus of the book?\nA2. The focus of the book is on how to apply architecture within an organization.\nWrong Answers:\n-The focus of the book is on how to design a system's code.\n-The focus of the book is on how to gather user requirements.\n-The focus of the book is on how to write code.\n\nQ3. What is the authors' approach to making decisions?\nA3. The authors give strong, opinionated guidance where appropriate, and provide a framework to help make the best choice for the circumstances where the options are less clearcut.\nWrong Answers:\n-The authors provide vague guidance and let the reader make the best choice.\n-The authors encourage the reader to make decisions without considering the circumstances.\n-The authors provide a detailed set of rules to follow without considering the context."}
{"prompt": "Context: building APIs in your organization.  Sarah Wells, Co-Chair of the QCon London conference, independent consultant, and former Technical Director at the Financial Times, Reading, UK, September 2022      Preface    Why Did We Write This Book?   In early 2020 we attended O\u2019Reilly Software Architecture in New York, where Jim and Matt gave a workshop on APIs and a presentation on API gateways. Jim and Daniel know each other from the London Java Community, and like at many architecture events, we got together to talk about our thoughts and understanding around API architectures. As we were talking on the hallway track, several conference delegates came up to us and chatted about their experiences with APIs. People were asking for our thoughts and guidance on their API journey. It was at this point that we thought writing a book on the topic of APIs would help share our discussions from conferences with other architects.      Why Should You Read This Book?   This book has been designed to provide a complete picture on designing, operating, and evolving an API architecture. We have shared our experience and advice through both our writing and an accompanying case study that mimics a real-life event-management conference system that enables attendees to view and book presentation sessions. The case study runs throughout the book, with the goal of you exploring how abstract concepts sometimes translate into practical application. If you want a high-level overview of the evolution of the\n\nQuestions:", "completion": "Question 1: What is the goal of the accompanying case study in this book? \nCorrect Answer: The goal of the accompanying case study is to explore how abstract concepts translate into practical application.\nWrong Answer 1: The goal of the accompanying case study is to explore how to build APIs.\nWrong Answer 2: The goal of the accompanying case study is to explore how to write software.\nWrong Answer 3: The goal of the accompanying case study is to explore how to design databases.\n\nQuestion 2: Who wrote this book?\nCorrect Answer: Sarah Wells, Jim, Matt, and Daniel wrote this book.\nWrong Answer 1: Jim, Matt, and Daniel wrote this book.\nWrong Answer 2: Sarah Wells wrote this book.\nWrong Answer 3: OReilly Software Architecture wrote this book.\n\nQuestion 3: When did the authors attend OReilly Software Architecture? \nCorrect Answer: The authors attended OReilly Software Architecture in early 2020.\nWrong Answer 1: The authors attended OReilly Software Architecture in late 2020.\nWrong Answer 2: The authors attended OReilly Software Architecture in early 2022.\nWrong Answer 3: The authors attended OReilly Software Architecture in late 2022."}
{"prompt": "Context: case study, you can find this in Chapter 10.  We also believe in allowing you to make your own decisions. To support this, we will:  Be clear when we have a strong recommendation or guidance.    Highlight areas of caution and problems that you may encounter.    Supply an Architecture Decision Record (ADR) Guideline1 to help inform the best possible decision given the circumstances of your architecture and provide guidance on what to consider (because sometimes the answer is \u201cit depends\u201d).    Highlight references and useful articles where you can find more in-depth content.      The book is not just a greenfield technology book. We felt that covering existing architectures with an evolutionary approach toward more suitable API architectures would provide the most benefit for you. We also tried to balance this with looking forward to newer technologies and developments in the API architecture domain.      Who This Book Is For   Although we had an initial persona in mind when creating this book, during the writing and reviewing process three key personas emerged: the developer, an accidental architect, and the solutions or enterprise architect. We have outlined these personas in the following sections, with the aim that you not only identify with at least one of them, but also so that you can look at each chapter through the different lens these personas provide.      Developer   You have most likely been coding professionally for several years and have a good understanding of\n\nQuestions:", "completion": "Q1. Who is this book for? \nA1. This book is for developers, accidental architects and solutions or enterprise architects. \nWrong Answers: \nA1a. This book is for novice developers. \nA1b. This book is for experienced architects. \nA1c. This book is for web developers."}
{"prompt": "Context: common software development challenges, patterns, and best practices. You are increasingly realizing that the software industry\u2019s march toward building service-oriented architecture (SOA) and adopting cloud services means that building and operating APIs is fast becoming a core skill. You are keen to learn more about designing effective APIs and testing them. You want to explore the various implementation choices (e.g., synchronous versus asynchronous communication) and technologies and learn how to ask the right questions and evaluate which approach is best for a given context.      Accidental Architect   You have most likely been developing software for many years and have often operated as a team lead or resident software architect (even if you don\u2019t have the official titles). You understand core architectural concepts, such as designing for high cohesion and loose coupling, and apply these to all aspects of software development, including design, testing, and operating systems.  You realize that your role is increasingly focused on combining systems to meet customer requirements. This could include internally built applications and third-party SaaS-type offerings. APIs play a big part in successfully integrating your systems with external systems. You want to learn more about the supporting technologies (e.g., API gateway, service mesh, etc.) and also understand how to operate and secure API-based systems.      Solutions/Enterprise Architect   You have been designing and\n\nQuestions:", "completion": "Q1: What is an API Gateway?\nA1: An API Gateway is a type of service that acts as a single point of entry for multiple APIs and provides a means of controlling and managing API traffic. \nWrong Answers:\n1. An API Gateway is a type of monitoring tool for API performance.\n2. An API Gateway is a type of authentication service for managing API access.\n3. An API Gateway is a type of database for storing API data.\n\nQ2: What is the purpose of Service Mesh?\nA2: The purpose of Service Mesh is to provide a way of managing inter-service communication and routing in a distributed system.\nWrong Answers:\n1. The purpose of Service Mesh is to provide a way of writing code for distributed systems.\n2. The purpose of Service Mesh is to provide a way of managing API traffic.\n3. The purpose of Service Mesh is to provide a way of securely storing API data.\n\nQ3: What is the difference between synchronous and asynchronous communication?\nA3: The difference between synchronous and asynchronous communication is that in synchronous communication, the sender and receiver communicate in real time while in asynchronous communication, the sender and receiver do not communicate in real time.\nWrong Answers:\n1. The difference between synchronous and asynchronous communication is that in synchronous communication, the sender and receiver communicate via a single channel while in asynchronous communication, the sender"}
{"prompt": "Context: building enterprise software systems for several years and most likely have the word architect in your job title or role description. You are responsible for the big picture of software delivery and typically work within the context of a large organization or a series of large interconnected organizations.  You recognize the changes that the latest iteration of service-based architectural styles are having on the design, integration, and governance of software, and you see APIs are pivotal to the success of your organization\u2019s software strategy. You are keen to learn more about evolutionary patterns and understand how the choice of API design and implementation will impact this. You also want to focus on the cross-functional \u201cilities\u201d\u2014usability, maintainability, scalability, and availability\u2014and understand how to build API-based systems that exhibit such properties, as well as provide security.      What You Will Learn   After reading this book you will understand:  The fundamentals of REST APIs and how to best build, version, and test APIs    The architectural patterns involved in building an API platform    The differences in managing API traffic at ingress and within service-to-service communication, and how to apply patterns and technologies such as API gateways and service meshes    Threat modeling and key security considerations for APIs, such as authentication, authorization, and encryption    How to evolve existing systems toward APIs and different target deployments,\n\nQuestions:", "completion": "Q1: What is the main purpose of APIs? \nA1: The main purpose of APIs is to allow applications to communicate and interact with each other.\nWrong Answers:\nA1: The main purpose of APIs is to store data.\nA1: The main purpose of APIs is to create graphical user interfaces.\nA1: The main purpose of APIs is to optimize code.\n\nQ2: What technology is used to manage API traffic at ingress and within service-to-service communication?\nA2: API gateways and service meshes are used to manage API traffic at ingress and within service-to-service communication.\nWrong Answers:\nA2: Load balancers are used to manage API traffic at ingress and within service-to-service communication.\nA2: Content Delivery Networks are used to manage API traffic at ingress and within service-to-service communication.\nA2: Database systems are used to manage API traffic at ingress and within service-to-service communication.\n\nQ3: What are the key security considerations for APIs?\nA3: The key security considerations for APIs include authentication, authorization, and encryption.\nWrong Answers:\nA3: The key security considerations for APIs include hashing and encryption.\nA3: The key security considerations for APIs include encryption and data masking.\nA3: The key security considerations for APIs include user authentication and data"}
{"prompt": "Context: such as the cloud      And you will be able to:  Design, build, and test API-based systems    Help to implement and drive an organization\u2019s API program from an architectural perspective    Deploy, release, and configure key components of an API platform    Deploy gateways and service meshes based on case studies    Identify vulnerabilities in API architecture and implement measured security mitigations    Contribute to emerging API trends and the associated communities      What This Book Is Not   We realize that APIs encompass a vast market space and we want to be clear what this book will not cover. That doesn\u2019t mean to say that we believe these topics are not important; however, if we tried to cover everything we wouldn\u2019t be able to share our knowledge effectively with you.  We will cover application patterns for migration and modernization that will include taking advantage of cloud platforms, but the book is not wholly focused on cloud technologies. Many of you will have hybrid architectures or even have all of your systems hosted in data centers. We want to ensure that we cover the design and operational factors of API architectures that support both approaches.  The book is not tied to a specific language but will use some lightweight examples to demonstrate approaches to building/designing APIs and their corresponding infrastructure. The book will focus more on the approach, and code examples will be available in the accompanying GitHub repository.  The book does not\n\nQuestions:", "completion": "Q1: How can using cloud technologies help with designing, building and testing APIs?\nA1: Using cloud technologies can help with designing, building and testing APIs by providing a platform to quickly deploy applications and services that can be easily scaled and adapted.\n\nWrong answers: \nA1a: Using cloud technologies can help with designing, building and testing APIs by providing a secure environment.\nA1b: Using cloud technologies can help with designing, building and testing APIs by providing a cost-efficient solution.\nA1c: Using cloud technologies can help with designing, building and testing APIs by providing an easy-to-use interface.\n\nQ2: What is the focus of the book?\nA2: The book focuses on the approach to building and designing APIs and their corresponding infrastructure.\n\nWrong answers:\nA2a: The book focuses on cloud technologies.\nA2b: The book focuses on a specific language.\nA2c: The book focuses on security mitigations.\n\nQ3: What are some of the topics not covered in the book?\nA3: Some of the topics not covered in the book include cloud technologies, a specific language and security mitigations.\n\nWrong answers:\nA3a: Some of the topics not covered in the book include application patterns for migration and modernization.\nA3b: Some of the topics not covered in the book include taking"}
{"prompt": "Context: favor one style of architecture over another, however we will discuss situations in which architectural approaches may cause limitations to the API offering presented.      Conventions Used in This Book   The following typographical conventions are used in this book:  Italic  Indicates new terms, URLs, email addresses, filenames, and file extensions.    Constant width  Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.    Constant width bold  Shows commands or other text that should be typed literally by the user.    Constant width italic  Shows text that should be replaced with user-supplied values or by values determined by context.      Tip   This element signifies a tip or suggestion.      Note   This element signifies a general note.      Warning   This element indicates a warning or caution.      Using Code Examples   Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/masteringapi.  If you have a technical question or a problem using the code examples, please send an email to bookquestions@oreilly.com.  This book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for permission unless you\u2019re reproducing a significant portion of the code. For example, writing\n\nQuestions:", "completion": "Q1: What is the capital of France? \nA1: Paris \nWrong answers: London, Madrid, Rome \n\nQ2: What is the time complexity of a binary search? \nA2: O(log n) \nWrong answers: O(n), O(n log n), O(1) \n\nQ3: What is the official language of Brazil? \nA3: Portuguese \nWrong answers: Spanish, English, French \n\nQ4: What is the largest ocean in the world? \nA4: Pacific Ocean \nWrong answers: Atlantic Ocean, Indian Ocean, Arctic Ocean \n\nQ5: What is the world's longest river?\nA5: The Nile \nWrong answers: Amazon, Yangtze, Congo"}
{"prompt": "Context: a program that uses several chunks of code from this book does not require permission. Selling or distributing examples from O\u2019Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a significant amount of example code from this book into your product\u2019s documentation does require permission.  We appreciate, but generally do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: \u201cMastering API Architecture by James Gough, Daniel Bryant, and Matthew Auburn (O\u2019Reilly). Copyright 2023 James Gough Ltd, Big Picture Tech Ltd, and Matthew Auburn Ltd, 978-1-492-09063-2.\u201d  If you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com.      O\u2019Reilly Online Learning    Note   For more than 40 years, O\u2019Reilly Media has provided technology and business training, knowledge, and insight to help companies succeed.    Our unique network of experts and innovators share their knowledge and expertise through books, articles, and our online learning platform. O\u2019Reilly\u2019s online learning platform gives you on-demand access to live training courses, in-depth learning paths, interactive coding environments, and a vast collection of text and video from O\u2019Reilly and 200+ other publishers. For more information, visit https://oreilly.com.      How to Contact Us   Please address\n\nQuestions:", "completion": "Q1: Does selling or distributing examples from OReilly books require permission?\nA1: Yes, selling or distributing examples from OReilly books does require permission. \nWrong Answers: No, selling or distributing examples from OReilly books does not require permission; Maybe, selling or distributing examples from OReilly books requires permission; It depends, selling or distributing examples from OReilly books requires permission.\n\nQ2: How can I contact OReilly?\nA2: You can contact OReilly by visiting https://oreilly.com. \nWrong Answers: You can contact OReilly by sending an email to permissions@oreilly.com; You can contact OReilly by calling their customer service line; You can contact OReilly by visiting their physical store."}
{"prompt": "Context: comments and questions concerning this book to the publisher:  O\u2019Reilly Media, Inc.  1005 Gravenstein Highway North  Sebastopol, CA 95472  800-998-9938 (in the United States or Canada)  707-829-0515 (international or local)  707-829-0104 (fax)    We have a web page for this book, where we list errata, examples, and any additional information. You can access this page at https://oreil.ly/Mastering-API-Architecture.  Email bookquestions@oreilly.com to comment or ask technical questions about this book.  For news and information about our books and courses, visit https://oreilly.com.  Find us on LinkedIn: https://linkedin.com/company/oreilly-media.  Follow us on Twitter: https://twitter.com/oreillymedia.  Watch us on YouTube: https://www.youtube.com/oreillymedia.      Acknowledgments   As with almost all technical books, only three names may be listed as authors on the front of this book, but the reality is that many people have contributed, either directly in the form of feedback as the book was written, or indirectly by their teaching and guidance over the years.  Although we can\u2019t possibly list everyone who has helped us during this journey, we would like to explicitly thank the people who took time out of their busy schedules to provide extensive discussions, feedback, and support.  Our technical reviewers: Sam Newman, Dov Katz, Sarah Wells, Antoine Cailliau, Stefania Chaplin, Matt McLarty, and Neal Ford.  For general detail, encouragement, advice, and introductions: Charles\n\nQuestions:", "completion": "Q1: How can I ask technical questions about this book? \nA1: You can email bookquestions@oreilly.com to ask technical questions about this book. \nWrong Answers:\n1. You can call the publisher to ask technical questions. \n2. You can ask the authors directly. \n3. You can post your questions on the book's web page."}
{"prompt": "Context: Humble, Richard Li, Simon Brown, Nick Ebbitt, Jason Morgan, Nic Jackson, Cliff Tiltman, Elspeth Minty, George Ball, Benjamin Evans, and Martijn Verberg.  The O\u2019Reilly Team: Virginia Wilson, Melissa Duffield, and Nicole Tache.      James Gough   I would like to thank my incredible family: Megan, Emily, and Anna. Writing would not have been possible without their help and support. I\u2019d also like to thank my parents, Heather and Paul, for encouraging me to learn and for their constant support.  I\u2019d like to thank my coauthors Daniel and Matt; writing a book is a challenge and, like architecture, is never perfect. It has been a fun journey and we have all learned a lot from each other and our amazing reviewers. Finally, I\u2019d like to thank Jon Daplyn, Ed Safo, David Halliwell, and Dov Katz for providing me support, opportunities, and encouragement throughout my career.      Daniel Bryant   I would like to thank my entire family for their love and support, both during the authoring process and throughout my career. I would also like to thank Jim and Matt for being great partners on this writing journey. We started this book in early 2020, just as the pandemic hit. Our weekly Wednesday morning calls were not only useful for collaboration but also a great source of fun and support as our worlds rapidly changed. Finally, I would like to thank everyone involved in the London Java Community (LJC), the InfoQ/QCon teams, and everyone at Ambassador Labs. These three communities have provided\n\nQuestions:", "completion": "Q1: Who was part of the OReilly Team?\nA1: Virginia Wilson, Melissa Duffield, and Nicole Tache\nWrong Answers: Richard Li, Simon Brown, Nick Ebbitt\n\nQ2: Who did James Gough thank?\nA2: Megan, Emily, Anna, Heather and Paul, Daniel and Matt, Jon Daplyn, Ed Safo, David Halliwell, and Dov Katz\nWrong Answers: Elspeth Minty, George Ball, Benjamin Evans, and Martijn Verberg, Virginia Wilson, Melissa Duffield, and Nicole Tache"}
{"prompt": "Context: me with access to mentors, guidance, and so many opportunities. I hope to someday pay all of this forward.      Matthew Auburn   I would like to thank my amazing wife Hannah\u2014without your support I would not have been able to write this book. Thank you to both of my parents; you have shown me that anything is possible and never stopped believing in me. This book has been such an amazing experience to go through, and Jim and Dan, you have both been excellent mentors. Both of you have taught me so much and helped me write the best material possible. An additional thanks to Jim\u2014you introduced me to speaking and have helped me more than anyone else in my career. Finally and most importantly, I want to thank my son Joshi: you are an absolute joy.      1 You will learn more about ADRs and their importance to making and documenting architectural decisions in the Introduction.      Introduction   In this introduction, you will discover the basics of APIs and their potential to be part of the architecture journey. We will introduce a lightweight definition for APIs and their use in and out of process. In order to demonstrate the importance of APIs, we will introduce the conference system case study, a running example that will evolve throughout the book. Out-of-process APIs allow you to think beyond a simple three-tiered architecture; we will introduce traffic patterns and their importance to demonstrate this. We will outline a summary of the case study steps, allowing you to skip\n\nQuestions:", "completion": "1. What is an API?\nCorrect Answer: API stands for Application Programming Interface and it is a set of tools and protocols used to build software applications. \nWrong Answers:\n- A type of computer virus\n- An operating system\n- A programming language\n\n2. What does ADR mean?\nCorrect Answer: ADR stands for Architectural Decision Record, and it is a record of decisions made about the architecture of a system. \nWrong Answers:\n- An acronym for Anti-DDoS Regulation\n- A type of software version control\n- A type of computer virus\n\n3. What is the purpose of a traffic pattern?\nCorrect Answer: A traffic pattern is a set of guidelines used to manage traffic and optimize performance of a system.\nWrong Answers:\n- A type of computer virus\n- A type of software version control\n- A type of networking protocol\n\n4. What is a three-tiered architecture?\nCorrect Answer: A three-tiered architecture is a type of software architecture that divides a system into three components: presentation, logic and data. \nWrong Answers:\n- A type of computer virus\n- A type of software version control\n- A type of networking protocol"}
{"prompt": "Context: ahead if an area is of interest to you straight away.  In order to present APIs and their associated ecosystem, we will use a series of important artifacts. We will introduce the case study with C4 model diagrams and revisit the specifics and logic behind the approach. You will also learn about the use of Architecture Decision Records (ADRs) and the value of clearly defining decisions across the software lifecycle. As the introduction closes, we will outline ADR Guidelines\u2014our approach to help you make decisions when the answer is \u201cit depends.\u201d      The Architecture Journey   Anyone who has taken a long journey will no doubt have encountered the question (and possibly persistently) \u201care we there yet?\u201d For the first few inquiries, you look at the GPS or a route planner and provide an estimate\u2014hoping that you don\u2019t encounter any delays along the way. Similarly, the journey to building API-based architectures can be complex for developers and architects to navigate; even if there was an Architecture GPS, what would your destination be?  Architecture is a journey without a destination, and you cannot predict how technologies and architectural approaches will change. For example, you may not have been able to predict service mesh technology would become so widely used, but once you learn about its capabilities it may cause you to think about evolving your existing architecture. It is not only technologies that influence change in architecture; new business requirements and\n\nQuestions:", "completion": "Q1: What is the C4 model?\nA1: The C4 model is a software architecture model used to describe the structure of a system.\nWrong Answers:\n- The C4 model is a tool used to analyze data.\n- The C4 model is a technique used to create software applications.\n- The C4 model is a method used to visualize information.\n\nQ2: What is the purpose of Architecture Decision Records (ADRs)?\nA2: The purpose of Architecture Decision Records (ADRs) is to document important design decisions and their contexts.\nWrong Answers:\n- The purpose of Architecture Decision Records (ADRs) is to record changes to the software code.\n- The purpose of Architecture Decision Records (ADRs) is to store information about the software architecture.\n- The purpose of Architecture Decision Records (ADRs) is to track the progress of software development.\n\nQ3: What is the goal of the Architecture Journey?\nA3: The goal of the Architecture Journey is to build API-based architectures.\nWrong Answers:\n- The goal of the Architecture Journey is to develop software applications.\n- The goal of the Architecture Journey is to analyze data.\n- The goal of the Architecture Journey is to visualize information.\n\nQ4: What is service mesh technology?\nA4: Service mesh technology is a technology used to manage inter-service communication and secure microservices"}
{"prompt": "Context: constraints also drive change in architectural direction.  The culminating effect of delivering incremental value combined with new emerging technologies leads to the concept of evolutionary architecture. Evolutionary architecture is an approach to incrementally changing an architecture, focusing on the ability to change with speed and reducing the risk of negative impacts. Along the way, we ask you to keep the following advice in approaching API architecture in mind:  Though architects like to be able to strategically plan for the future, the constantly changing software development ecosystem makes that difficult. Since we can\u2019t avoid change, we need to exploit it.  Building Evolutionary Architectures by Neal Ford, Rebecca Parsons, and Patrick Kua (O\u2019Reilly)    In many projects APIs themselves are evolutionary, requiring change as more systems and services are integrated. Most developers have built services that focus on a single function without considering the broader API reuse from a consumer perspective.  API-First design is an approach where developers and architects consider the functionality of their service and design an API in a consumer-centric manner. The API consumer could be a mobile application, another service, or even an external customer. In Chapter 1 we will review design techniques to support an API-First approach and discover how we build APIs that are durable to change and deliver value to a broad consumer base.  The good news is that you can start an\n\nQuestions:", "completion": "Q1: What is Evolutionary Architecture?\nA1: Evolutionary architecture is an approach to incrementally changing an architecture, focusing on the ability to change with speed and reducing the risk of negative impacts. \nWrong Answers:\n1. Evolutionary Architecture is a type of software design pattern.\n2. Evolutionary Architecture is a type of database architecture.\n3. Evolutionary Architecture is a type of distributed computing architecture. \n\nQ2: What is API-First design?\nA2: API-First design is an approach where developers and architects consider the functionality of their service and design an API in a consumer-centric manner.\nWrong Answers: \n1. API-First design is an approach where developers design an API to be consumer-agnostic. \n2. API-First design is an approach where developers design an API to be platform-agnostic. \n3. API-First design is an approach where developers design an API to be language-agnostic. \n\nQ3: What advice is given for approaching API architecture?\nA3: Though architects like to be able to strategically plan for the future, the constantly changing software development ecosystem makes that difficult. Since we cant avoid change, we need to exploit it.\nWrong Answers:\n1. Though architects like to be able to strategically plan for the future, the software development ecosystem is stable and predictable. \n2. Though architects"}
{"prompt": "Context: API-driven architecture journey at any point. If you are responsible for preexisting technical inventory, we will show you techniques to evolve your architecture to promote the use of APIs in your platform. On the other hand, if you are lucky and have a blank canvas to work with, we will share with you the benefit of adopting API architectures based on our years of experience, while also highlighting key factors in decision making.      A Brief Introduction to APIs   In the field of software architecture, there are a handful of terms that are incredibly difficult to define. The term API, which stands for application programming interface, falls into this categorization, as the concept first surfaced as many as 80 years ago. Terms that have been around for a significant amount of time end up being overused and having multiple meanings in different problem spaces. We consider an API to mean the following:  An API represents an abstraction of the underlying implementation.    An API is represented by a specification that introduces types. Developers can understand the specifications and use tooling to generate code in multiple languages to implement an API consumer (software that consumes an API).    An API has defined semantics or behavior to effectively model the exchange of information.    Effective API design enables extension to customers or third parties for a business integration.      Broadly speaking, APIs can be broken into two general categories depending on whether\n\nQuestions:", "completion": "Q1. What is an API? \nA1. An API is an application programming interface that represents an abstraction of the underlying implementation. \nWrong Answers: \nA1. An API is a type of software. \nA1. An API is a type of database. \nA1. An API is a type of programming language."}
{"prompt": "Context: the API invocation is in process or out of process. The process being referred to here is an operating system (OS) process. For example, a Java method invocation from one class to another is an in-process API invocation, as the call is handled by the same process from which the call was made. A .NET application invocating an external REST-like API using an HTTP library is an out-of-process API invocation, as the call is handled by an additional external process other than the process from which the call was made. Typically, an out-of-process API call will involve data traversing a network, potentially a local network, virtual private cloud (VPC) network, or the internet. We will focus on the latter style of APIs; however, architects will often encounter the requirement to remodel an in-process API to an out-of-process API. In order to demonstrate this concept (and others), we will create a running case study that will evolve throughout the book.      Running Example: Conference System Case Study   We have chosen to model a conference system for our case study because the domain is easily recognizable but also provides enough complexity for modeling an evolutionary architecture. Figure I-1 visualizes the conference system at the top level, allowing us to set the context of the architecture under discussion. The system is used by an external customer to create their attendee account, review the conference sessions available, and book their attendance.      Figure I-1. C4\n\nQuestions:", "completion": "Q1: What is an in-process API invocation? \nA1: An in-process API invocation is a Java method invocation from one class to another which is handled by the same process from which the call was made. \nWrong Answers: \n1. An in-process API invocation is a .NET application invocating an external REST-like API using an HTTP library. \n2. An in-process API invocation is a data traversing a network. \n3. An in-process API invocation is a remodeling of an out-of-process API.\n\nQ2: What is an out-of-process API invocation? \nA2: An out-of-process API invocation is a .NET application invocating an external REST-like API using an HTTP library which is handled by an additional external process other than the process from which the call was made. \nWrong Answers: \n1. An out-of-process API invocation is a Java method invocation from one class to another. \n2. An out-of-process API invocation is a remodeling of an in-process API.\n3. An out-of-process API invocation is an invocation that is handled by the same process from which the call was made. \n\nQ3: What is the running example for this book?\nA3: The running example for this book is a conference system case study"}
{"prompt": "Context: conference system context diagram      Let\u2019s zoom in to the conference system box in Figure I-2. Expanding the conference system provides us more detail about its major technical building blocks. The customer interacts with the web application, which invokes APIs on the conference application. The conference application uses SQL to query the backing datastore.      Figure I-2. C4 conference system container diagram      Figure I-2 reveals that from an API perspective the most interesting functionality is within the conference application container. Figure I-3 zooms in to this specific container, allowing you to explore the structure and interactions.  Four major components and the database are involved in the current system. The API Controller faces all incoming traffic from the UI and makes a decision about where to route the request in the system. This component would also be responsible for marshaling from the on the wire network-level representation to an object or representation in code. The API Controller component is intriguing from the perspective of in-process routing and acting as a junction point or front controller pattern. For API requests and processing, this is an important pattern; all requests pass through the controller, which makes a decision on where the request is directed. In Chapter 3 we will look at the potential for taking the controller out of process.  The Attendee, Booking, and Session components are involved in translating the requests into\n\nQuestions:", "completion": "Q1: What is the main component of the conference system? \nA1: The API Controller is the main component of the conference system.\nWrong Answers: 1. The Web Application 2. The SQL Database 3. The Conference Application\n\nQ2: What pattern is used for API requests and processing?\nA2: The Front Controller pattern is used for API requests and processing.\nWrong Answers: 1. The Observer Pattern 2. The Facade Pattern 3. The Singleton Pattern\n\nQ3: What does the API Controller do?\nA3: The API Controller makes a decision about where to route the request in the system and marshals from the on the wire network-level representation to an object or representation in code.\nWrong Answers: 1. It handles all incoming traffic from the UI 2. It stores data in the database 3. It translates requests into database queries\n\nQ4: What components are involved in the current system?\nA4: The four major components involved in the current system are the API Controller, Attendee, Booking, and Session components.\nWrong Answers: 1. The Web Application 2. The SQL Database 3. The Conference Application"}
{"prompt": "Context: queries and execute SQL against the database out of process. In the existing architecture, the database is an important component, potentially enforcing relationships\u2014for example, constraints between bookings and sessions.      Figure I-3. C4 conference system component diagram      Now that we have drilled down to the appropriate level of detail, let\u2019s revisit the types of API interactions in the case study at this point.      Types of APIs in the Conference Case Study   In Figure I-3 the Web Application to API Controller arrow is an out-of-process call, whereas the API Controller to Attendee Component arrow is an example of an in-process call. All interactions within the Conference Application boundary are examples of in-process calls. The in-process invocation is well defined and restricted by the programming language used to implement the Conference Application. The invocation is compile-time safe (the conditions under which the exchange mechanism are enforced at the time of writing code).      Reasons for Changing the Conference System   The current architectural approach has worked for the conference system for many years, however the conference owner has asked for three improvements, which are driving architectural change:  The conference organizers would like to build a mobile application.    The conference organizers plan to go global with their system, running tens of conferences instead of one per year. In order to facilitate this expansion, they would like to\n\nQuestions:", "completion": "Q1. What type of API interaction is shown in Figure I-3?\nA1. Out-of-process call \nWrong answers:\n-In-process call\n-Real-time call\n-Compile-time call"}
{"prompt": "Context: integrate with an external Call for Papers (CFP) system for managing speakers and their application to present sessions at the conference.    The conference organizers would like to decommission their private data center and instead run the conference system on a cloud platform with global reach.      Our goal is to migrate the conference system to be able to support the new requirements, without impacting the existing production system or rewriting everything in one go.      From Tiered Architecture to Modeling APIs   The starting point of the case study is a typical three-tier architecture, composed of a UI, a server-side processing tier, and a datastore. To begin to discuss an evolutionary architecture we need a model to think about the way API requests are processed by the components. We need a model/abstraction that will work for both the public cloud, virtual machines in a data center and a hybrid approach.  The abstraction of traffic will allow us to consider out-of-process interactions between an API consumer and an API service, sometimes referred to as the API producer. With architectural approaches like service-oriented architecture (SOA) and microservices-based architecture, the importance of modeling API interactions is critical. Learning about API traffic and the style of communication between components will be the difference between realizing the advantages of increased decoupling or creating a maintenance nightmare.      Warning   Traffic patterns are used by\n\nQuestions:", "completion": "Q1: What is the starting point of the case study?\nA1: The starting point of the case study is a typical three-tier architecture. \nWrong Answers: \nA) The starting point of the case study is a public cloud platform. \nB) The starting point of the case study is a virtual machine in a data center.\nC) The starting point of the case study is a hybrid approach.\n\nQ2: What is the goal of the case study?\nA2: The goal of the case study is to migrate the conference system to be able to support the new requirements, without impacting the existing production system or rewriting everything in one go.\nWrong Answers:\nA) The goal of the case study is to build a new conference system from scratch. \nB) The goal of the case study is to integrate with an external Call for Papers (CFP) system.\nC) The goal of the case study is to decommission the private data center.\n\nQ3: What is the purpose of modeling API interactions?\nA3: The purpose of modeling API interactions is to increase decoupling and realize the advantages of an architectural approach like service-oriented architecture (SOA) or microservices-based architecture. \nWrong Answers:\nA) The purpose of modeling API interactions is to reduce complexity.\nB) The purpose of modeling API interactions is to simplify maintenance."}
{"prompt": "Context: data center engineers to describe network exchanges within data centers and between low-level applications. At the API level we are using traffic patterns to describe flows between groups of applications. For the purposes of this book, we are referring to application and API-level traffic patterns.      Case Study: An Evolutionary Step   To start to consider traffic pattern types, it will be useful to take a small evolutionary step in our case study architecture. In Figure I-4 a step has been taken to refactor the Attendee component into an independent service, as opposed to a package or module within the legacy conference system. The conference system now has two traffic flows: the interaction between the customer and the legacy conference system and the interaction between the legacy system and the attendee system.      Figure I-4. C4 conference system context\u2014evolutionary step      North\u2013south traffic   In Figure I-4 interaction between the customer and the legacy conference system is referred to a north\u2013south traffic, and it represents an ingress flow. The customer is using the UI, which is sending requests to the legacy conference system over the internet. This represents a point in our network that is exposed publicly and will be accessed by the UI.1 This means that any component handling north\u2013south traffic must make concrete checks about client identity and also include appropriate challenges before allowing traffic to progress through the system. Chapter 7 will go\n\nQuestions:", "completion": "Q1: What is northsouth traffic?\nA1: Northsouth traffic is the interaction between the customer and the legacy conference system, representing an ingress flow. \nWrong Answers:\n- Northsouth traffic is the interaction between two data centers. \n- Northsouth traffic is the interaction between two applications. \n- Northsouth traffic is the interaction between two API\u2019s."}
{"prompt": "Context: into detail about securing north\u2013south API traffic.      East\u2013west traffic   The new interaction between the legacy conference system and the Attendee service introduces an east\u2013west traffic flow to our system. East\u2013west traffic can be thought of as service-to-service style of communication within a group of applications. Most east\u2013west traffic, particularly if the origin is within your wider infrastructure, can be trusted to some degree. Although we can trust the source of the traffic, it is still necessary to consider securing east\u2013west traffic.      API Infrastructure and Traffic Patterns   There are two key infrastructure components present in API-based architectures, which are key to controlling traffic. Controlling and coordinating traffic is often described as traffic management. Generally north\u2013south traffic will be controlled by API gateways, the key subject for Chapter 3.  East\u2013west traffic will often be handled by infrastructure components like Kubernetes or service mesh, the key subject for Chapter 4. Infrastructure components like Kubernetes and service mesh use network abstractions to route to services, requiring services to run inside a managed environment. In some systems east\u2013west traffic is managed by the application itself and service discovery techniques are implemented to locate other systems.      Roadmap for the Conference Case Study   Throughout the course of the book you will observe the following changes or applications of technology to the case\n\nQuestions:", "completion": "Question 1: Which of the following is an example of eastwest traffic?\n\nCorrect Answer: Service-to-service communication within a group of applications.\n\nWrong Answers: \n1. API requests from a client to a server.\n2. Communication between two geographically distant locations.\n3. Sending data over a wireless network.\n\nQuestion 2: What is traffic management?\n\nCorrect Answer: Controlling and coordinating traffic.\n\nWrong Answers: \n1. Allocating resources in order to manage data traffic.\n2. The use of encryption to secure data.\n3. Monitoring data traffic to detect potential threats.\n\nQuestion 3: What is the key subject of Chapter 4?\n\nCorrect Answer: Kubernetes or service mesh.\n\nWrong Answers: \n1. API gateways.\n2. Service discovery.\n3. Network abstractions."}
{"prompt": "Context: study:  In Chapter 1 you will explore the design and specification of the Attendee API. We will also present the importance of versioning and modeling exchanges for performance of the Attendee API.    In Chapter 2 you will explore contract and component testing to verify behavior of the Attendee service. You will also see how Testcontainers can help with integration testing.    In Chapter 3 you will look at exposing the Attendee service to consumers using an API gateway. We will also demonstrate how to evolve the conference system using an API gateway on Kubernetes.    In Chapter 4 we will refactor the sessions functionality out of the legacy conference system using a service mesh. You will also learn about how service mesh helps with routing, observability, and security.    In Chapter 5 we will discuss feature flagging and how this can help to evolve the conference system and avoid a coupled deployment and release. You will also explore approaches for modeling releases in the conference system and we will demonstrate the use of Argo Rollouts for the Attendee service.    In Chapter 6 you will explore how to apply threat modeling and mitigate OWASP concerns in the Attendee service.    In Chapter 7 you will look at authentication and authorization and how this is implemented for the Attendee service.    In Chapter 8 you will look at establishing the Attendee service domain boundaries and how different service patterns can help.    In Chapter 9 you will look at cloud adoption\n\nQuestions:", "completion": "Q1: What is the main topic of Chapter 1?\nA1: The main topic of Chapter 1 is the design and specification of the Attendee API.\nWrong Answers: \nA) The main topic of Chapter 1 is contract and component testing.\nB) The main topic of Chapter 1 is exposing the Attendee service to consumers.\nC) The main topic of Chapter 1 is refactoring the sessions functionality out of the legacy conference system.\n\nQ2: What is the topic of Chapter 5?\nA2: The topic of Chapter 5 is feature flagging and how this can help to evolve the conference system and avoid a coupled deployment and release.\nWrong Answers: \nA) The topic of Chapter 5 is the design and specification of the Attendee API.\nB) The topic of Chapter 5 is authentication and authorization.\nC) The topic of Chapter 5 is cloud adoption."}
{"prompt": "Context: and how to move the Attendee service to the cloud and consider replatforming.      The case study and the planned roadmap require us to visualize architectural change and record decisions. These are important artifacts that help to explain and plan changes in software projects. We believe that C4 diagrams and Architecture Decision Records (ADRs) represent a clear way of recording change.      Using C4 Diagrams   As part of introducing the case study, we revealed three types of C4 diagrams from the C4 model. We believe C4 is the best documentation standard for communicating architecture, context, and interactions to a diverse set of stakeholders. You may be wondering what about UML? The Unified Modeling Language (UML) provides an extensive dialect for communicating software architectures. A major challenge is that the majority of what UML provides is not committed to memory by architects and developers, and people quickly revert to boxes/circles/diamonds. It becomes a real challenge to understand the structure of diagrams before getting into the technical content of the discussion. Many diagrams are only committed to a project history if someone accidentally uses a permanent marker instead of dry wipe marker by mistake. The C4 model provides a simplified set of diagrams that act as a guide to your project architecture at various levels of detail.      C4 Context Diagram   Figure I-1 is represented using a C4 context diagram from the C4 model. The intention of this diagram is\n\nQuestions:", "completion": "Q1: What is the C4 model? \nA1: The C4 model is a simplified set of diagrams that act as a guide to a project architecture at various levels of detail. \nWrong Answers:\n-The C4 model is a software development framework. \n-The C4 model is a unified modeling language. \n-The C4 model is an architecture decision record.\n\nQ2: What type of diagrams are part of the C4 model? \nA2: The C4 model includes four types of diagrams: Context, Container, Component, and Code. \nWrong Answers:\n-The C4 model includes three types of diagrams: Context, Code, and Connectors. \n-The C4 model includes five types of diagrams: Context, Container, Component, Connectors, and Code. \n-The C4 model includes six types of diagrams: Context, Container, Component, Connectors, Code, and Collaboration.\n\nQ3: How does the C4 model help with software projects?\nA3: The C4 model helps to communicate architecture, context, and interactions to a diverse set of stakeholders. \nWrong Answers:\n-The C4 model helps to create software projects. \n-The C4 model helps to build software projects. \n-The C4 model helps to manage software projects.\n\nQ4: What is an Architecture Decision"}
{"prompt": "Context: to set context for both a technical and nontechnical audience. Many architecture conversations dive straight into the low-level details and miss setting the context of the high-level interactions. Consider the implications of getting a system context diagram wrong\u2014the benefit of summarizing the approach may save months of work to correct a misunderstanding.      C4 Container Diagram   While Figure I-1 provides the big picture of the conference system, a container diagram helps describe the technical breakout of the major participants in the architecture. A container in C4 is defined as \u201csomething that needs to be running in order for the overall system to work\u201d (for example, the conference database). Container diagrams are technical in nature and build on the higher-level system context diagram. Figure I-2, a container diagram, documents the detail of a customer interacting with the conference system.      Note   The conference application container in Figure I-2 is documented as simply software. Normally a C4 container would provide more detail into the type of container (e.g., Java Spring Application). However in this book we will be avoiding technology specifics, unless it helps to demonstrate a specific solution. The advantage of APIs and indeed modern applications is that there is a significant amount of flexibility in the solution space.      C4 Component Diagram   The C4 component diagram in Figure I-3 helps to define the roles and responsibilities within each\n\nQuestions:", "completion": "1. What is C4?\nCorrect Answer: C4 stands for Context, Containers, Components, and Code and is a method for visualizing software architectures.\nWrong Answers: \nA) C4 is a type of programming language.\nB) C4 is an operating system.\nC) C4 is an application development tool.\n\n2. What is a System Context Diagram?\nCorrect Answer: A system context diagram is a type of diagram used to provide a high-level overview of the system and its interactions with external systems.\nWrong Answers:\nA) A system context diagram is a type of diagram used to provide a low-level view of the system.\nB) A system context diagram is used to document the detail of a customer interaction with the system.\nC) A system context diagram is used to provide a detailed view of the components and their interactions.\n\n3. What is a Container Diagram?\nCorrect Answer: A container diagram is a type of diagram used to describe the technical breakout of the major participants in the architecture.\nWrong Answers: \nA) A container diagram is used to document the detail of a customer interaction with the system.\nB) A container diagram is used to provide a low-level view of the system.\nC) A container diagram is used to provide a detailed view of the components and their interactions.\n\n4. What is a Component Di"}
{"prompt": "Context: container, along with the internal interactions. This diagram is useful if the detail of a container is queried, and it also provides a very useful map to the codebase. Think about the first time starting work on a new project: browsing a self-documenting codebase is one approach\u2014but it can be difficult to piece everything together. A component diagram reveals the detail of the language/stack you are using to build your software. In order to remain technology agnostic, we have used the term package/module.      Using Architecture Decision Records   As developers, architects, and indeed humans, we have all been in the position where we ask the question \u201cwhat were they thinking??\u201d If you have ever driven on the M62 between Leeds and Manchester in the United Kingdom, you may have been baffled by the construction of the motorway. As you climb the hill on the three-lane highway, it starts to deviate away from the traffic contraflow, until eventually Scott Hall Farm emerges surrounded by around 15 acres of farming land nestled between the carriages. Local legend of what happened described the owner of the land as stubborn and refusing to move or hand over his land, so the engineers simply built around him.2 Fifty years later a documentary surfaced revealing that the real reason for this was a geological fault beneath the land, meaning the motorway had to be built that way. When people guess why something was done in a particular way, expect rumor, humor, and criticism to emerge.\n\nQuestions:", "completion": "1. What is a component diagram? \nCorrect Answer: A component diagram is a graphical representation of the architecture of a container, showing the internal interactions. \nWrong Answers:\n- A component diagram is a graphical representation of a program's data set. \n- A component diagram is an analysis of the relationships between different elements of a system. \n- A component diagram is a graphical representation of the user interface elements. \n\n2. What is the purpose of an Architecture Decision Record?\nCorrect Answer: The purpose of an Architecture Decision Record is to document the rationale behind a design decision. \nWrong Answers:\n- The purpose of an Architecture Decision Record is to document the design of a system. \n- The purpose of an Architecture Decision Record is to provide guidance on how to design a system. \n- The purpose of an Architecture Decision Record is to provide a record of all the design decisions made in a system. \n\n3. What is the M62?\nCorrect Answer: The M62 is a motorway that runs between Leeds and Manchester in the United Kingdom. \nWrong Answers:\n- The M62 is a motorway that runs between London and Birmingham in the United Kingdom. \n- The M62 is a railway line that runs between Leeds and Manchester in the United Kingdom. \n- The M62 is a river that runs between Leeds and Manchester in the United Kingdom."}
{"prompt": "Context: In software architecture there will be many constraints that we have to build around, so it is important to ensure our decisions are recorded and transparent. ADRs help make decisions clear in software architecture.  One of the hardest things to track during the life of a project is the motivation behind certain decisions. A new person coming on to a project may be perplexed, baffled, delighted, or infuriated by some past decision.  Michael Nygard, creator of the ADR concept    There are four key sections in an ADR: status, context, decision, and consequences. An ADR is created in a proposed status and based on discussion will usually be either accepted or rejected. It is also possible that the decision may be superseded later by a new ADR. The context helps to set the scene and describe the problem or the bounds in which the decision will be made. Although creating a blog post ahead of the ADR and then linking from the ADR helps the community to follow your work, the context is not intended to be a blog post or detailed description. The decision clearly sets out what you plan to do and how you plan to do it. All decisions carry consequences or trade-offs in architecture, and these can sometimes be incredibly costly to get wrong.  When reviewing an ADR it is important to see if you agree with the decision in the ADR or if there is an alternative approach. An alternative approach that has not been considered may cause the ADR to be rejected. There is a lot of value in a\n\nQuestions:", "completion": "Question 1: What are the four sections of an ADR?\n\nCorrect Answer: Status, Context, Decision, and Consequences.\n\nWrong Answers: 1. History, Requirements, Design, and Implementation; 2. Problem, Solution, Benefits, and Drawbacks; 3. Introduction, Relevant Information, Alternatives, and Conclusion; 4. Scope, Intent, Process, and Outcome."}
{"prompt": "Context: rejected ADR and most teams choose to keep ADRs immutable to capture the change in perspective. ADRs work best when they are presented in a location where key participants can view them, comment, and help move the ADR to accepted.      Tip   A question we often get asked is at what point should the team create an ADR? It is useful to ensure that there has been discussion ahead of the ADR and the record is a result of collective thinking in the team. Publishing an ADR to the wider community allows the opportunity for feedback beyond the immediate team.      Attendees Evolution ADR   In Figure I-4 we made the decision to take an evolutionary step in the conference system architecture. This is a major change and would warrant an ADR. Table I-1 is an example ADR that might have been proposed by the engineering team owning the conference system.  Table I-1. ADR001 separating attendees from the legacy conference system Status Proposed  Context  The conference owners have requested two new major features to the current conference system that need to be implemented without disrupting the current system. The conference system will need to be evolved to support a mobile application and an integration with an external CFP system. Both the mobile application and the external CFP system need to be able to access attendees to log in users to the third-party service.    Decision  We will take an evolutionary step as documented in Figure I-4 to split out the Attendee component into a\n\nQuestions:", "completion": "Q1: What is the purpose of an ADR? \nA1: The purpose of an ADR is to document and communicate architectural decisions made by a team.\nWrong Answers: \nA1a: The purpose of an ADR is to solve technical issues.\nA1b: The purpose of an ADR is to create a plan for development. \nA1c: The purpose of an ADR is to document business decisions.\n\nQ2: When should an ADR be created?\nA2: An ADR should be created after there has been discussion and collective thinking among the team.\nWrong Answers:\nA2a: An ADR should be created before the discussion. \nA2b: An ADR should be created when a decision is made. \nA2c: An ADR should be created before the implementation of the decision. \n\nQ3: What is the benefit of publishing an ADR?\nA3: The benefit of publishing an ADR is that it allows for feedback from a wider community. \nWrong Answers:\nA3a: The benefit of publishing an ADR is that it captures the change in perspective. \nA3b: The benefit of publishing an ADR is that it moves the ADR to accepted. \nA3c: The benefit of publishing an ADR is that it helps to log in users to a"}
{"prompt": "Context: standalone service. This will allow API-First development against the Attendee service and allow the API to be invoked from the legacy conference service. This will also support the ability to design for direct access to the Attendee service to provide user information to the external CFP system.    Consequences  The call to the Attendee service will not be out of process and may introduce a latency that will need to be tested. The Attendee service could become a single point of failure in the architecture and we may need to take steps to mitigate the potential impact of running a single Attendee service. With the planned multiple consumer model for the Attendee service, we will need to ensure good design, versioning, and testing to reduce accidental breaking changes.      Some of the consequences in the ADR are fairly major and definitely require further discussion. We are going to defer some of the consequences to later chapters.      Mastering API: ADR Guidelines   Within Mastering API Architecture we will be supplying ADR Guidelines to help collect important questions to ask when making decisions on the topic we are covering. Making decisions about an API-based architecture can be really tough, and in a lot of situations the answer is \u201cit depends.\u201d Rather than say it depends without context, the ADR Guidelines will help describe what it depends on and help inform your decisions. The ADR Guidelines can be used as a reference point to come back to or to read ahead to if\n\nQuestions:", "completion": "Q1: What is the purpose of the standalone service?\nA1: The purpose of the standalone service is to allow API-First development against the Attendee service and allow the API to be invoked from the legacy conference service.\nWrong Answers: \n- The purpose of the standalone service is to provide user information to the external CFP system. \n- The purpose of the standalone service is to create a single point of failure in the architecture.\n- The purpose of the standalone service is to provide versioning and testing for the Attendee service. \n\nQ2: What are the consequences of running a single Attendee service?\nA2: The Attendee service could become a single point of failure in the architecture and we may need to take steps to mitigate the potential impact of running a single Attendee service.\nWrong Answers:\n- The consequences of running a single Attendee service are increased latency. \n- The consequences of running a single Attendee service are design flaws. \n- The consequences of running a single Attendee service are accidental breaking changes. \n\nQ3: What is the purpose of ADR Guidelines?\nA3: The purpose of ADR Guidelines is to help collect important questions to ask when making decisions on the topic being covered and to provide context for decisions that may otherwise seem to depend on the situation.\nWrong Answers: \n- The purpose of ADR Guidelines is to provide"}
{"prompt": "Context: you\u2019re facing a specific challenge. Table I-2 outlines the format for the ADR Guidelines and what you should expect from them.  Table I-2. ADR Guideline: Format Decision  Describes a decision that you might need to make when considering an aspect of this book.    Discussion Points  This section helps to identify the key discussions that you should be having when making a decision about your API architecture.  In this section we will reveal some of our experiences that may have influenced the decision. We will help you to identify the key information to inform your decision making process.    Recommendations  We will make specific recommendations that you should consider when creating your ADR, explaining the rationale behind why we are making a specific recommendation.      Summary   In this introduction we have provided a foundation with both the case study and the approach we will take to discussing API-driven architectures:  Architecture is an endless journey and APIs can play a major part in helping it evolve.    APIs are an abstraction of the implementation and can either be in process or out of process. Often architects find themselves in a position of evolving to out-of-process APIs, the onward focus of this book.    The conference case study is to describe and explain concepts. In this introduction you have seen a small evolutionary step to break out the Attendee service to address the upcoming business requirements.    You have seen the first three levels of C4\n\nQuestions:", "completion": "1. What is the purpose of the ADR Guidelines?\nCorrect Answer: The ADR Guidelines provide a set of guidelines for decision-making when considering an aspect of architecture. \nWrong Answers:\n- The ADR Guidelines provide a set of tools for building APIs. \n- The ADR Guidelines are a list of best practices for developing applications. \n- The ADR Guidelines provide a set of rules for creating a secure architecture."}
{"prompt": "Context: diagrams and their importance in sharing and communicating architecture.    ADRs provide a valuable record for making decisions and have both present and historical value in the lifetime of a project.    You have seen the structure for ADR Guidelines that will be used throughout the book to help facilitate decision making.      With the decision made to break the Attendee service out from the conference system, we will now explore the options for designing and specifying the Attendee API.      1 The intention is it will be the UI accessing the ingress point. However, it is open for potential exploit.  2 Local stubborn traits fueled this likely explanation.      Part I. Designing, Building, and Testing APIs   This section provides the foundational building blocks for API-driven architectures.  In Chapter 1 you will learn about REST and Remote Procedure Call (RPC)\u2013based APIs. We will explore specifications and schemas, recommended standards, strategies for versioning, and how to choose the right API for your system.  In Chapter 2 you will learn about testing APIs and how different test styles are best applied to API-driven architectures.      Chapter 1. Design, Build, and Specify APIs   You will be presented with many options when designing and building APIs. It is incredibly fast to build a service with modern technologies and frameworks, but creating a durable approach requires careful thought and consideration. In this chapter we will explore REST and RPC to model the\n\nQuestions:", "completion": "Q1. What type of APIs are typically used for API-driven architectures?\nA1. REST and Remote Procedure Call (RPC) \nWrong Answers: 1. GraphQL 2. JSON 3. XML\n\nQ2. What is the first step in designing and building APIs?\nA2. Exploring specifications and schemas \nWrong Answers: 1. Choosing a framework 2. Testing the APIs 3. Writing code\n\nQ3. What is an important step in choosing the right API for a system?\nA3. Careful thought and consideration \nWrong Answers: 1. Exploring specifications and schemas 2. Choosing a framework 3. Writing code\n\nQ4. What are some strategies for versioning APIs?\nA4. Major version, minor version, and patch level \nWrong Answers: 1. Major version and minor version 2. Major version and patch level 3. Minor version and patch level\n\nQ5. What are the two main components of the ADR Guidelines?\nA5. Present and historical value \nWrong Answers: 1. Design, build, and specify APIs 2. Testing APIs 3. Choosing the right API"}
{"prompt": "Context: producer and consumer relationships in the case study.  You will discover how standards can help to shortcut design decisions and navigate away from potential compatibility issues. You will look at OpenAPI Specifications, the practical uses for teams, and the importance of versioning.  RPC-based interactions are specified using a schema; to compare and contrast with a REST approach, we will explore gRPC. With both REST and gRPC in mind, we will look at the different factors to consider in how we model exchanges. We will look at the possibility of providing both a REST and RPC API in the same service and whether this is the right thing to do.      Case Study: Designing the Attendee API   In the Introduction we decided to migrate our legacy conference system and move toward a more API-driven architecture. As a first step to making this change, we are going to create a new Attendee service, which will expose a matching Attendee API. We also provided a narrow definition of an API. In order to design effectively, we need to consider more broadly the exchange between the producer and consumer, and more importantly who the producer and consumer are. The producer is owned by the attendee team. This team maintains two key relationships:  The attendee team owns the producer, and the legacy conference team owns the consumer. There is a close relationship between these two teams and any changes in structure are easily coordinated. A strong cohesion between the producer/consumer services\n\nQuestions:", "completion": "Q1: Who owns the producer of the Attendee API?\nA1: The attendee team owns the producer. \nWrong answers: The legacy conference team, the design team, the marketing team.\n\nQ2: What type of exchange is specified when using RPC-based interactions?\nA2: A schema.\nWrong answers: An API, a message, a request.\n\nQ3: Is it recommended to provide both a REST and RPC API in the same service?\nA3: It depends on the requirements and factors to consider.\nWrong answers: Yes, No, Maybe."}
{"prompt": "Context: is possible to achieve.    The attendee team owns the producer, and the external CFP system team owns the consumer. There is a relationship between the teams, but any changes need to be coordinated to not break the integration. A loose coupling is required and breaking changes would need to be carefully managed.      We will compare and contrast the approaches to designing and building the Attendee API throughout this chapter.      Introduction to REST   REpresentation State Transfer (REST) is a set of architectural constraints, most commonly applied using HTTP as the underlying transport protocol. Roy Fielding\u2019s dissertation \u201cArchitectural Styles and the Design of Network-based Software Architectures\u201d provides a complete definition of REST. From a practical perspective, to be considered RESTful your API must ensure that:  A producer-to-consumer interaction is modeled where the producer models resources the consumer can interact with.    Requests from producer to consumer are stateless, meaning that the producer doesn\u2019t cache details of a previous request. In order to build up a chain of requests on a given resource, the consumer must send any required information to the producer for processing.    Requests are cachable, meaning the producer can provide hints to the consumer where this is appropriate. In HTTP this is often provided in information contained in the header.    A uniform interface is conveyed to the consumer. You will explore the use of verbs, resources, and\n\nQuestions:", "completion": "Q1. What is REST? \nA1. REST stands for Representation State Transfer and is a set of architectural constraints, usually applied using HTTP as the underlying transport protocol. \nWrong Answers: \nA1. REST is a type of software architecture. \nA1. REST is a type of coding language.\nA1. REST is a set of tools used for debugging.\n\nQ2. How many architectural constraints does REST have? \nA2. REST has a set of architectural constraints. \nWrong Answers: \nA2. REST has four architectural constraints. \nA2. REST has six architectural constraints. \nA2. REST has eight architectural constraints. \n\nQ3. What is the purpose of the uniform interface in REST? \nA3. The uniform interface in REST is used to convey a consistent interface to the consumer. \nWrong Answers: \nA3. The uniform interface in REST is used to ensure the producer caches details of a previous request. \nA3. The uniform interface in REST is used to provide hints to the consumer where appropriate.\nA3. The uniform interface in REST is used to model resources that the consumer can interact with. \n\nQ4. What is the purpose of statelessness in REST? \nA4. The purpose of statelessness in REST is to ensure that the producer does not cache details of a"}
{"prompt": "Context: other patterns shortly.    It is a layered system, abstracting away the complexity of systems sitting behind the REST interface. For example, the consumer should not know or care if they\u2019re interacting with a database or other services.      Introduction to REST and HTTP by Example   Let\u2019s see an example of REST over HTTP. The following exchange is a GET request, where GET represents the method or verb. A verb such as GET describes the action to take on a particular resource; in this example, we consider the attendees resource. An Accept header is passed to define the type of content the consumer would like to retrieve. REST defines the notion of a representation in the body and allows for representation metadata to be defined in the headers.  In the examples in this chapter, we represent a request above the --- separator and a response below:   GET http://mastering-api.com/attendees Accept: application/json --- 200 OK Content-Type: application/json { \"displayName\": \"Jim\", \"id\": 1 }  The response includes the status code and message from the server, which enables the consumer to interrogate the result of the operation on the server-side resource. The status code of this request was a 200 OK, meaning the request was successfully processed by the producer. In the response body a JSON representation containing the conference attendees is returned. Many content types are valid for return from a REST, however it is important to consider if the content type is parsable by the\n\nQuestions:", "completion": "Question 1: What is REST?\nCorrect Answer: REST is an architectural style of software development that defines a set of constraints for creating web services. \nWrong Answers: \na) REST is a type of database\nb) REST is a programming language\nc) REST is a type of encryption"}
{"prompt": "Context: consumer. For example, returning application/pdf is valid but would not represent an exchange that could easily be used by another system. We will explore approaches to modeling content types, primarily looking at JSON, later in this chapter.      Note   REST is relatively straightforward to implement because the client and server relationship is stateless, meaning no client state is persisted by the server. The client must pass the context back to the server in subsequent requests; for example, a request for http://mastering-api.com/attendees/1 would retrieve more information on a specific attendee.      The Richardson Maturity Model   Speaking at QCon in 2008, Leonard Richardson presented his experiences of reviewing many REST APIs. Richardson found levels of adoption that teams apply to building APIs from a REST perspective. Martin Fowler also covered Richardson\u2019s maturity heuristics on his blog. Table 1-1 explores the different levels represented by Richardson\u2019s maturity heuristics and their application to RESTful APIs.  Table 1-1. Richardson maturity heuristics Level 0 - HTTP/RPC  Establishes that the API is built using HTTP and has the notion of a single URI. Taking our preceding example of /attendees and not applying a verb to specify intent, we would open up an endpoint for exchange. Essentially this represents an RPC implementation over the REST protocol.    Level 1 - Resources  Establishes the use of resources and starts to bring in the idea of modeling resources in\n\nQuestions:", "completion": "Q1. What is the Richardson Maturity Model?\nA1. The Richardson Maturity Model is a way to measure the adoption of the REST protocol for building APIs.\nWrong answers: \na. The Richardson Maturity Model is a way to measure the efficiency of API design.\nb. The Richardson Maturity Model is a way to track API usage. \nc. The Richardson Maturity Model is a way to measure the effectiveness of API development.\n\nQ2. What is an example of a Level 0 API?\nA2. A Level 0 API is an API that uses HTTP and has the notion of a single URI, such as /attendees without applying a verb to specify intent.\nWrong answers:\na. A Level 0 API is an API that uses JSON and has the notion of a single URI. \nb. A Level 0 API is an API that uses XML and has the notion of a single URI.\nc. A Level 0 API is an API that uses GraphQL and has the notion of a single URI. \n\nQ3. What is an example of content type modeling?\nA3. An example of content type modeling is using JSON to represent data.\nWrong answers:\na. An example of content type modeling is using HTML to represent data.\nb. An example of content type modeling is using XML to represent data.\nc. An example of content type"}
{"prompt": "Context: the context of the URI. In our example, if we added GET /attendees/1 returning a specific attendee, it would start to look like a level 1 API. Martin Fowler draws an analogy to the classic object-oriented world of introducing identity.    Level 2 - Verbs (Methods)  Starts to introduce the correct modeling of multiple resource URIs accessed by different request methods (also known as HTTP verbs) based on the effect of the resources on the server. An API at level 2 can make guarantees around GET methods not impacting server state and presenting multiple operations on the same resource URI. In our example adding DELETE /attendees/1, PUT /attendees/1 would start to add the notion of a level 2\u2013compliant API.    Level 3 - Hypermedia Controls  This is the epitome of REST design and involves navigable APIs by the use of HATEOAS (Hypertext As The Engine Of Application State). In our example, when we call GET /attendees/1, the response would contain the actions that are possible on the object returned from the server. This would include the option to be able to update the attendee or delete the attendee and what the client is required to invoke in order to do so. In practical terms level 3 is rarely used in modern RESTful HTTP services, and although the navigation is a benefit in flexible UI style systems, it doesn\u2019t suit interservice API calls. Using HATEOAS would be a chatty experience and is often short-circuited by having a complete specification of possible interactions up front\n\nQuestions:", "completion": "Q1: What is the highest level of a RESTful API?\nA1: Level 3 - Hypermedia Controls \nWrong Answers: \n- Level 1 - Resources \n- Level 2 - Verbs (Methods) \n- Level 4 - Data Modeling \n\nQ2: What is the purpose of HATEOAS?\nA2: The purpose of HATEOAS is to enable navigable APIs. \nWrong Answers:\n- To enable object-oriented programming \n- To enable data modeling \n- To enable multiple resource URIs"}
{"prompt": "Context: while programming against the producer.      When designing API exchanges, the different levels of Richardson Maturity are important to consider. Moving toward level 2 will enable you to project an understandable resource model to the consumer, with appropriate actions available against the model. In turn, this reduces coupling and hides the full detail of the backing service. Later we will also see how this abstraction is applied to versioning.  If the consumer is the CFP team, modeling an exchange with low coupling and projecting a RESTful model would be a good starting point. If the consumer is the legacy conference team, we may still choose to use a RESTful API, but there is also another option with RPC. In order to start to consider this type of traditionally east\u2013west modeling, we will explore RPC.      Introduction to Remote Procedure Call (RPC) APIs   A Remote Procedure Call (RPC) involves calling a method in one process but having it execute code in another process. While REST can project a model of the domain and provides an abstraction from the underlying technology to the consumer, RPC involves exposing a method from one process and allows it to be called directly from another.  gRPC is a modern open source high-performance RPC. gRPC is under stewardship of the Linux Foundation and is the de facto standard for RPC across most platforms. Figure 1-1 describes an RPC call in gRPC, which involves the legacy conference service invoking the remote method on the Attendee\n\nQuestions:", "completion": "Q1: What type of API exchange should be used when programming against the producer? \nA1: The best type of API exchange to use when programming against the producer is a RESTful model. \nWrong Answers: \n1. A flat file format \n2. A JSON format\n3. A SOAP format\n\nQ2: What is gRPC?\nA2: gRPC is a modern open source high-performance RPC. \nWrong Answers: \n1. A type of programming language \n2. An operating system \n3. A data storage system"}
{"prompt": "Context: service. The gRPC Attendee service starts and exposes a gRPC server on a specified port, allowing methods to be invoked remotely. On the client side (the legacy conference service), a stub is used to abstract the complexity of making the remote call into the library. gRPC requires a schema to fully cover the interaction between producer and consumer.      Figure 1-1. Example C4 component diagram using gRPC      A key difference between REST and RPC is state. REST is by definition stateless\u2014with RPC state depends on the implementation. RPC-based integrations in certain situations can also build up state as part of the exchange. This buildup of state has the convenience of high performance at the potential cost of reliability and routing complexities. With RPC the model tends to convey the exact functionality at a method level that is required from a secondary service. This optionality in state can lead to an exchange that is potentially more coupled between producer and consumer. Coupling is not always a bad thing, especially in east\u2013west services where performance is a key consideration.      A Brief Mention of GraphQL   Before we explore REST and RPC styles in detail, we would be remiss not to mention GraphQL and where it fits into the API world. RPC offers access to a series of individual functions provided by a producer but does not usually extend a model or abstraction to the consumer. REST, on the other hand, extends a resource model for a single API provided by the\n\nQuestions:", "completion": "Q1: What is the primary difference between REST and RPC?\nA1: The primary difference between REST and RPC is state; REST is stateless while RPC state depends on the implementation. \nWrong Answers:\n- The primary difference between REST and RPC is the method of communication; REST is based on HTTP while RPC is based on TCP. \n- The primary difference between REST and RPC is the protocol; REST is based on JSON while RPC is based on XML. \n- The primary difference between REST and RPC is the speed; REST is faster than RPC.\n\nQ2: What type of coupling can be caused by RPC-based integrations?\nA2: RPC-based integrations can cause high coupling between producer and consumer.\nWrong Answers:\n- RPC-based integrations can cause low coupling between producer and consumer. \n- RPC-based integrations can cause no coupling between producer and consumer. \n- RPC-based integrations can cause asynchronous coupling between producer and consumer.\n\nQ3: What is GraphQL used for?\nA3: GraphQL is used to provide access to a series of individual functions provided by a producer and to extend a resource model for a single API provided by the producer.\nWrong Answers:\n- GraphQL is used to provide access to a series of individual resources provided by a consumer. \n- GraphQL is used to provide access to a series of individual"}
{"prompt": "Context: producer. It is possible to offer multiple APIs on the same base URL using API gateways. We will explore this notion further in Chapter 3. If we offer multiple APIs in this way, the consumer will need to query sequentially to build up state on the client side. The consumer also needs to understand the structure of all services involved in the query. This approach is wasteful if the consumer is only interested in a subset of fields on the response. Mobile devices are constrained by smaller screens and network availability, so GraphQL is excellent in this scenario.  GraphQL introduces a technology layer over existing services, datastores, and APIs that provides a query language to query across multiple sources. The query language allows the client to ask for exactly the fields required, including fields that span across multiple APIs. GraphQL uses the GraphQL schema language, to specify the types in individual APIs and how APIs combine. One major advantage of introducing a GraphQL schema in your system is the ability to provide a single version across all APIs, removing the need for potentially complex version management on the consumer side.  GraphQL excels when a consumer requires uniform API access over a wide range of interconnected services. The schema provides the connection and extends the domain model, allowing the customer to specify exactly what is required on the consumer side. This works extremely well for modeling a user interface and also reporting systems or data\n\nQuestions:", "completion": "Q1. What is API gateways? \nA1. API Gateways are a layer between the client and the backend services that can route, manage, and secure API requests. \nWrong Answers: \na. API Gateways are a type of web server \nb. API Gateways are a type of database \nc. API Gateways are a type of software application\n\nQ2. What is GraphQL?\nA2. GraphQL is a query language that allows the client to ask for exactly the fields required, including fields that span across multiple APIs.\nWrong Answers:\na. GraphQL is a type of web server \nb. GraphQL is a type of database \nc. GraphQL is a type of programming language\n\nQ3. What are the advantages of introducing a GraphQL schema? \nA3. One major advantage of introducing a GraphQL schema in your system is the ability to provide a single version across all APIs, removing the need for potentially complex version management on the consumer side.\nWrong Answers:\na. The major advantage of introducing a GraphQL schema is the ability to store more data \nb. The major advantage of introducing a GraphQL schema is the ability to increase security \nc. The major advantage of introducing a GraphQL schema is the ability to reduce development time\n\nQ4. What is the GraphQL Schema Language used for?"}
{"prompt": "Context: warehousing\u2013style systems. In systems where vast amounts of data are stored across different subsystems, GraphQL can provide an ideal solution to abstracting away internal system complexity.  It is possible to place GraphQL over existing legacy systems and use this as a facade to hide away the complexity, though providing GraphQL over a layer of well-designed APIs often means the facade is simpler to implement and maintain. GraphQL can be thought of as a complementary technology and should be considered when designing and building APIs. GraphQL can also be thought of as a complete approach to building up an entire API ecosystem.  GraphQL shines in certain scenarios and we would encourage you to take a look at Learning GraphQL (O\u2019Reilly) and GraphQL in Action (O\u2019Reilly) for a deeper dive into this topic.      REST API Standards and Structure   REST has some very basic rules, but for the most part the implementation and design is left as an exercise for the developer. For example, what is the best way to convey errors? How should pagination be implemented? How do you accidentally avoid building an API where compatibility frequently breaks? At this point, it is useful to have a more practical definition around APIs to provide uniformity and expectations across different implementations. This is where standards or guidelines can help, however there are a variety of sources to choose from.  For the purposes of discussing design, we will use the Microsoft REST API Guidelines, which\n\nQuestions:", "completion": "Q1: What technology can be used to abstract away internal system complexity when dealing with vast amounts of data?\nA1: GraphQL. \nWrong Answers: \n- HTML\n- Java\n- SQL"}
{"prompt": "Context: represent a series of internal guidelines that have been open sourced. The guidelines use RFC-2119, which defines terminology for standards such as MUST, SHOULD, SHOULD NOT, MUST NOT, etc., allowing the developer to determine whether requirements are optional or mandatory.      Tip   As REST API standards are evolving, an open list of API standards are available on the book\u2019s Github page. Please contribute via pull request any open standards you think would be useful for other readers to consider.    Let\u2019s consider the design of the Attendee API using the Microsoft REST API Guidelines and introduce an endpoint to create a new attendee. If you are familiar with REST, the thought will immediately be to use POST:   POST http://mastering-api.com/attendees { \"displayName\": \"Jim\", \"givenName\": \"James\", \"surname\": \"Gough\", \"email\": \"jim@mastering-api.com\" } --- 201 CREATED Location: http://mastering-api.com/attendees/1  The Location header reveals the location of the new resource created on the server, and in this API we are modeling a unique ID for the user. It is possible to use the email field as a unique ID, however the Microsoft REST API Guidelines recommend in section 7.9 that personally identifiable information (PII) should not be part of the URL.      Warning   The reason for removing sensitive data from the URL is that paths or query parameters might be inadvertently cached in the network\u2014for example, in server logs or elsewhere.    Another aspect of APIs that can be\n\nQuestions:", "completion": "Q1. What is RFC-2119?\nA1. RFC-2119 is a document that defines terminology for standards such as MUST, SHOULD, SHOULD NOT, MUST NOT, etc. \nWrong Answers:\n- It is a protocol that governs how APIs should be designed.\n- It is a document that defines a set of rules for developing web applications.\n- It is a document that defines the structure of a REST API. \n\nQ2. What is the purpose of the Location header?\nA2. The Location header reveals the location of the new resource created on the server.\nWrong Answers:\n- It is used to store metadata about the resource.\n- It is used to send the response back to the client.\n- It is used to indicate the type of request that was sent. \n\nQ3. What is the recommendation from the Microsoft REST API Guidelines regarding personally identifiable information (PII)?\nA3. The Microsoft REST API Guidelines recommend that PII should not be part of the URL.\nWrong Answers:\n- PII should be included in the body of the request.\n- PII should be sent in a separate request.\n- PII should be encrypted. \n\nQ4. What is the purpose of removing sensitive data from the URL?\nA4. The purpose of removing sensitive data from the URL is to prevent it from being cached in the network,"}
{"prompt": "Context: difficult is naming. As we will discuss in \u201cAPI Versioning\u201d, something as simple as changing a name can break compatibility. There is a short list of standard names that should be used in the Microsoft REST API Guidelines, however teams should expand this to have a common domain data dictionary to supplement the standards. In many organizations it is incredibly helpful to proactively investigate the requirements around data design and in some cases governance. Organizations that provide consistency across all APIs offered by a company present a uniformity that enables consumers to understand and connect responses. In some domains there may already be widely known terminology\u2014use them!      Collections and Pagination   It seems reasonable to model the GET /attendees request as a response containing a raw array. The following source snippet shows an example of what that might look like as a response body:   GET http://mastering-api.com/attendees --- 200 OK [ { \"displayName\": \"Jim\", \"givenName\": \"James\", \"surname\": \"Gough\", \"email\": \"jim@mastering-api.com\", \"id\": 1, }, ... ]  Let\u2019s consider an alternative model to the GET /attendees request that nests the array of attendees inside an object. It may seem strange that an array response is returned in an object, however the reason for this is that it allows for us to model bigger collections and pagination. Pagination involves returning a partial result, while providing instructions for how the consumer can request the next set of\n\nQuestions:", "completion": "Question 1: What is API Versioning? \nCorrect Answer: API Versioning is the process of maintaining multiple versions of an API to ensure compatibility between the API and its consumers. \nWrong Answers: \n-API Versioning is the process of creating multiple APIs to ensure compatibility between the API and its consumers. \n-API Versioning is the process of creating multiple databases to ensure compatibility between the API and its consumers. \n-API Versioning is the process of creating multiple websites to ensure compatibility between the API and its consumers."}
{"prompt": "Context: results. This is reaping the benefits of hindsight; adding pagination later and converting from an array to an object in order to add a @nextLink (as recommended by the standards) would break compatibility:   GET http://mastering-api.com/attendees --- 200 OK { \"value\": [ { \"displayName\": \"Jim\", \"givenName\": \"James\", \"surname\": \"Gough\", \"email\": \"jim@mastering-api.com\", \"id\": 1, } ], \"@nextLink\": \"{opaqueUrl}\" }      Filtering Collections   Our conference is looking a little lonely with only one attendee, however when collections grow in size we may need to add filtering in addition to pagination. The filtering standard provides an expression language within REST to standardize how filter queries should behave, based upon the OData Standard. For example, we could find all attendees with the displayName Jim using:   GET http://mastering-api.com/attendees?$filter=displayName eq 'Jim'  It is not necessary to complete all filtering and searching features from the start. However, designing an API in line with the standards will allow the developer to support an evolving API architecture without breaking compatibility for consumers. Filtering and querying is a feature that GraphQL is really good at, especially if querying and filtering across many of your services becomes relevant.      Error Handling   An important consideration when extending APIs to consumers is defining what should happen in various error scenarios. Error standards are useful to define upfront and share with\n\nQuestions:", "completion": "1. What is an API?\n\nCorrect Answer: An application programming interface (API) is a set of protocols, routines, and tools for building software applications.\n\nWrong Answers: \nA) An API is a type of computer programming language.\nB) An API is a type of software development framework.\nC) An API is a type of database.\n\n2. What is pagination?\n\nCorrect Answer: Pagination is a technique used to divide large amounts of data into smaller chunks or pages.\n\nWrong Answers:\nA) Pagination is a type of computer programming language.\nB) Pagination is a type of software development framework.\nC) Pagination is a type of database.\n\n3. What is the OData Standard?\n\nCorrect Answer: The OData Standard is an open protocol for querying and updating data using web technologies.\n\nWrong Answers:\nA) The OData Standard is a type of computer programming language.\nB) The OData Standard is a type of software development framework.\nC) The OData Standard is a type of database.\n\n4. What is GraphQL?\n\nCorrect Answer: GraphQL is an open-source data query language that provides an efficient and flexible way to access data.\n\nWrong Answers:\nA) GraphQL is a type of computer programming language.\nB) GraphQL is a type"}
{"prompt": "Context: producers to provide consistency. It is important that errors describe to the consumer exactly what has gone wrong with the request, as this will avoid increasing the support required for the API.  The guidelines state \u201cFor non-success conditions, developers SHOULD be able to write one piece of code that handles errors consistently.\u201d An accurate status code must be provided to the consumer, because often consumers will build logic around the status code provided in the response. We have seen many APIs that return errors in the body along with a 2xx type of response, which is used to indicate success. 3xx status codes for redirects are actively followed by some consuming library implementations, enabling providers to relocate and access external sources. 4xx usually indicates a client-side error; at this point the content of the message field is extremely useful to the developer or end user. 5xx usually indicates a failure on the server side and some client libraries will retry on these types of failures. It is important to consider and document what happens in the service based on an unexpected failure\u2014for example, in a payment system does a 500 mean the payment has gone through or not?      Warning   Ensure that the error messages sent back to an external consumer do not contain stack traces and other sensitive information. This information can help a hacker aiming to compromise the system. The error structure in the Microsoft guidelines has the concept of an InnerError,\n\nQuestions:", "completion": "Q1. What kind of status code should be provided to the consumer when an error occurs?\n\nCorrect Answer: A 4xx status code should be provided to the consumer when an error occurs.\nWrong Answers: \nA) A 2xx status code should be provided to the consumer when an error occurs.\nB) A 3xx status code should be provided to the consumer when an error occurs.\nC) A 5xx status code should be provided to the consumer when an error occurs."}
{"prompt": "Context: which could be useful in which to place more detailed stack traces/descriptions of issues. This would be incredibly helpful for debugging but must be stripped prior to an external consumer.    We have just scratched the surface on building REST APIs, but clearly there are many important decisions to be made when beginning to build an API. If we combine the desire to present intuitive APIs that are consistent and allow for an evolving and compatible API, it is worth adopting an API standard early.      ADR Guideline: Choosing an API Standard   To make your decision on API standards, the guideline in Table 1-2 lists important topics to consider. There are a range of guidelines to choose from, including the Microsoft guidelines discussed in this section, and finding one that best matches the styles of APIs being produced is a key decision.  Table 1-2. API Standards Guideline Decision  Which API standard should we adopt?    Discussion Points  Does the organization already have other standards within the company? Can we extend those standards to external consumers?  Are we using any third-party APIs that we will need to expose to a consumer (e.g., Identity Services) that already have a standard?  What does the impact of not having a standard look like for our consumers?    Recommendations  Pick an API standard that best matches the culture of the organization and formats of APIs you may already have in the inventory.  Be prepared to evolve and add to a standard any\n\nQuestions:", "completion": "Q1: What is an API Standard? \nA1: An API Standard is a set of rules and guidelines for creating and developing APIs.\nWrong Answers: \na. An API Standard is a programming language \nb. An API Standard is a type of data structure \nc. An API Standard is a type of software development tool\n\nQ2: What is the purpose of an API Standard?\nA2: The purpose of an API Standard is to provide a consistent format for creating and developing APIs that can be used by any organization.\nWrong Answers:\na. The purpose of an API Standard is to provide a secure way to store data \nb. The purpose of an API Standard is to provide an easy way to create applications \nc. The purpose of an API Standard is to create a single unified platform for all APIs\n\nQ3: What are the benefits of using an API Standard?\nA3: The benefits of using an API Standard include improved readability, compatibility, and consistency across different APIs.\nWrong Answers:\na. The benefits of using an API Standard include improved security and performance \nb. The benefits of using an API Standard include improved scalability and flexibility \nc. The benefits of using an API Standard include improved development and debugging speed\n\nQ4: What are the important topics to consider when choosing an API Standard?\nA4: Important topics to consider when"}
{"prompt": "Context: domain/industry-specific amendments.  Start with something early to avoid having to break compatibility later for consistency.  Be critical of existing APIs. Are they in a format that consumers would understand or is more effort required to offer the content?      Specifying REST APIs Using OpenAPI   As we\u2019re beginning to see, the design of an API is fundamental to the success of an API platform. The next consideration we\u2019ll discuss is sharing the API with developers consuming our APIs.  API marketplaces provide a public or private listing of APIs available to a consumer. A developer can browse documentation and quickly try out an API in the browser to explore the API behavior and functionality. Public and private API marketplaces have placed REST APIs prominently into the consumer space. The success of REST APIs has been driven by both the technical landscape and the low barrier to entry for both the client and server.  As the number of APIs grew, it quickly became necessary to have a mechanism to share the shape and structure of APIs with consumers. This is why the OpenAPI Initiative was formed by API industry leaders to construct the OpenAPI Specification (OAS). Swagger was the original reference implementation of the OpenAPI Specifications, but most tooling has now converged on using OpenAPI.  The OpenAPI Specifications are JSON- or YAML-based representations of the API that describe the structure, the domain objects exchanged, and any security requirements of the API. In\n\nQuestions:", "completion": "Q1: What is the OpenAPI Initiative? \nA1: The OpenAPI Initiative is a collaboration between API industry leaders to create the OpenAPI Specification (OAS). \nWrong Answers: \nA1: The OpenAPI Initiative is a software development tool. \nA1: The OpenAPI Initiative is a database of APIs. \nA1: The OpenAPI Initiative is a collection of best practices for API design. \n\nQ2: What is the OpenAPI Specification? \nA2: The OpenAPI Specification (OAS) is a JSON- or YAML-based representation of an API that describes its structure, domain objects exchanged, and security requirements. \nWrong Answers: \nA2: The OpenAPI Specification is a database of APIs. \nA2: The OpenAPI Specification is a software development tool. \nA2: The OpenAPI Specification is a collection of best practices for API design. \n\nQ3: What is Swagger? \nA3: Swagger is the original reference implementation of the OpenAPI Specifications. \nWrong Answers: \nA3: Swagger is a software development tool. \nA3: Swagger is a database of APIs. \nA3: Swagger is a collection of best practices for API design. \n\nQ4: What is the purpose of API marketplaces"}
{"prompt": "Context: addition to the structure, they also convey metadata about the API, including any legal or licensing requirements, and also carry documentation and examples that are useful to developers consuming the API. OpenAPI Specifications are an important concept surrounding modern REST APIs, and many tools and products have been built around its usage.      Practical Application of OpenAPI Specifications   Once an OAS is shared, the power of the specification starts to become apparent. OpenAPI.Tools documents a full range of available open and closed source tools. In this section we will explore some of the practical applications of tools based on their interaction with the OpenAPI Specification.  In the situation where the CFP team is the consumer, sharing the OAS enables the team to understand the structure of the API. Using some of the following practical applications can help both improve the developer experience and ensure the health of the exchange.      Code Generation   Perhaps one of the most useful features of an OAS is allowing the generation of client-side code to consume the API. As discussed earlier, we can include the full details of the server, security, and of course the API structure itself. With all this information we can generate a series of model and service objects that represent and invoke the API. The OpenAPI Generator project supports a wide range of languages and toolchains. For example, in Java you can choose to use Spring or JAX-RS and in TypeScript you\n\nQuestions:", "completion": "Q1. What is an OpenAPI Specification?\nA1. An OpenAPI Specification (OAS) is a document that describes the structure of a REST API. \nWrong Answers:\n- It is a database for storing API data.\n- It is a programming language for building APIs.\n- It is an API client for making requests to an API.\n\nQ2. What are some practical applications of OpenAPI Specifications?\nA2. Code generation, API testing, API monitoring and analytics, and API documentation are some practical applications of OpenAPI Specifications. \nWrong Answers:\n- It is used to create graphical user interfaces.\n- It is used to manage servers and cloud computing.\n- It is used for security and authentication."}
{"prompt": "Context: can choose a combination of TypeScript with your favorite framework. It is also possible to generate the API implementation stubs from the OAS.  This raises an important question about what should come first\u2014the specification or the server-side code? In Chapter 2, we discuss \u201ccontract tracing,\u201d which presents a behavior-driven approach to testing and building APIs. The challenge with OpenAPI Specifications is that alone they only convey the shape of the API. OpenAPI Specifications do not fully model the semantics (or expected behavior) of the API under different conditions. If you are going to present an API to external users, it is important that the range of behaviors is modeled and tested to help avoid having to drastically change the API later.  APIs should be designed from the perspective of the consumer and consider the requirement to abstract the underlying representation to reduce coupling. It is important to be able to freely refactor components behind the scenes without breaking API compatibility, otherwise the API abstraction loses value.      OpenAPI Validation   OpenAPI Specifications are useful for validating the content of an exchange to ensure the request and response match the expectations of the specification. At first it might not seem apparent where this would be useful\u2014if code is generated, surely the exchange will always be right? One practical application of OpenAPI validation is in securing APIs and API infrastructure. In many organizations a zonal\n\nQuestions:", "completion": "Q1. What is OpenAPI?\nA1. OpenAPI is a language-agnostic specification for describing and documenting RESTful APIs. \nWrong answers:\n- OpenAPI is a software development tool.\n- OpenAPI is a type of database.\n- OpenAPI is a programming language.\n\nQ2. What is the purpose of OpenAPI Specifications?\nA2. The purpose of OpenAPI Specifications is to provide a common language for describing and documenting RESTful APIs.\nWrong answers: \n- The purpose of OpenAPI Specifications is to provide a graphical user interface for developing APIs.\n- The purpose of OpenAPI Specifications is to provide a way to store and manage data.\n- The purpose of OpenAPI Specifications is to provide a way to create visually appealing websites.\n\nQ3. What is contract tracing?\nA3. Contract tracing is a behavior-driven approach to testing and building APIs.\nWrong answers:\n- Contract tracing is a way to track changes to a contract.\n- Contract tracing is a type of encryption.\n- Contract tracing is a type of software development methodology."}
{"prompt": "Context: architecture is common, with a notion of a demilitarized zone (DMZ) used to shield a network from inbound traffic. A useful feature is to interrogate messages in the DMZ and terminate the traffic if the specification does not match. We will cover security in more detail in Chapter 6.  Atlassian, for example, open sourced a tool called the swagger-request-validator, which is capable of validating JSON REST content. The project also has adapters that integrate with various mocking and testing frameworks to help ensure that API Specifications are conformed to as part of testing. The tool has an OpenApiInteractionValidator, which is used to create a ValidationReport on an exchange. The following code demonstrates building a validator from the specification, including any basePathOverrides\u2014which may be necessary if deploying an API behind infrastructure that alters the path. The validation report is generated from analyzing the request and response at the point where validation is executed:   //Using the location of the specification create an interaction validator //The base path override is useful if the validator will be used //behind a gateway/proxy final OpenApiInteractionValidator validator = OpenApiInteractionValidator .createForSpecificationUrl(specUrl) .withBasePathOverride(basePathOverride) .build; //Requests and Response objects can be converted or created using a builder final ValidationReport report = validator.validate(request, response); if (report.hasErrors()) { //\n\nQuestions:", "completion": "Q1. What is a demilitarized zone (DMZ)?\nA1. A DMZ is a network security mechanism used to create a secure zone between a private network and the public internet.\nWrong Answers: \n- A DMZ is a network security mechanism used to create a secure zone between two private networks.\n- A DMZ is a secure zone between two public networks.\n- A DMZ is a secure zone between two private networks with no external access.\n\nQ2. What is the purpose of the swagger-request-validator?\nA2. The swagger-request-validator is a tool used to validate JSON REST content against an API specification.\nWrong Answers:\n- The swagger-request-validator is a tool used to generate an API specification.\n- The swagger-request-validator is a tool used to create a ValidationReport on an exchange. \n- The swagger-request-validator is a tool used to integrate with various mocking and testing frameworks.\n\nQ3. What does the OpenApiInteractionValidator do?\nA3. The OpenApiInteractionValidator is used to create a ValidationReport on an exchange.\nWrong Answers:\n- The OpenApiInteractionValidator is used to create an OpenApi Interaction. \n- The OpenApiInteractionValidator is"}
{"prompt": "Context: Capture or process error information }      Examples and Mocking   The OAS can provide example responses for the paths in the specification. Examples, as we\u2019ve discussed, are useful for documentation to help developers understand the expected API behavior. Some products have started to use examples to allow the user to query the API and return example responses from a mock service. This can be really useful in features such as a developer portal, which allows developers to explore documentation and invoke APIs. Another useful feature of mocks and examples is the ability to share ideas between the producer and consumer ahead of committing to build the service. Being able to \u201ctry out\u201d the API is often more valuable than trying to review if a specification would meet your requirements.  Examples can potentially introduce an interesting problem, which is that this part of the specification is essentially a string (in order to model XML/JSON, etc.). openapi-examples-validator validates that an example matches the OAS for the corresponding request/response component of the API.      Detecting Changes   OpenAPI Specifications can also be helpful in detecting changes in an API. This can be incredibly useful as part of a DevOps pipeline. Detecting changes for backward compatibility is very important, but first we need to understand versioning of APIs in more detail.      API Versioning   We have explored the advantages of sharing an OAS with a consumer, including the speed of\n\nQuestions:", "completion": "1. What is the purpose of OpenAPI Specifications?\nCorrect Answer: The purpose of OpenAPI Specifications is to provide a standard, language-agnostic interface to RESTful APIs which allows both humans and computers to discover and understand the capabilities of the service without access to source code, documentation, or network traffic inspection.\nWrong Answers:\na. The purpose of OpenAPI Specifications is to provide a visualization of a RESTful API.\nb. The purpose of OpenAPI Specifications is to create a secure connection between two different systems.\nc. The purpose of OpenAPI Specifications is to allow developers to write code faster.\n\n2. What is OpenAPI Examples Validator used for?\nCorrect Answer: OpenAPI Examples Validator is used to validate that an example matches the OAS for the corresponding request/response component of the API.\nWrong Answers:\na. OpenAPI Examples Validator is used to create a secure connection between two different systems.\nb. OpenAPI Examples Validator is used to generate a visual representation of an API.\nc. OpenAPI Examples Validator is used to detect changes in an API.\n\n3. How can OpenAPI Specifications be used for versioning APIs?\nCorrect Answer: OpenAPI Specifications can be used for versioning APIs by detecting changes for backward compatibility.\nWrong Answers:\na. OpenAPI Specifications can be used for versioning APIs by creating a secure connection between two different systems."}
{"prompt": "Context: integration. Consider the case where multiple consumers start to operate against the API. What happens when there is a change to the API or one of the consumers requests the addition of new features to the API?  Let\u2019s take a step back and think about if this was a code library built into our application at compile time. Any changes to the library would be packaged as a new version and until the code is recompiled and tested against the new version, there would be no impact to production applications. As APIs are running services, we have a few upgrade options that are immediately available to us when changes are requested:  Release a new version and deploy in a new location.  Older applications continue to operate against the older version of the APIs. This is fine from a consumer perspective, as the consumer only upgrades to the new location and API if they need the new features. However, the owner of the API needs to maintain and manage multiple versions of the API, including any patching and bug fixing that might be necessary.    Release a new version of the API that is backward compatible with the previous version of the API.  This allows additive changes without impacting existing users of the API. There are no changes required by the consumer, but we may need to consider downtime or availability of both old and new versions during the upgrade. If there is a small bug fix that changes something as small as an incorrect field name, this would break compatibility.    Break\n\nQuestions:", "completion": "Question 1: What are the different options for upgrading an API?\n\nCorrect Answer: Release a new version and deploy in a new location, release a new version of the API that is backward compatible with the previous version of the API, and break compatibility with the previous version and upgrade.\n\nWrong Answers: Update the API in place, modify the existing API, and make no changes. \n\nQuestion 2: What is the main benefit of releasing a new version of the API that is backward compatible with the previous version?\n\nCorrect Answer: There are no changes required by the consumer.\n\nWrong Answers: Allows for additive features, eliminates the need for maintenance, and improves performance."}
{"prompt": "Context: compatibility with the previous API and all consumers must upgrade code to use the new API.  This seems like an awful idea at first, as that would result in things breaking unexpectedly in production.1 However, a situation may present itself where we cannot avoid breaking compatibility with older versions. This type of change can trigger a whole-system lockstep change that requires coordination of downtime.      The challenge is that all of these different upgrade options offer advantages but also drawbacks either to the consumer or the producer. The reality is that we want to be able to support a combination of all three options. In order to do this we need to introduce rules around versioning and how versions are exposed to the consumer.      Semantic Versioning   Semantic versioning offers an approach that we can apply to REST APIs to give us a combination of the preceding upgrade options. Semantic versioning defines a numerical representation attributed to an API release. That number is based on the change in behavior in comparison to the previous version, using the following rules:  A major version introduces noncompatible changes with previous versions of the API. In an API platform, upgrading to a new major version is an active decision by the consumer. There is likely going to be a migration guide and tracking as consumers upgrade to the new API.    A minor version introduces a backward compatible change with the previous version of the API. In an API service\n\nQuestions:", "completion": "Q1: What is Semantic Versioning?\nA1: Semantic versioning is an approach that can be applied to REST APIs to give us a combination of upgrade options.\nWrong Answers:\n- Semantic versioning is an approach that can be applied to databases to give us a combination of upgrade options.\n- Semantic versioning is an approach that can be applied to web applications to give us a combination of upgrade options.\n- Semantic versioning is an approach that can be applied to mobile applications to give us a combination of upgrade options.\n\nQ2: What are the two types of versioning that are used in Semantic Versioning?\nA2: Major versioning and minor versioning.\nWrong Answers:\n- Major versioning and patch versioning. \n- Minor versioning and patch versioning.\n- Major versioning and micro versioning.\n\nQ3: What is the difference between major and minor versioning?\nA3: Major versioning introduces noncompatible changes with previous versions of the API, while minor versioning introduces a backward compatible change with the previous version of the API.\nWrong Answers:\n- Major versioning introduces a backward compatible change with previous versions of the API, while minor versioning introduces noncompatible changes with the previous version of the API.\n- Major versioning introduces changes with previous versions of the API, while minor versioning introduces no changes with the"}
{"prompt": "Context: platform, it is acceptable for consumers to receive minor versions without making an active change on the client side.    A patch version does not change or introduce new functionality but is used for bug fixes on an existing Major.Minor version of functionality.      Formatting for semantic versioning can be represented as Major.Minor.Patch. For example, 1.5.1 would represent major version 1, minor version 5, with patch upgrade of 1. In Chapter 5 you will explore how semantic versioning connects with the concept of API lifecycle and releases.      OpenAPI Specification and Versioning   Now that we have explored versioning we can look at examples of breaking changes and nonbreaking changes using the Attendee API specification. There are several tools to choose from to compare specifications, and in this example we will use openapi-diff from OpenAPITools.  We will start with a breaking change: we will change the name of the givenName field to firstName. This is a breaking change because consumers will be expecting to parse givenName, not firstName. We can run the diff tool from a docker container using the following command:   $docker run --rm -t \\ -v $(pwd):/specs:ro \\ openapitools/openapi-diff:latest /specs/original.json /specs/first-name.json ========================================================================== ... - GET /attendees Return Type: - Changed 200 OK Media types: - Changed */* Schema: Broken compatibility Missing property: [n].givenName (string)\n\nQuestions:", "completion": "Q1: What does the abbreviation \"API\" stand for?\nA1: API stands for Application Programming Interface. \nWrong Answers: a) Automated Programming Interface, b) Application Programming Integrator, c) Automated Programing Integrator. \n\nQ2: What is the purpose of semantic versioning?\nA2: The purpose of semantic versioning is to communicate the changes between successive versions of a software program. \nWrong Answers: a) To set the version number of a software program, b) To ensure that all software programs are compatible with each other, c) To ensure that software programs are secure. \n\nQ3: What is the format of semantic versioning?\nA3: The format of semantic versioning is Major.Minor.Patch. \nWrong Answers: a) Major.Minor.Major, b) Major.Minor.Release, c) Major.Release.Patch. \n\nQ4: What is the purpose of openapi-diff?\nA4: The purpose of openapi-diff is to compare two versions of an OpenAPI Specification. \nWrong Answers: a) To create OpenAPI Specifications, b) To debug OpenAPI Specifications, c) To secure OpenAPI Specifications. \n\nQ5: What is the result of changing the name of the givenName field to firstName?\nA5: The result of changing the name"}
{"prompt": "Context: -------------------------------------------------------------------------- -- Result -- -------------------------------------------------------------------------- API changes broke backward compatibility --------------------------------------------------------------------------  We can try to add a new attribute to the /attendees return type to add an additional field called age. Adding new fields does not break existing behavior and therefore does not break compatibility:   $ docker run --rm -t \\ -v $(pwd):/specs:ro \\ openapitools/openapi-diff:latest --info /specs/original.json /specs/age.json ========================================================================== ... - GET /attendees Return Type: - Changed 200 OK Media types: - Changed */* Schema: Backward compatible -------------------------------------------------------------------------- -- Result -- -------------------------------------------------------------------------- API changes are backward compatible --------------------------------------------------------------------------  It is worth trying this out to see which changes would be compatible and which would not. Introducing this type of tooling as part of the API pipeline is going to help avoid unexpected noncompatible changes for consumers. OpenAPI Specifications are an important part of an API program, and when combined with tooling, versioning, and lifecycle, they are invaluable.      Warning   Tools are often OpenAPI version\u2013specific, so it is important\n\nQuestions:", "completion": "Q1: What is the purpose of OpenAPI Specifications? \nA1: The purpose of OpenAPI Specifications is to provide a machine-readable interface to describe an API and its operations. \nWrong Answers: \nA1a: The purpose of OpenAPI Specifications is to provide a graphical user interface for developers.\nA1b: The purpose of OpenAPI Specifications is to provide a secure connection for data exchange. \nA1c: The purpose of OpenAPI Specifications is to provide a unified data model for all APIs. \n\nQ2: What is the advantage of using tooling with OpenAPI Specifications? \nA2: The advantage of using tooling with OpenAPI Specifications is that it helps to avoid unexpected non-compatible changes for consumers.\nWrong Answers: \nA2a: The advantage of using tooling with OpenAPI Specifications is that it helps to reduce the development time. \nA2b: The advantage of using tooling with OpenAPI Specifications is that it helps to provide a secure connection for data exchange. \nA2c: The advantage of using tooling with OpenAPI Specifications is that it helps to provide a unified data model for all APIs. \n\nQ3: What is the most important part of an API program?\nA3: Versioning is the most important part of an API program. \nWrong Answers:\nA3a: Security is the most important"}
{"prompt": "Context: to check whether the tool supports the specification you are working with. In the preceding example we tried the diff tool with an earlier version of a spec and no breaking changes were detected.      Implementing RPC with gRPC   East\u2013west services such as Attendee tend to be higher traffic and can be implemented as microservices used across the architecture. gRPC may be a more suitable tool than REST for east\u2013west services, owing to the smaller data transmission and speed within the ecosystem. Any performance decisions should always be measured in order to be informed.  Let\u2019s explore using a Spring Boot Starter to rapidly create a gRPC server. The following .proto file models the same attendee object that we explored in our OpenAPI Specification example. As with OpenAPI Specifications, generating code from a schema is quick and supported in multiple languages.  The attendees .proto file defines an empty request and returns a repeated Attendee response. In protocols used for binary representations, it is important to note that the position and order of fields is critical, as they govern the layout of the message. Adding a new service or new method is backward compatible as is adding a field to a message, but care is required. Any new fields that are added must not be mandatory fields, otherwise backward compatibility would break.  Removing a field or renaming a field will break compatibility, as will changing the data type of a field. Changing the field number is also an\n\nQuestions:", "completion": "Q1. What tool can be used to check whether a specification is compatible with an earlier version?\nA1. Diff Tool (Wrong answers: REST, gRPC, OpenAPI Specification)\n\nQ2. What is required to add a new field to a message in a binary representation?\nA2. The field must not be mandatory (Wrong answers: The field must be mandatory, The field must be renamed, The data type of the field must be changed)\n\nQ3. What happens when a field is removed from a binary representation?\nA3. Backward compatibility would break (Wrong answers: Backward compatibility is maintained, The data type of the field changes, The field is renamed)\n\nQ4. What does gRPC provide compared to REST?\nA4. Smaller data transmission and speed (Wrong answers: Larger data transmission and speed, More secure connections, Improved performance)"}
{"prompt": "Context: issue as field numbers are used to identify fields on the wire. The restrictions of encoding with gRPC mean the definition must be very specific. REST and OpenAPI are quite forgiving as the specification is only a guide.2 Extra fields and ordering do not matter in OpenAPI, and therefore versioning and compatibility is even more important when it comes to gRPC:   syntax = \"proto3\"; option java_multiple_files = true; package com.masteringapi.attendees.grpc.server; message AttendeesRequest { } message Attendee { int32 id = 1; string givenName = 2; string surname = 3; string email = 4; } message AttendeeResponse { repeated Attendee attendees = 1; } service AttendeesService { rpc getAttendees(AttendeesRequest) returns (AttendeeResponse); }  The following Java code demonstrates a simple structure for implementing the behavior on the generated gRPC server classes:   @GrpcService public class AttendeesServiceImpl extends AttendeesServiceGrpc.AttendeesServiceImplBase { @Override public void getAttendees(AttendeesRequest request, StreamObserver<AttendeeResponse> responseObserver) { AttendeeResponse.Builder responseBuilder = AttendeeResponse.newBuilder(); //populate response responseObserver.onNext(responseBuilder.build()); responseObserver.onCompleted(); } }  You can find the Java service modeling this example on this book\u2019s GitHub page. gRPC cannot be queried directly from a browser without additional libraries, however you can install gRPC UI to use the browser for testing. grpcurl\n\nQuestions:", "completion": "Q1. What is gRPC? \nA1. gRPC is a remote procedure call protocol that uses HTTP/2 for communications. \nWrong Answers: \nA1a. gRPC is a web service protocol. \nA1b. gRPC is a programming language. \nA1c. gRPC is an application framework. \n\nQ2. How is versioning handled in gRPC?\nA2. Versioning is handled in gRPC through the use of field numbers to identify fields on the wire.\nWrong Answers: \nA2a. Versioning is handled in gRPC through the use of syntax. \nA2b. Versioning is handled in gRPC through the use of OpenAPI. \nA2c. Versioning is handled in gRPC through the use of option flags. \n\nQ3. How do you implement the behavior on a generated gRPC server class? \nA3. The behavior on a generated gRPC server class is implemented using Java code. \nWrong Answers: \nA3a. The behavior on a generated gRPC server class is implemented using HTML. \nA3b. The behavior on a generated gRPC server class is implemented using C++. \nA3c. The behavior on a generated gRPC server class is implemented using"}
{"prompt": "Context: also provides a command-line tool:   $ grpcurl -plaintext localhost:9090 \\ com.masteringapi.attendees.grpc.server.AttendeesService/getAttendees { \"attendees\": [ { \"id\": 1, \"givenName\": \"Jim\", \"surname\": \"Gough\", \"email\": \"gough@mail.com\" } ] }  gRPC gives us another option for querying our service and defines a specification for the consumer to generate code. gRPC has a more strict specification than OpenAPI and requires methods/internals to be understood by the consumer.      Modeling Exchanges and Choosing an API Format   In the Introduction we discussed the concept of traffic patterns and the difference between requests originating from outside the ecosystem and requests within the ecosystem. Traffic patterns are an important factor in determining the appropriate format of API for the problem at hand. When we have full control over the services and exchanges within our microservices-based architecture, we can start to make compromises that we would not be able to make with external consumers.  It is important to recognize that the performance characteristics of an east\u2013west service are likely to be more applicable than a north\u2013south service. In a north\u2013south exchange, traffic originating from outside the producer\u2019s environment will generally involve the exchange using the internet. The internet introduces a high degree of latency, and an API architecture should always consider the compounding effects of each service. In a microservices-based architecture it is likely that\n\nQuestions:", "completion": "Q1: What is gRPC?\nA1: gRPC is a remote procedure call protocol used to facilitate communication between services. \nWrong Answers:\n- gRPC is a programming language\n- gRPC is a database\n- gRPC is a web framework\n\nQ2: What is an API format?\nA2: An API format is the structure of a request and response between a client and server.\nWrong Answers:\n- An API format is a programming language\n- An API format is a database\n- An API format is a web framework\n\nQ3: What is an eastwest service?\nA3: An eastwest service is a service that handles communication between services within the same ecosystem.\nWrong Answers:\n- An eastwest service is a service that handles communication between services outside the same ecosystem\n- An eastwest service is a programming language\n- An eastwest service is a database"}
{"prompt": "Context: one north\u2013south request will involve multiple east\u2013west exchanges. High east\u2013west traffic exchange needs to be efficient to avoid cascading slow-downs propagating back to the consumer.      High-Traffic Services   In our example, Attendees is a central service. In a microservices-based architecture, components will keep track of an attendeeId. APIs offered to consumers will potentially retrieve data stored in the Attendee service, and at scale it will be a high-traffic component. If the exchange frequency is high between services, the cost of network transfer due to payload size and limitations of one protocol versus another will be more profound as usage increases. The cost can present itself in either monetary costs of each transfer or the total time taken for the message to reach the destination.      Large Exchange Payloads   Large payload sizes may also become a challenge in API exchanges and are susceptible to decreasing transfer performance across the wire. JSON over REST is human readable and will often be more verbose than a fixed or binary representation fuelling an increase in payload sizes.      Tip   A common misconception is that \u201chuman readability\u201d is quoted as a primary reason to use JSON in data transfers. The number of times a developer will need to read a message versus the performance consideration is not a strong case with modern tracing tools. It is also rare that large JSON files will be read from beginning to end. Better logging and error handling can\n\nQuestions:", "completion": "1. What is the main challenge with high eastwest traffic exchange?\nCorrect Answer: High eastwest traffic exchange can cause cascading slow-downs propagating back to the consumer.\nWrong Answers: \na. High eastwest traffic exchange can cause data loss. \nb. High eastwest traffic exchange can cause lower latency. \nc. High eastwest traffic exchange can cause higher throughput. \n\n2. What type of service is typically a high-traffic component? \nCorrect Answer: Attendee service. \nWrong Answers:\na. Database service \nb. File storage service \nc. Messaging service \n\n3. What is the primary reason for using JSON in data transfers?\nCorrect Answer: Human readability. \nWrong Answers:\na. Improved logging and error handling \nb. Faster transfer performance \nc. Fixed or binary representation"}
{"prompt": "Context: mitigate the human-readable argument.    Another factor in large payload exchanges is the time it takes components to parse the message content into language-level domain objects. Performance time of parsing data formats varies vastly depending on the language a service is implemented in. Many traditional server-side languages can struggle with JSON compared to a binary representation, for example. It is worth exploring the impact of parsing and include that consideration when choosing an exchange format.      HTTP/2 Performance Benefits   Using HTTP/2-based services can help to improve performance of exchanges by supporting binary compression and framing. The binary framing layer is transparent to the developer, but behind the scenes will split and compress the message into smaller chunks. The advantage of binary framing is that it allows for a full request and response multiplexing over a single connection. Consider processing a list in another service and the requirement is to retrieve 20 different attendees; if we retrieved these as individual HTTP/1 requests it would require the overhead of creating 20 new TCP connections. Multiplexing allows us to perform 20 individual requests over a single HTTP/2 connection.  gRPC uses HTTP/2 by default and reduces the size of exchange by using a binary protocol. If bandwidth is a concern or cost, then gRPC will provide an advantage, in particular as content payloads increase significantly in size. gRPC may be beneficial compared to\n\nQuestions:", "completion": "Q1: What protocol does gRPC use? \nA1: gRPC uses HTTP/2 by default. \nWrong Answers: \na) gRPC uses TCP by default. \nb) gRPC uses UDP by default. \nc) gRPC uses HTTP/1 by default."}
{"prompt": "Context: REST if payload bandwidth is a cumulative concern or the service exchanges large volumes of data. If large volumes of data exchanges are frequent, it is also worth considering some of the asynchronous capabilities of gRPC.      Tip   HTTP/3 is on the way and it will change everything. HTTP/3 uses QUIC, a transport protocol built on UDP. You can find out more in HTTP/3 explained.      Vintage Formats   Not all services in an architecture will be based on a modern design. In Chapter 8 we will look at how to isolate and evolve vintage components, as older components will be an active consideration for evolving architectures. It is important that those involved with an API architecture understand the overall performance impact of introducing vintage components.      Guideline: Modeling Exchanges   When the consumer is the legacy conference system team, the exchange is typically an east\u2013west relationship. When the consumer is the CFP team, the exchange is typically a north\u2013south relationship. The difference in coupling and performance requirements will require the teams to consider how the exchange is modeled. You will see some aspects for consideration in the guideline shown in Table 1-3.  Table 1-3. Modeling exchanges guideline Decision  What format should we use to model the API for our service?    Discussion Points  Is the exchange a north\u2013south or east\u2013west exchange? Are we in control of the consumer code?  Is there a strong business domain across multiple services or do we\n\nQuestions:", "completion": "1.What format should we use to model the API for our service?\nCorrect Answer: REST\nWrong Answers: HTML, CSS, JavaScript\n\n2.What is the difference between a northsouth and eastwest exchange?\nCorrect Answer: A northsouth exchange involves two or more components that communicate with each other in a linear fashion, while an eastwest exchange involves components that communicate with each other in a non-linear fashion. \nWrong Answers: Northsouth exchanges involve more components, Eastwest exchanges involve fewer components, Northsouth exchanges involve more data\n\n3.What is the latest version of HTTP?\nCorrect Answer: HTTP/3\nWrong Answers: HTTP/2, HTTP/1.1, HTTP/1\n\n4.How can we isolate and evolve vintage components in an API architecture?\nCorrect Answer: It is important to understand the overall performance impact of introducing vintage components and create strategies to ensure they are isolated and evolved.\nWrong Answers: Ignore them, Remove them, Replace them\n\n5.What protocol is used by HTTP/3?\nCorrect Answer: QUIC\nWrong Answers: TCP, UDP, HTTP"}
{"prompt": "Context: want to allow consumers to construct their own queries?  What versioning considerations do we need to have?  What is the deployment/change frequency of the underlying data model?  Is this a high-traffic service where bandwidth or performance concerns have been raised?    Recommendations  If the API is consumed by external users, REST is a low barrier to entry and provides a strong domain model. External users also usually means that a service with loose coupling and low dependency is desirable.  If the API is interacting between two services under close control of the producer or the service is proven to be high traffic, consider gRPC.      Multiple Specifications   In this chapter we have explored a variety of API formats to consider in an API architecture, and perhaps the final question is \u201cCan we provide all formats?\u201d The answer is yes, we can support an API that has a RESTful presentation, a gRPC service, and connections into a GraphQL schema. However, it is not going to be easy and may not be the right thing to do. In this final section, we will explore some of the options available for a multiformat API and the challenges it can present.      Does the Golden Specification Exist?   The .proto file for attendees and the OpenAPI Specification do not look too dissimilar; they contain the same fields and both have data types. Is it possible to generate a .proto file from an OAS using the openapi2proto tool? Running openapi2proto --spec spec-v2.json will output the .proto\n\nQuestions:", "completion": "Q1. What is the best format to use for an API?\nA1. The best format to use for an API is based on the requirements of the system and the preferences of the consumer.\nWrong Answers: \n- HTML\n- CSS\n- Java\n\nQ2. What is the best way to provide multiple API formats?\nA2. The best way to provide multiple API formats is to create a multiformat API with a RESTful presentation, a gRPC service, and connections into a GraphQL schema.\nWrong Answers: \n- Have a single version of the API\n- Generate a .proto file from an OAS using the openapi2proto tool\n- Use HTML or CSS"}
{"prompt": "Context: file with fields ordered alphabetically by default. This is fine until we add a new field to the OAS that is backward compatible and suddenly the ID of all fields changes, breaking backward compatibility.  The following sample .proto file shows that adding a_new_field would be alphabetically added to the beginning, changing the binary format and breaking existing services:   message Attendee { string a_new_field = 1; string email = 2; string givenName = 3; int32 id = 4; string surname = 5; }      Note   Other tools are available to solve the specification conversion problem, however it is worth noting that some tools only support OpenAPI Specification version 2. The time taken to move between versions 2 and 3 in some of the tools built around OpenAPI has led to many products needing to support both versions of the OAS.    An alternative option is grpc-gateway, which generates a reverse proxy providing a REST facade in front of the gRPC service. The reverse proxy is generated at build time against the .proto file and will produce a best-effort mapping to REST, similar to openapi2proto. You can also supply extensions within the .proto file to map the RPC methods to a nice representation in the OAS:   import \"google/api/annotations.proto\"; //... service AttendeesService { rpc getAttendees(AttendeesRequest) returns (AttendeeResponse) {         option(google.api.http) = {             get: \"/attendees\"       }; }  Using grpc-gateway gives us another option for presenting both a\n\nQuestions:", "completion": "Q1: Which tool is used to generate a reverse proxy providing a REST facade in front of the gRPC service?\nA1: grpc-gateway \nWrong Answers: \n- OpenAPI Specification\n- openapi2proto\n- Apache Thrift \n\nQ2: What is the purpose of importing the \"google/api/annotations.proto\" file?\nA2: To map the RPC methods to a nice representation in the OpenAPI Specification \nWrong Answers: \n- To generate a reverse proxy providing a REST facade in front of the gRPC service \n- To order fields alphabetically by default \n- To convert from OpenAPI Specification version 2 to 3"}
{"prompt": "Context: REST and gRPC service. However, grpc-gateway involves several commands and a setup that would only be familiar to developers who work with the Go language or build environment.      Challenges of Combined Specifications   It\u2019s important to take a step back here and consider what we are trying to do. When converting from OpenAPI we are effectively trying to convert our RESTful representation into a gRPC series of calls. We are trying to convert an extended hypermedia domain model into a lower-level function-to-function call. This is a potential conflation of the difference between RPC and APIs and is likely going to result in wrestling with compatibility.  With converting gRPC to OpenAPI we have a similar issue; the objective is trying to take gRPC and make it look like a REST API. This is likely going to create a difficult series of issues when evolving the service.  Once specifications are combined or generated from one another, versioning becomes a challenge. It is important to be mindful of how both the gRPC and OpenAPI Specifications maintain their individual compatibility requirements. An active decision should be made as to whether coupling the REST domain to an RPC domain makes sense and adds overall value.  Rather than generate RPC for east\u2013west from north\u2013south, what makes more sense is to carefully design the microservices-based architecture (RPC) communication independently from the REST representation, allowing both APIs to evolve freely. This is the choice we\n\nQuestions:", "completion": "Q1. What is the main challenge when converting from OpenAPI to gRPC? \nA1. The main challenge when converting from OpenAPI to gRPC is trying to convert an extended hypermedia domain model into a lower-level function-to-function call.\nWrong Answers:\n- The main challenge when converting from OpenAPI to gRPC is trying to convert a gRPC series of calls into an extended hypermedia domain model.\n- The main challenge when converting from OpenAPI to gRPC is setting up grpc-gateway.\n- The main challenge when converting from OpenAPI to gRPC is versioning. \n\nQ2. What should be taken into account when combining specifications? \nA2. When combining specifications, it is important to consider whether coupling the REST domain to an RPC domain makes sense and adds overall value.\nWrong Answers:\n- When combining specifications, it is important to consider whether coupling the REST domain to an OpenAPI domain makes sense and adds overall value.\n- When combining specifications, it is important to consider whether coupling the gRPC domain to an RPC domain makes sense and adds overall value.\n- When combining specifications, it is important to consider whether coupling the OpenAPI domain to an RPC domain makes sense and adds overall value.\n\nQ3. What is the best approach for maintaining compatibility between gRPC and OpenAPI Specifications? \nA3"}
{"prompt": "Context: have made for the conference case study and would be recorded as an ADR in the project.      Summary   In this chapter we have covered how to design, build, and specify APIs and the different circumstances under which you may choose REST or gRPC. It is important to remember that it is not REST versus gRPC, but rather given the situations, which is the most appropriate choice for modeling the exchange. The key takeaways are:  The barrier to building REST- and RPC-based APIs is low in most technologies. Carefully considering the design and structure is an important architectural decision.    When choosing between REST and RPC models, consider the Richardson Maturity Model and the degree of coupling between the producer and consumer.    REST is a fairly loose standard. When building APIs, conforming to an agreed API standard ensures your APIs are consistent and have the expected behavior for your consumers. API standards can also help to short-circuit potential design decisions that could lead to an incompatible API.    OpenAPI Specifications are a useful way of sharing API structure and automating many coding-related activities. You should actively select OpenAPI features and choose what tooling or generation features will be applied to projects.    Versioning is an important topic that adds complexity for the producer but is necessary to ease API usage for the consumer. Not planning for versioning in APIs exposed to consumers is dangerous. Versioning should be an active\n\nQuestions:", "completion": "Q1: What is the Richardson Maturity Model?\nA1: The Richardson Maturity Model is a way of analyzing the degree of coupling between the producer and consumer of an API. \nWrong Answers:\n- The Richardson Maturity Model is a way of analyzing the performance of an API. \n- The Richardson Maturity Model is a way of analyzing the scalability of an API. \n- The Richardson Maturity Model is a way of analyzing the security of an API. \n\nQ2: What are OpenAPI Specifications?\nA2: OpenAPI Specifications are a useful way of sharing API structure and automating many coding-related activities.\nWrong Answers:\n- OpenAPI Specifications are a way of setting up secure APIs. \n- OpenAPI Specifications are a way of setting up secure user accounts. \n- OpenAPI Specifications are a way of testing APIs. \n\nQ3: What is the purpose of versioning an API?\nA3: The purpose of versioning an API is to ease usage for the consumer and provide an easier way to manage and maintain the API. \nWrong Answers:\n- The purpose of versioning an API is to make sure it is secure. \n- The purpose of versioning an API is to improve its performance. \n- The purpose of versioning an API is to make sure it is up-to-date. \n\nQ4:"}
{"prompt": "Context: decision in the product feature set and a mechanism to convey versioning to consumers should be part of the discussion.    gRPC performs incredibly well in high-bandwidth exchanges and is an ideal option for east\u2013west exchanges. Tooling for gRPC is powerful and provides another option when modeling exchanges.    Modeling multiple specifications starts to become quite tricky, especially when generating from one type of specification to another. Versioning complicates matters further but is an important factor to avoid breaking changes. Teams should think carefully before combining RPC representations with RESTful API representations, as there are fundamental differences in terms of usage and control over the consumer code.      The challenge for an API architecture is to meet the requirements from a consumer business perspective, to create a great developer experience around APIs, and to avoid unexpected compatibility issues. In Chapter 2 you will explore testing, which is essential in ensuring services meet these objectives.      1 We have been in this situation many times, usually first thing on a Monday!  2 Validation of OpenAPI Specifications at runtime helps enforce a greater strictness.      Chapter 2. Testing APIs   Chapter 1 covered the different types of APIs and the value that they provide to your architecture. This chapter closes out the Designing, Building, and Testing APIs section of this book by reviewing approaches to testing APIs. The new Attendee API that was\n\nQuestions:", "completion": "1. What is gRPC?\n  Correct answer: gRPC is a high-performance, open-source universal RPC framework. \n  Wrong answers:\n  a. gRPC is a type of web hosting service \n  b. gRPC is a type of content management system \n  c. gRPC is a type of serverless computing platform \n\n2. What is the purpose of versioning?\nCorrect answer: Versioning is used to avoid breaking changes and to ensure compatibility. \nWrong answers:\na. Versioning is used to track the number of users accessing an API. \nb. Versioning is used to increase the security of an API. \nc. Versioning is used to speed up the performance of an API."}
{"prompt": "Context: extracted within the Introduction should obviously be tested and validated. We believe that testing is core to building APIs. It helps provide a high level of confidence to you that your service is working as expected, which will help you deliver a quality product to consumers of your API. It is only by testing your API under varying conditions that you will gain the confidence that it is operating correctly.  When building APIs, as with creating any product, the only way to verify that the product works as expected is to test it. In the case of a mouthguard, this can mean stretching, hitting, pushing, and pulling the product, or even running simulations.1  As discussed in \u201cSpecifying REST APIs Using OpenAPI\u201d, an API should not return anything that differs from what is documented. It is also frustrating when an API introduces breaking changes or causes network timeouts due to the large duration of time to retrieve a result. These types of issues drive customers away and are entirely preventable by creating quality tests around the API service. Any API built should be ready to fulfill a variety of requirements, including sending useful feedback to users who provide a bad input, being secure, and returning results within a specified service-level objective (SLO) based on our service-level indicators (SLIs) that are agreed.2  In this chapter we will introduce the different types of testing that you can apply to your API to help avoid these issues from occurring.  We will\n\nQuestions:", "completion": "1. What is the purpose of testing an API?\nCorrect Answer: Testing an API helps provide a high level of confidence that the service is working as expected and helps deliver a quality product to consumers. \nWrong Answers: \na. Testing an API is not necessary. \nb. Testing an API is only used to improve the speed of the service. \nc. Testing an API is only used to ensure the API is secure."}
{"prompt": "Context: highlight the positives and the negatives of each type of testing, so you can decide where best to invest your time. We are going to focus on testing APIs and where we believe that you will gain the most value; we will not be covering generic testing of services. The chapter will contain additional resources for those readers seeking to gain a significantly more in-depth and specialist knowledge about testing.      Conference System Scenario for This Chapter   In \u201cAttendees Evolution ADR\u201d, we explained the reasons to separate the Attendee API from the rest of the conference system. The separation of the Attendee API introduces new interactions. The Attendee API will be used by the external CFP system and legacy conference system, as is shown in Figure 2-1. You will spend this chapter covering the testing needed for the Attendee service and how testing can help verify the interactions between the legacy conference system and the Attendee API. As a collective we have seen enough APIs that become inconsistent or produce accidental breaking changes as new releases are made, and this is primarily due to a lack of testing. For the new Attendee API it is important to ensure that it avoids these pitfalls by providing confidence that the correct results will always be returned, and the only way this can happen is by investing in the right levels of testing.      Figure 2-1. Scenario for the chapter      Testing can be applied at different levels of an API, starting with the individual\n\nQuestions:", "completion": "Q1. What is the main focus of this chapter?\n\nA1. The main focus of this chapter is testing needed for the Attendee service and how testing can help verify the interactions between the legacy conference system and the Attendee API. \nWrong Answers: 1. The main focus of this chapter is understanding the Attendee service and its interactions with the legacy conference system. 2. The main focus of this chapter is to cover the reasons to separate the Attendee API from the rest of the conference system. 3. The main focus of this chapter is to provide resources for readers seeking specialist knowledge about testing.\n\nQ2. What type of testing are we covering in this chapter?\n\nA2. We are covering testing APIs in this chapter.\nWrong Answers: 1. We are covering generic testing of services in this chapter. 2. We are covering the reasons to separate the Attendee API from the rest of the conference system in this chapter. 3. We are covering specialist knowledge about testing in this chapter.\n\nQ3. What is the purpose of investing in the right levels of testing?\n\nA3. The purpose of investing in the right levels of testing is to ensure that the Attendee API will always return the correct results and avoid any inconsistency or accidental breaking changes. \nWrong Answers: 1. The purpose of investing in the right levels of testing is to understand the Attendee service and its interactions with the"}
{"prompt": "Context: building blocks that make up the service, going all the way to verifying that it works as part of the entire ecosystem. Before showing you some tools and frameworks that are available for API testing, it is important to understand the strategies that can be used.      Testing Strategies   Testing is important; it ensures that you are building a working application. However, you don\u2019t want something that just works, you want something that also has the right behavior. Realistically, though, you have limited time and resources to write tests, so you will want to ensure that you are not wasting cycles writing tests that provide little to no value. After all, the value for customers is when you are running in production. Therefore, you need to be smart about deciding upon the coverage and proportions of the types of tests you should be using. Avoid creating irrelevant tests, duplicating tests, and any tests that are going to take more time and resources than the value they provide (i.e., flaky tests). Not all the testing that is introduced needs to be implemented to be able to release an API, as it may not be feasible due to time constraints and business demands.  To guide you to getting the right balance and the right tests for your case, we will introduce the test quadrant and test pyramid. These will give you focus on identifying the testing you should be implementing.      Test Quadrant   The test quadrant was first introduced by Brian Marick in his blog series on agile\n\nQuestions:", "completion": "Q1. What is the purpose of testing an API?\nA1. The purpose of testing an API is to ensure that it works as part of the entire ecosystem.\nWrong Answers: \n  1. To identify errors in code\n  2. To improve performance\n  3. To make sure it is secure\n\nQ2. What are the two main strategies for API testing?\nA2. The two main strategies for API testing are the test quadrant and the test pyramid.\nWrong Answers:\n  1. Unit testing and integration testing\n  2. Automation and manual testing\n  3. Exploratory testing and stress testing\n\nQ3. What is the goal of the test quadrant?\nA3. The goal of the test quadrant is to provide focus on identifying the testing that should be implemented.\nWrong Answers:\n  1. To automate the entire testing process\n  2. To identify areas of code that need refactoring\n  3. To find any bugs in the system\n\nQ4. What does the test pyramid provide?\nA4. The test pyramid provides guidance for deciding the coverage and proportions of the types of tests that should be used.\nWrong Answers:\n  1. A list of all the types of tests that need to be implemented\n  2. A diagram that shows the relationship between different types of tests\n  3. A way to measure"}
{"prompt": "Context: testing. This became popularized in the book Agile Testing by Lisa Crispin and Janet Gregory (Addison-Wesley). The technology side of building an API cares that it has been built correctly, that its pieces (e.g., functions or endpoints) respond as expected, and that it is resilient and continues to behave under abnormal circumstances. The business cares that the right service is being developed (i.e., in our case, that the Attendee API provides the right functionality). To clarify, the term \u201cthe business\u201d means someone who has a clear understanding of the product and the features and the functionality that should be developed; they need not have a technical understanding.  The test quadrant brings together tests that help technology and business stakeholders alike\u2014each perspective will have different opinions on priorities. The popular image of the test quadrant is shown in Figure 2-2.      Figure 2-2. Agile Test Quadrants from Agile Testing (Addison-Wesley) by Lisa Crispin and Janet Gregory      The test quadrant does not depict any order. The quadrants are labeled for convenience and this is a common source of confusion that Lisa describes in one of her blog posts. The four quadrants can be generally described as follows:  Q1  Unit and component tests for technology. These should verify that the service that has been created works, and this verification should be performed using automated testing.    Q2  Tests with the business. These ensure what is being built is serving a\n\nQuestions:", "completion": "Q1: What is the purpose of the test quadrant?\nA1: The test quadrant helps technology and business stakeholders evaluate the functionality of an API.\nWrong Answers:\n- The test quadrant helps to identify potential risks in an API.\n- The test quadrant helps to identify design flaws in an API.\n- The test quadrant helps to build an API.\n\nQ2: Who created the test quadrant?\nA2: The test quadrant was created by Lisa Crispin and Janet Gregory in their book Agile Testing (Addison-Wesley).\nWrong Answers:\n- The test quadrant was created by Microsoft.\n- The test quadrant was created by Apple.\n- The test quadrant was created by Google.\n\nQ3: What are the four quadrants of the test quadrant?\nA3: The four quadrants of the test quadrant are Q1 (Unit and component tests for technology), Q2 (Tests with the business), Q3 (Integration tests), and Q4 (User acceptance tests).\nWrong Answers:\n- Q1 (Unit tests), Q2 (Integration tests), Q3 (User acceptance tests), and Q4 (Business tests).\n- Q1 (Component tests), Q2 (Business tests), Q3 (Integration tests), and Q4 (User acceptance tests).\n- Q1 (Unit tests), Q2"}
{"prompt": "Context: purpose. This is verified with automated testing and can also include manual testing.    Q3  Testing for the business. This is about ensuring that functional requirements are met and also includes exploratory testing. When Figure 2-2 was originally created, this type of testing was manual; now it is possible to perform automated testing in this area as well.      Q4  Ensuring that what exists works from a technical standpoint. From Q1 you know that what has been built works; however, when the product is being used, is it performing as expected? Examples of performing correctly from a technical standpoint could include security enforcement, SLA integrity, and autoscaling.      The left side of quadrant (Q1, Q2) is all about supporting the product. It helps guide the product and prevent defects. The right side (Q3, Q4) is about critiquing the product and finding defects. The top of the quadrant (Q2, Q3) is the external quality of your product, making sure that it meets your users\u2019 expectations. This is what the business finds important. The bottom of the quadrant (Q1, Q4) is the technology-facing tests to maintain the internal quality of your application.3  The test quadrant does not say where you should start testing; it helps guide you on the tests that you might want. This is something that you must decide and should be based on the factors important to you. For example, a ticketing system must handle large traffic spikes, so it may be best to start with ensuring that your\n\nQuestions:", "completion": "Q1: How do you ensure that a product meets the expectations of users?\n\nA1: Testing for the business to make sure functional requirements are met. \nWrong answers: \n1. Automated testing only \n2. Exploratory testing only \n3. Manual testing only"}
{"prompt": "Context: ticket system is resilient (e.g., performance testing). This would be part of Q4.      Test Pyramid   In addition to the test quadrants, the test pyramid (also known as the test automation pyramid) can be used as part of your strategy for test automation. The test pyramid was first introduced in the book Succeeding with Agile by Mike Cohn. This pyramid illustrates a notion of how much time should be spent on a given test area, its corresponding difficulty to maintain, and the value it provides in terms of additional confidence. The test pyramid at its core has remained unchanged. It has unit tests as its foundation, service tests in the middle block, and UI tests at the peak of the pyramid. Figure 2-3 shows the areas of the test pyramid you will explore.      Figure 2-3. The test pyramid, showing the proportion of tests desired      The test automation pyramid shows the trade-offs that exist in terms of confidence, isolation, and scope. By testing small parts of the codebase, you have better isolation and faster tests; however, this does not give confidence that the whole application is working. By testing the entire application in its ecosystem, the opposite is true. The tests give you more confidence that the application is working, but the scope of the test will be large as many pieces will be interacting together. This also makes it more difficult to maintain and slow. The following defines each of the core elements of the test pyramid:  Unit tests are at the bottom of\n\nQuestions:", "completion": "Q1: How many layers are in the Test Pyramid?\nA1: Three (Unit, Service and UI)\nWrong Answers: Two, Four, Five\n\nQ2: What is the purpose of the Test Pyramid?\nA2: The Test Pyramid illustrates a notion of how much time should be spent on a given test area, its corresponding difficulty to maintain, and the value it provides in terms of additional confidence.\nWrong Answers: To measure the success of a test, To identify potential bugs, To determine which tests to prioritize\n\nQ3: What is the first step in test automation?\nA3: Unit tests are at the bottom of the pyramid and should be the first step in test automation.\nWrong Answers: Service Tests, UI Tests, Performance Tests"}
{"prompt": "Context: the pyramid; they form the foundation of your testing. They test small, isolated units of your code to ensure that your defined unit is running as expected.4 If your test is going to escape the boundaries of your unit, you can use test doubles. Test doubles are objects that look like real versions of an external entity; however, they are under your control.5 Because unit tests form the foundation of your pyramid, there should be more unit tests than any other type of test; we recommend using TDD as a practice.6 TDD is about writing tests before you write the logic. Unit tests fit into Q1 of the test quadrant and are used to provide quality to the internals of the application. Unit testing will not be covered further as we will be focusing on tests that verify your API from an external consumer standpoint as opposed to the internals of an API.    Service tests make up the middle tier of the pyramid. They will provide you with more confidence that your API is working correctly than unit tests, though they are more expensive. The expense comes from the tests having a larger scope and less isolation, which incurs a higher maintenance and development cost. Service tests include some of the following cases: the verification that multiple units are working together, that behavior is as expected, and that the application itself is resilient. Therefore, service tests fit into Q1, Q2, and Q4 of the test quadrant.    UI tests sit at the top of the test pyramid. In the old days a\n\nQuestions:", "completion": "Q1: What is the purpose of Unit Tests? \nA1: The purpose of Unit Tests is to test small, isolated units of code to ensure that they are running as expected. \nWrong Answers: \n- The purpose of Unit Tests is to test the internals of an API. \n- The purpose of Unit Tests is to provide quality to the API. \n- The purpose of Unit Tests is to verify the API from an external consumer standpoint. \n\nQ2: What is the main purpose of Service Tests? \nA2: The main purpose of Service Tests is to provide more confidence that an API is working correctly. \nWrong Answers: \n- The main purpose of Service Tests is to test small, isolated units of code. \n- The main purpose of Service Tests is to test the internals of an API. \n- The main purpose of Service Tests is to provide quality to the API. \n\nQ3: What type of tests fit into all four quadrants of the test quadrant? \nA3: Service tests fit into all four quadrants of the test quadrant. \nWrong Answers: \n- Unit tests fit into all four quadrants of the test quadrant. \n- UI tests fit into all four quadrants of the test quadrant. \n- TDD fits into all four quadrants of the test quadrant."}
{"prompt": "Context: majority of applications being built for the web were LAMP stacks, and the only way to test your application from the front to the backend was through the Web UI. There is a UI with APIs: it is just not graphical, so these tests will now be referred to as end-to-end tests. They cover the same ground of a request flowing from a start to an end point but do not necessarily imply or assume that traffic originates from a Web UI. End-to-end tests are the most complex. They have the largest scope and are slow to run; however, they will verify entire modules are working together so they provide lots of confidence. End-to-end tests will generally sit in Q2, Q3, Q4 of the quadrant. Tooling for testing has improved and become more advanced, and it is now making more and more of Q3 available for automation.      One type of test is not better than another\u2014the test pyramid is a guide to the proportions of each type of testing you should aim to implement. It can be tempting to ignore the test pyramid and concentrate on end-to-end testing as this gives a high degree of confidence. However, this is a fallacy and instead gives a false sense of security that these higher-level tests are of higher quality/value than unit tests. The fallacy gives rise to the ice cream cone representation of testing, which is the opposite of a test pyramid. For a robust argument on this topic, please read Steve Smith\u2019s blog post \u201cEnd-to-End Testing considered harmful\u201d. You may also consider implementing other\n\nQuestions:", "completion": "Q1. What is the most complex type of test? \nA1. End-to-end tests\nWrong Answers: Unit tests, Integration tests, Performance tests \n\nQ2. What is the benefit of end-to-end tests?\nA2. They provide lots of confidence.\nWrong Answers: They are fast to run, They verify individual modules, They provide a false sense of security \n\nQ3. What is the opposite of a test pyramid?\nA3. The ice cream cone representation of testing.\nWrong Answers: The ladder representation of testing, The diamond representation of testing, The triangle representation of testing"}
{"prompt": "Context: proportions of tests, though it is not recommended. Martin Fowler wrote an updated piece on testing shapes and covered why he feels that testing that is guided by any shape other than the test pyramid is incorrect.      ADR Guideline for Testing Strategies   To help you decide on the testing strategy that you should use, the ADR Guideline in Table 2-1 should help you make an informed decision.  Table 2-1. ADR Guideline: Testing strategies Decision  When building your API, which testing strategy should be made part of the development process?    Discussion Points  Do all parties that have a stake in the API have the time and the availability to regularly discuss how the API should be working? If you are unable to effectively communicate with the stakeholders, you could end up stalling your product waiting for a decision to be made.  Are the skills and experience available to effectively use these testing strategies? Not everyone has used these practices before, so you need to weigh if you have the time resources to train everyone on them.  Are there other practices within your workplace that are recommended and should be used? Sometimes there can be internal strategies to building software that work for an organization or are required due to the nature of the business.    Recommendations  We recommend using test quadrants and the test pyramid.  The test quadrant is very valuable to ensure that your customers are getting the right product. The test quadrant coupled with the\n\nQuestions:", "completion": "Question 1: What is the recommended testing strategy? \nCorrect Answer: We recommend using test quadrants and the test pyramid.\nWrong Answers: \n1. We recommend using only the test pyramid. \n2. We recommend using only proportions of tests. \n3. We recommend not using any testing.\n\nQuestion 2: What should be taken into consideration when deciding on a testing strategy?\nCorrect Answer: Do all parties that have a stake in the API have the time and the availability to regularly discuss how the API should be working? \nWrong Answers: \n1. Are the skills and experience available to effectively build the API? \n2. Does the organization have the time and resources to train stakeholders on the testing strategies? \n3. Are there other practices within your workplace that should not be used? \n\nQuestion 3: What is the ADR Guideline? \nCorrect Answer: The ADR Guideline is a table in the article that helps you make an informed decision when deciding on a testing strategy.\nWrong Answers: \n1. The ADR Guideline is a set of internal strategies to building software. \n2. The ADR Guideline is an updated piece on testing shapes. \n3. The ADR Guideline is a set of proportions that should be used for testing."}
{"prompt": "Context: test pyramid will help you build a great API.  We do recognize that using the test quadrant in its truest form by having someone readily available from the business to help guide your testing is not always possible. However, at a minimum, use the test pyramid as this concentrates the automated side of the test quadrant. This at the very least will ensure that you find bugs early in your development cycle.  Whatever the case, you will always need someone to help guide the product direction.      Contract Testing   Contract testing has two entities: a consumer and a producer. A consumer requests data from an API (e.g., web client, terminal shell), and a producer (also known as a provider) responds to the API requests, i.e., it is producing data, such as a RESTful web service. A contract is a definition of an interaction between the consumer and producer. It is a statement to say if a consumer makes a request that matches the contract request definition, then the producer will return a response that matches the contract response definition. In the case of the Attendee API, it is a producer and the consumer is the legacy conference system. The legacy conference system is a consumer as it is calling the Attendee API.7 So why use contracts? What do they offer you?      Why Contract Testing Is Often Preferable   As you learned in \u201cSpecifying REST APIs Using OpenAPI\u201d, APIs should have a specification, and it is important that your API responses conform to the API specification that\n\nQuestions:", "completion": "Q1: What is test pyramid? \nA1: Test pyramid is a testing strategy that focuses on building tests at the unit, integration, and end-to-end levels.\nWrong Answers:\nA1a: Test pyramid is a testing tool.\nA1b: Test pyramid is a testing methodology.\nA1c: Test pyramid is a testing framework.\n\nQ2: What is contract testing?\nA2: Contract testing is a type of testing where two entities, a consumer and a producer, agree to a request-response definition.\nWrong Answers:\nA2a: Contract testing is a type of testing that is used to test the user interface of an application.\nA2b: Contract testing is a type of testing that is used to test the performance of an application.\nA2c: Contract testing is a type of testing that is used to evaluate the scalability of an application.\n\nQ3: What is the purpose of contract testing?\nA3: The purpose of contract testing is to ensure that the API responses conform to the API specification.\nWrong Answers:\nA3a: The purpose of contract testing is to identify and fix bugs.\nA3b: The purpose of contract testing is to verify the security of an API.\nA3c: The purpose of contract testing is to measure the overall quality of an API.\n\nQ4: What is the"}
{"prompt": "Context: you have laid out. Having a written definition of these interactions that must be adhered to by the producer ensures that consumers can keep using your API and makes it possible to generate tests. The contract defines what a request and response should look like and these can be used to verify that the producer (the API) is fulfilling the contract. If you break a contract test, then it means that the producer is not fulfilling the contract anymore, which means that consumers will be broken.  As the contract has the response definition, it is also possible to generate a stub server.8 This stub server can be used by consumers to verify that they can call the producer correctly and parse the response from the producer. Contract testing can be performed locally\u2014it is not required to launch additional services, which makes it part of your service tests. Contracts will evolve and the consumers and producers pick up these changes as they are made available, which ensures that they are able to continually integrate with the latest contract.  There is already a lot of value here about why you would want to use contracts. Additionally, contract testing has a well-developed ecosystem. There are established methodologies that guide what the contract should be, as well as frameworks and test integrations to generate contracts and provide effective ways to distribute them. We believe that contracts are the best way to define interactions between the service you implement and a consumer.\n\nQuestions:", "completion": "Q1: What is contract testing?\nA: Contract testing is a software testing method used to verify that the producer (the API) is fulfilling the contract it has with consumers. \nWrong: \n- Contract testing is a software development method\n- Contract testing is a way to generate a stub server\n- Contract testing is used to integrate with the latest contract\n\nQ2: What value does contract testing provide?\nA: Contract testing provides a well-defined way to verify that the producer is fulfilling the contract it has with consumers, making it possible to generate tests, and ensuring that consumers can keep using the API.\nWrong: \n- Contract testing provides a way to generate a stub server\n- Contract testing provides a way to create a contract\n- Contract testing provides a way to distribute the contract \n\nQ3: What is the ecosystem for contract testing?\nA: The ecosystem for contract testing includes established methodologies for defining the contract, as well as frameworks and test integrations for generating and distributing the contract.\nWrong: \n- The ecosystem for contract testing includes a way to generate a stub server \n- The ecosystem for contract testing includes a way to create a contract \n- The ecosystem for contract testing includes a way to distribute the contract"}
{"prompt": "Context: Other tests are important and should be implemented as well, but these offer the most bang for the buck.      Note   It is important to note that contract testing is not the same as saying that an API conforms to a schema. A system is either compatible with a schema (like OpenAPI Spec) or it is not; a contract is about a defined interaction between parties and provides examples. Matt Fellows has an excellent piece on this titled \u201cSchema-based contract testing with JSON schemas and Open API (Part 1)\u201d.      How a Contract Is Implemented   As mentioned, a contract is a shared definition of how a producer and consumer interact. The following example shows a contract for a GET request to the endpoint /conference/{conference-id}/attendees. It states that the expected response has a property called value that contains an array of values about the attendees. In this sample definition of a contract, you can see that it is defining an interaction, which is used to generate the tests and stub server:   Contract.make { request { description('Get a list of all the attendees at a conference') method GET() url '/conference/1234/attendees' headers { contentType('application/json') } } response { status OK() headers { contentType('application/json') } body( value: [ $( id: 123456, givenName: 'James', familyName: 'Gough' ), $( id: 123457, givenName: 'Matthew', familyName: 'Auburn' ) ] ) } }  In Figure 2-4 you see how the generated tests are used by the consumer and producer.      Figure 2-4.\n\nQuestions:", "completion": "Q1: What is contract testing? \nA1: Contract testing is a process to verify that two or more systems interact properly according to a defined agreement.\nWrong Answers: 1. Contract testing is a form of API testing. 2. Contract testing is a way to check if an API conforms to a schema. 3. Contract testing is a manual process.\n\nQ2: What is an example of a contract?\nA2: An example of a contract is a shared definition of how a producer and consumer interact, such as a GET request to the endpoint /conference/{conference-id}/attendees.\nWrong Answers: 1. A contract is a document that outlines the terms of an agreement. 2. A contract is a form of authentication. 3. A contract is a type of software.\n\nQ3: How is a contract implemented?\nA3: A contract is implemented by defining an interaction between the producer and consumer, which is used to generate the tests and stub server.\nWrong Answers: 1. A contract is implemented by manually verifying the interaction between the producer and consumer. 2. A contract is implemented by creating a JSON schema. 3. A contract is implemented by using Open API."}
{"prompt": "Context: Generated stub server and tests from a contract      Warning   It is tempting to use contracts for scenario tests. For example:  Step 1: add an attendee to a conference.  Step 2: get the list of attendees of the conference and check that the attendee was added correctly.    Frameworks do support this but also discourage it. Contracts are about defining interaction; if you wish to test this type of behavior, then use component tests.    A key benefit of using contracts is that once the producer agrees to implement a contract, this decouples the dependency of building the consumer and producer.      Tip   We have used generated stub servers to run demos for stakeholders. This was useful as the producer was still implementing the logic; however, they had agreed to the contracts.    The consumer has a stub server to develop against and the producer has tests to ensure that they are building the right interaction. The contract test process saves time, as when both the consumer and producer are deployed, they should integrate seamlessly.      Note   The generated tests need to be executed against your running API (producer). When your API launches, you should use test doubles for external dependencies. You do not want to be testing integrations with other services as part of your generated contract tests against the consumer.    To understand how contracts are agreed upon, let\u2019s look at the two main contract methodologies.      Producer contracts   Producer contract testing is when\n\nQuestions:", "completion": "Q1. What is the purpose of using contracts for scenario tests?\nA1. The purpose of using contracts for scenario tests is to decouple the dependency of building the consumer and producer. \nWrong Answers:\n- The purpose of using contracts for scenario tests is to test integrations with other services. \n- The purpose of using contracts for scenario tests is to check that the attendee was added correctly. \n- The purpose of using contracts for scenario tests is to run demos for stakeholders. \n\nQ2. What type of framework supports scenario tests?\nA2. Frameworks that support scenario tests discourage the use of contracts for this purpose. \nWrong Answers:\n- Frameworks that support scenario tests encourage the use of contracts for this purpose. \n- Frameworks that support scenario tests do not use contracts for this purpose. \n- Frameworks that support scenario tests require the use of contracts for this purpose. \n\nQ3. What are the two main contract methodologies?\nA3. The two main contract methodologies are producer contracts and consumer contracts. \nWrong Answers:\n- The two main contract methodologies are server contracts and client contracts. \n- The two main contract methodologies are stub server contracts and integration contracts. \n- The two main contract methodologies are component tests and integration tests. \n\nQ4. How can generated tests be executed against a running API?"}
{"prompt": "Context: a producer defines its own contracts. This practice is commonly utilized when your API is being used outside your immediate organization (i.e., external third parties). When you\u2019re developing an API for an external audience, the API needs to maintain its integrity, because the interface cannot make breaking changes without a migration plan, as you learned in \u201cAPI Versioning\u201d. Though interactions will be updated and improved, no individual consumer is likely to be able to ask for changes that affect the whole API and receive a quick change, because these changes need to be carefully orchestrated.  A real-world example of such an API is the Microsoft Graph API. Microsoft has thousands of consumers of this API from companies all over the world. Having companies or individuals adjust contracts for the Graph API with what they believe the contract should look like isn\u2019t feasible. That is not to say that changes should not be suggested to Microsoft, as they definitely are. However, even if a change is agreed to it will not be made quickly as the change will need to be verified and tested carefully.  If the Attendee API is going to be made available for public consumption, then the same concerns occur. What is important for the Attendee API is to use contracts to ensure that the interactions do not diverge and that the data returned is consistent.  Another reason to use producer contracts is that it is easier to get started. It is a good way to introduce contracts to your APIs.\n\nQuestions:", "completion": "Q1: What is the purpose of producer contracts?\nA1: The purpose of producer contracts is to maintain the integrity of an API and ensure that interactions do not diverge and the data returned is consistent.\nWrong Answers: \n- To enable companies and individuals to adjust contracts for the API.\n- To quickly make changes to the API.\n- To save time and resources when developing an API.\n\nQ2: What is an example of an API that uses producer contracts?\nA2: Microsoft Graph API is an example of an API that uses producer contracts.\nWrong Answers:\n- Google Maps API\n- Twitter API\n- Amazon Web Services API"}
{"prompt": "Context: Having contracts is far more beneficial than not having them. However, when consumers and producers are both in the same organization, we suggest that you use the consumer-driven contracts methodology.      Consumer-driven contracts   Consumer-driven contracts (CDCs), by definition, are implemented by a consumer driving the functionality that they wish to see in an interaction. Consumers submit contracts, or changes to a contract, to the producer for new or additional API functionality. When the new/updated contract is submitted to the producer, a discussion about the change will begin, which will result in accepting or rejecting this change.  CDC is very much an interactive and social process. The owners of the applications that are consumers and producers should be within reach (e.g., in the same organization as one another). When a consumer would like a new interaction (e.g., API call) or have an interaction updated (e.g., a new property added), then they submit a request for that feature.      Case study: Applying CDC   In our case, this may mean that a pull request is submitted from the legacy conference system to the new Attendee API service. The request for the new interaction is then reviewed and a discussion takes place about this new functionality. This discussion is to ensure that this is something that the Attendee service should and will fulfill. For example, if a contract is suggested for a PUT request, a discussion can take place, as it may be preferable to\n\nQuestions:", "completion": "Q1: What is Consumer-driven Contracts?\nA1: Consumer-driven Contracts (CDC) are contracts implemented by a consumer to drive the functionality they wish to see in an interaction. \nWrong answers:\na) Consumer-driven Contracts (CDC) are contracts implemented by a producer to drive the functionality they wish to see in an interaction. \nb) Consumer-driven Contracts (CDC) are contracts implemented by a third-party to drive the functionality they wish to see in an interaction. \nc) Consumer-driven Contracts (CDC) are contracts implemented by a government to drive the functionality they wish to see in an interaction. \n\nQ2: What is the process of Consumer-driven Contracts?\nA2: The process of Consumer-driven Contracts (CDC) is interactive and social. Owners of applications that are consumers and producers should be within reach, and when a consumer would like a new interaction (e.g., API call) or have an interaction updated (e.g., a new property added), then they submit a request for that feature.\nWrong answers:\na) The process of Consumer-driven Contracts (CDC) is automated and private. Owners of applications that are consumers and producers should be within reach, and when a consumer would like a new interaction (e.g., API call) or have an interaction updated (e.g., a new property added), then they submit a request for that feature"}
{"prompt": "Context: have this as a PATCH request.  This is where a good part of the value of contracts comes from: this discussion for both parties about what the problem is, and using a contract to assert that this is what the two parties accept and agree to. Once the contract is agreed to, the producer (Attendee service) accepts the contract as part of the project and can start fulfilling it.      Contracts methodology overview   These methodologies should hopefully give an overview of how to use contracts as part of the development process. This should not be taken as gospel, as variations do exist on the exact steps. For example, one process may request that the consumer\u2014when writing the contract\u2014also creates a basic implementation of producer code to fulfill the contract. In another example, the consumer should TDD the functionality they require and then create the contract before submitting the pull request. The exact process that is put in place may vary by team. Once you understand the core concepts and patterns of CDC, the exact process that is used is just an implementation detail.  If you are starting out on a journey to add contracts, you should note that there is a cost\u2014the setup time to incorporate contracts into a project and also the cost of writing the contracts. It is worth looking at tooling that can create contracts for you based on an OpenAPI Specification.9      Contract testing frameworks   It is likely that when it comes to contract testing frameworks for HTTP, you will\n\nQuestions:", "completion": "Q1. What is the purpose of contracts? \nA1. The purpose of contracts is to provide a discussion between two parties about what the problem is, and to agree on a solution. \nWrong Answers: \nA1. Contracts are used to enforce legal obligations. \nA1. Contracts are used to protect the consumer. \nA1. Contracts are used to prevent disputes."}
{"prompt": "Context: want to look at Pact. Pact has evolved into the default contract testing framework due to the ecosystem that has been built around it and the sheer number of languages it supports. Other contract testing frameworks are available, and they can be opinionated. Pact is opinionated; it enforces that you should perform CDC and is specifically designed for that. A test is written by a consumer and that test generates a contract, which takes the form of an intermediate representation of the interaction. This language-agnostic intermediate representation is why Pact has such wide language usage. Other frameworks have differing opinions; for example, Spring Cloud Contracts does not have a strong opinion on CDC or producer contracts, and either can be achieved. This is possible as with Spring Cloud Contracts you write the contracts by hand as opposed to having them generated. Though Spring Cloud Contracts is language agnostic by using a containerized version of the product, to get the most out of it you need to be using the Spring and JVM ecosystem.10  There are options for contract testing for other protocols; it is not exclusively for HTTP communications.      API contracts storage and publishing   Having seen how contracts work and methodologies of incorporating them into the development process, the next consideration becomes where contracts are stored and how they should be published.  There are a few options for storing and publishing contracts and these again depend on the setup\n\nQuestions:", "completion": "Q1. What is Pact?\nA1. Pact is a contract testing framework that is designed to enforce consumer-driven contract (CDC) testing.\nWrong Answers:\n-Pact is a programming language\n-Pact is an API storage system\n-Pact is a website design tool\n\nQ2. What is the intermediate representation for Pact?\nA2. The intermediate representation for Pact is a language-agnostic representation of the interaction between the consumer and provider.\nWrong Answers:\n-The intermediate representation for Pact is a custom programming language\n-The intermediate representation for Pact is a language-specific representation of the interaction between the consumer and provider\n-The intermediate representation for Pact is a tool for generating contracts\n\nQ3. What is Spring Cloud Contracts?\nA3. Spring Cloud Contracts is a framework that does not strongly enforce consumer-driven contract (CDC) testing, and allows contracts to be written by hand.\nWrong Answers:\n-Spring Cloud Contracts is a programming language\n-Spring Cloud Contracts is a website design tool\n-Spring Cloud Contracts is an API storage system\n\nQ4. What protocols are supported by contract testing?\nA4. Contract testing is not exclusively for HTTP communications, and can be used for other protocols.\nWrong Answers:\n-Contract testing is only used for web applications\n-Contract testing is only used for HTTP communications\n-Contract testing is only used for mobile applications"}
{"prompt": "Context: that is available to you and your organization.  Contracts can be stored alongside the producer code in version control (e.g., Git). They can also be published alongside your build into an artifact repository such as Artifactory.  Ultimately the contracts need to be obtainable by the producer and the consumer. The storage point also needs to allow for the submission of new contracts. The producer should have control over which contracts are accepted in the project and can ensure that undesired changes aren\u2019t made or additional contracts are added. The downside to this approach is that in a large organization it can be difficult to find all the API services that use contracts.  Another option is to store all the contracts in a centralized location to enable visibility into other API interactions that are available. This central location could be a Git repository, but the downside to this approach is that unless organized and set up correctly, it is possible and likely that contracts get pushed into a module that the producer has no intention of fulfilling.  Yet another option for storing contracts is to use a broker. The Pact contract framework has a broker product that can be used as a central location to host contracts. A broker can show all contracts that have been validated by the producer as the producer will publish those contracts that have been fulfilled. A broker can also see who is using a contract to produce a network diagram, integrate with CI/CD pipelines, and\n\nQuestions:", "completion": "Q1. What is the best way to store contracts?\nA1. The best way to store contracts is to use a broker, such as the Pact contract framework, which can host contracts centrally and show all contracts that have been validated by the producer.\nWrong answers:\n- Storing contracts in an artifact repository such as Artifactory\n- Storing contracts in a Git repository\n- Storing contracts alongside the producer code in version control"}
{"prompt": "Context: provide even more valuable information. This is the most comprehensive solution available and if you use a framework that is compatible with the Pact Broker, then it is recommended.      ADR Guideline: Contract Testing   To understand if applying contract testing is valid for your case and weighing the pros and cons of using contracts, the ADR Guideline in Table 2-2 should help guide you to a decision.  Table 2-2. ADR Guideline: Contract testing Decision  When building an API should you use contract testing and, if so, should you use consumer-driven contracts or producer contracts?    Discussion Points  Determine whether you are ready to include contract testing as part of your API testing.  Do you want to add an extra layer of testing to your API that developers will be required to learn about?      If contracts have not been used before, then it requires time to decide how you will use them.  Should contracts be centralized or in a project?    Do additional tools and training need to be provided to help people with contracts?      If deciding to use contracts, then which methodology should be used\u2014CDC or producer contracts?  Do you know who will use this API?    Will this API be used just within your organization?    Does the API have consumers that are willing to engage with you to help drive your functionality?      Recommendations  We recommend using contract testing when building an API. Even if there is a developer learning curve and you are deciding how you are going\n\nQuestions:", "completion": "1. What is contract testing?\nCorrect Answer: Contract testing is a type of software testing that verifies the interactions between two or more services.\nWrong Answers:\na. Contract testing is a form of automated testing. \nb. Contract testing is a process of gathering customer feedback.\nc. Contract testing is a type of user acceptance testing.\n\n2. What type of contracts should be used when building an API?\nCorrect Answer: Consumer-driven contracts (CDC) or producer contracts.\nWrong Answers:\na. Monolithic contracts\nb. System contracts\nc. Service contracts\n\n3. What is the ADR Guideline for contract testing?\nCorrect Answer: The ADR Guideline for contract testing is a decision to determine whether to include contract testing as part of your API testing and which methodology should be used.\nWrong Answers:\na. The ADR Guideline for contract testing is a list of common contract testing techniques.\nb. The ADR Guideline for contract testing is a document outlining the legal requirements for contract testing.\nc. The ADR Guideline for contract testing is a reference guide to contract testing best practices.\n\n4. What is the main benefit of using contract testing?\nCorrect Answer: The main benefit of using contract testing is that it provides an extra layer of testing to ensure that the API functions as expected.\nWrong Answers:\na. The main benefit"}
{"prompt": "Context: to set up your contracts for the first time, we believe it is worth the effort. Defined interactions that are tested save so much time when integrating services together.  If you are exposing your API to a large external audience, it is important to use producer contracts. Again, having defined interactions that help ensure that your API does not break backward compatibility is crucial.  If you\u2019re building an internal API, the ideal is to work toward CDC, even if you have to start with producer contracts and evolve over to CDC.  If contract testing is not feasible, then for a producer you need alternatives to ensure that your API is conforming your agreed interactions and provide a way that consumers can test. This means that you have to be very careful with your tests that the responses and requests match with what is expected, which can be tricky and time-consuming.      API Component Testing   Component testing can be used to validate that multiple units work together and should be used to validate behavior\u2014they are service tests in the test pyramid in Figure 2-3. An example of a component test is sending a request to your API and verifying the response. At a high level it will require that your application can read the request, perform authentication and authorization, deserialize a payload, perform the business logic, serialize the payload, and respond. That is a lot of units being tested, and it would be difficult to point to exactly where a bug could be. Where this\n\nQuestions:", "completion": "Q1. What is contract testing?\nA1. Contract testing is a way to ensure that two different services are able to communicate and interact in the expected manner. \nWrong Answers: a) Contract testing is a type of API testing b) Contract testing is a way to test the functionality of code c) Contract testing is a way to test the security of an API.\n\nQ2. What is API component testing?\nA2. API component testing is a way to validate that multiple units work together and should be used to validate behavior.\nWrong Answers: a) API component testing is a way to test the security of an API b) API component testing is a type of API testing c) API component testing is a way to test the functionality of code."}
{"prompt": "Context: example differs from a contract test is that you should be checking that the service had the correct behavior; for example, if this was creating a new attendee, you want to verify that the service made a call to the (mocked) database. You are not just checking the shape of the response like contract tests do. As component tests verify multiple units together, they are (normally) slower running than unit tests. Component tests should not call out to external dependencies. Like contract testing, you are not using these tests to verify external integration points. The type of tests that you want to trigger in this scope varies based on the business case; however, for APIs you would be looking to validate cases such as:  Is the correct status code returned when a request is made?    Does the response contain the correct data?    Is an incoming payload rejected if a null or empty parameter is passed in?    When I send a request where the accepted content type is XML, will the data return the expected format?    If a request is made by a user who does not have the correct entitlements, what will the response be?    What will happen if an empty dataset is returned? Is this a 404 or is it an empty array?    When creating a resource, does the location header point to the new asset created?      From this selection of tests, you can see how these bleed into two areas of the test quadrant. This includes Q1, where you are confirming that the API being built works (i.e., it is producing\n\nQuestions:", "completion": "Q1: What will happen if an empty dataset is returned? \nCorrect answer: It depends on the API, but it could either be a 404 or an empty array.\nWrong answers:\n-It will return an error. \n-It will return a 500 error.\n-It will return a 200 status code.\n\nQ2: When creating a resource, what header should point to the new asset created?\nCorrect answer: The location header. \nWrong answers:\n-The content-type header.\n-The authorization header.\n-The request header. \n\nQ3: When I send a request where the accepted content type is XML, what will the data return?\nCorrect answer: The expected format. \nWrong answers:\n-The expected code. \n-The expected value.\n-The expected response."}
{"prompt": "Context: results), and Q2, where you test to verify that the responses of the Attendee API are correct.      Contract Testing Versus Component Testing   If contract testing is not available, you should use API component tests to verify that your API conforms to your agreed interactions, i.e., your API specification. Using API component tests to verify that your API conforms to an interaction is not ideal\u2014for a start, it is much more likely to be error-prone and is tedious to write. You should make contracts your golden source of agreed interactions, as the generated tests ensure that the shape of your API is accurate.      Case Study: Component Test to Verify Behavior   Let\u2019s look at an example of a case for our Attendee API for the endpoint /conference/{conference-id}/attendees. This endpoint returns a list of the attendees at a conference event. For this component test, a mock is used to represent our external database dependency, and as seen in Figure 2-5, in this case that is the DAO.  Some things to test this endpoint for are:  Requests that are successful have response of 200 (OK)    Users without the right level of access will return a status of 403 (Forbidden)    When a conference has no attendees, an empty array will be returned      Figure 2-5. API Component test with mocked DAO      A library or testing framework that wraps a request client can be really useful. Here REST-Assured is used to call the Attendee API endpoint and to verify these test cases:11   @Test void\n\nQuestions:", "completion": "Q1: What are the benefits of Contract Testing? \nA1: The benefits of Contract Testing are that it ensures that the shape of your API is accurate, it helps to reduce the number of errors in the API, and it helps to ensure that the interactions between the API and its clients are as agreed upon. \nWrong Answers: \nA1: The benefits of Contract Testing are that it helps to increase the speed of development, it helps to improve the user experience, and it helps to create a more secure API. \nA1: The benefits of Contract Testing are that it helps to reduce the cost of development, it helps to improve the performance of the API, and it helps to create a more reliable API. \nA1: The benefits of Contract Testing are that it helps to reduce the need for manual testing, it helps to track changes to the API, and it helps to create a more user friendly API. \n\nQ2: What is the difference between Contract Testing and Component Testing? \nA2: The difference between Contract Testing and Component Testing is that Contract Testing is used to verify that the interactions between the API and its clients are as agreed upon, while Component Testing is used to verify that the API conforms to an interaction, but it is more likely to be error-prone and is tedious to write. \nWrong Answers: \nA2: The difference between Contract Testing and Component Testing is that"}
{"prompt": "Context: response_for_attendees_should_be_200() { given() .header(\"Authorization\", VALID_CREDENTIAL) .when() .get(\"/conference/conf-1/attendees\") .then() .statusCode(HttpStatus.OK.value()); } @Test void response_for_attendees_should_be_403() { given() .header(\"Authorization\", INVALID_CREDENTIAL) .when() .get(\"/conference/conf-1/attendees\") .then() .statusCode(HttpStatus.FORBIDDEN.value()); ... }  Running this type of test gives us confidence that our API is behaving correctly.      API Integration Testing   Integration tests in our definition are tests across boundaries between the module being developed and any external dependencies. Integration tests are a type of service test and can be seen in the test pyramid image in Figure 2-3.  When performing integration testing, you want to confirm that the communication across the boundary is correct; i.e., your service can correctly communicate with another service that is external to it.  The types of things you want to verify are the following:  Ensuring that an interaction is being made correctly; e.g., for a RESTful service, this may be specifying the correct URL or that the payload body is correct.    Can the unit that is interacting with an external service handle the responses that are being returned?      In our case the legacy conference system needs to verify that it can make a request to the new Attendee API and can interpret the response.      Using Stub Servers: Why and How   If you are using contract tests, the generated stub\n\nQuestions:", "completion": "1. What is the purpose of API integration testing?\n\nCorrect Answer: To confirm that communication across boundaries between the module being developed and any external dependencies is correct.\n\nWrong Answers: \na. To ensure the accuracy of the data being transferred across the boundary. \nb. To ensure that the API is properly secured. \nc. To ensure that the API is functioning properly. \n\n2. What is the difference between unit tests and integration tests?\n\nCorrect Answer: Unit tests are tests that check the functionality of a single unit of code, while integration tests are tests across boundaries between the module being developed and any external dependencies.\n\nWrong Answers: \na. Unit tests are tests that check the functionality of multiple units of code, while integration tests are tests that check the functionality of a single unit of code. \nb. Unit tests are tests that check the functionality of a single unit of code, while integration tests are tests that check the functionality of multiple units of code. \nc. Unit tests are tests across boundaries between the module being developed and any external dependencies, while integration tests are tests that check the functionality of a single unit of code. \n\n3. What is the purpose of using stub servers in integration tests?\n\nCorrect Answer: To generate a simulated response that can be used to test how the system handles the response from external services.\n\nWrong Answers:\na."}
{"prompt": "Context: servers can be used to verify that the consumer can communicate with the producer. The legacy conference system has a generated stub server and can use this to test against. This will keep testing local, and the stub server will be accurate. This is the preferred option for testing an external boundary.  However, a generated stub server from a contract is not always available and other options are required, as in the case of testing with an external API, such as the Microsoft Graph API, or within your organization when contracts are not used. The simplest one is to hand roll a stub server that mimics the requests and responses of the service you interact with. This is certainly a viable option, as in your chosen language and framework it is usually very easy for a developer to create a stub server with canned responses that integrate with tests.  The key considerations when hand rolling a stub server is to make sure that the stub is accurate. It can be very easy to make mistakes, such as inaccurately portraying the URL or making mistakes in the response property names and values. Can you see the errors in this handtyped response?12   { \"values\": [ { \"id\": 123456, \"givenName\": \"James\", \"familyName\": \"Gough\" }, { \"id\": 123457, \"givenName\": \"Matthew\", \"familyNane\": \"Auburn\" }, { \"id\": 123456, \"givenName\": \"Daniel\", \"familyName\": \"Bryant\" } ] }  This should still not put you off as this is a good solution. One of the authors had great success with this approach after a\n\nQuestions:", "completion": "Q1. What is the purpose of a stub server? \nA1. The purpose of a stub server is to mimic the requests and responses of the service you interact with, in order to test an external boundary. \nWrong Answers: \na. The purpose of a stub server is to store data. \nb. The purpose of a stub server is to keep testing local. \nc. The purpose of a stub server is to generate contracts. \n\nQ2. What is the preferred option for testing an external boundary? \nA2. The preferred option for testing an external boundary is to use a generated stub server. \nWrong Answers: \na. The preferred option for testing an external boundary is to use a hand-rolled stub server. \nb. The preferred option for testing an external boundary is to use a Microsoft Graph API. \nc. The preferred option for testing an external boundary is to use contracts. \n\nQ3. What are the key considerations when hand rolling a stub server? \nA3. The key considerations when hand rolling a stub server are accuracy and making sure that the URL, response property names and values are correct. \nWrong Answers: \na. The key considerations when hand rolling a stub server are speed and efficiency. \nb. The key considerations when hand rolling a stub server are cost and scalability. \nc. The key"}
{"prompt": "Context: requirement for a project meant he had to hand roll a stub server for a login service.  A way to avoid these inaccuracies and to ensure that requests to URLs are accurately captured along with the responses is to use a recorder. It is possible to use a tool that will record the requests and responses to an endpoint and generate files that can be used for stubbing. Figure 2-6 shows how this works.      Figure 2-6. How a consumer of the Attendee API would use a recorder to capture a request/response for test data      These generated files are mappings that can then be used for tests to accurately portray requests and responses, and as they are not hand rolled they are guaranteed to be accurate at the point of generation. To use these generated files, a stub server is launched that is capable of reading in the mappings files. When a request is made to the stub server, it checks to see if the request matches any of the expected requests in the mappings file. If it matches, then the mapped response will be returned.13 Recording calls to APIs will produce more accurate stubs than that of hand rolling a stub. If you do use recordings, then you need to make sure they stay updated and in sync; also, if you make recordings against production, you need to watch that no PII is saved into the mapping files.      ADR Guideline: Integration Testing   Integration testing is important, so to help you understand what types of integration testing you need, see the ADR Guideline in Table 2-3.\n\nQuestions:", "completion": "Q1. What is a recorder? \nA1. A recorder is a tool that captures requests and responses to an endpoint and generates files that can be used for stubbing. \nWrong Answers: \n- A recorder is used to record audio and video. \n- A recorder is used to measure temperature.\n- A recorder is used to measure speed.\n\nQ2. What is a stub server?\nA2. A stub server is a server that is capable of reading in mapping files and returning a mapped response when a request is made.\nWrong Answers: \n- A stub server is a type of web hosting.\n- A stub server is a type of game server.\n- A stub server is a type of database server.\n\nQ3. What is the purpose of integration testing?\nA3. The purpose of integration testing is to ensure that all components of a system are working together as expected.\nWrong Answers: \n- The purpose of integration testing is to identify areas of improvement in a system.\n- The purpose of integration testing is to identify security vulnerabilities in a system.\n- The purpose of integration testing is to identify bugs in a system.\n\nQ4. What is the ADR Guideline?\nA4. The ADR Guideline is a set of guidelines for integration testing.\nWrong Answers: \n- The ADR Guideline is a set of"}
{"prompt": "Context: Table 2-3. ADR Guideline: Integration testing Decision  Should integration testing be added to API testing?    Discussion Points  If your API is integrating with any other service, what level of integration test should you use?  Do you feel confident that you can just mock responses and do not need to perform integration tests?    For creating a stub server to test against, are you able to accurately craft the request and responses or should they be recorded?    Will you be able to keep stub servers up-to-date and recognize if an interaction is incorrect?      If your stubs are incorrect or become out of date, this means it is possible to have tests that pass against your stub server, but when you deploy to production, your service fails to interact with the other API as it has changed.      Recommendations  We do recommend using the generated stub servers from contract tests. However, if this is not available, then having integration testing using recordings of interactions is the next best option. Having integration tests that can be run locally gives confidence that an integration will work, especially when refactoring an integration; it will help to ensure that any changes have not broken anything.      Integration testing is a really useful tool; however, definitions of these interactions have issues. The main issue is that they are point-in-time snapshots. These bespoke setups do not get updated with changes.  We have been using stub servers for the integrations we have\n\nQuestions:", "completion": "Q1: What is integration testing?\nA1: Integration testing is a process in which individual components of software are tested to determine whether they interact with each other correctly.\nWrong Answers:\n1. Integration testing is a process in which individual parts of a system are tested to determine how quickly they can be assembled.\n2. Integration testing is a process in which individual parts of a system are tested to determine how accurately they can be connected.\n3. Integration testing is a process in which individual parts of a system are tested to determine how efficiently they can be operated.\n\nQ2: What is the recommended approach for integration testing?\nA2: The recommended approach for integration testing is to use the generated stub servers from contract tests.\nWrong Answers:\n1. The recommended approach for integration testing is to use manual testing.\n2. The recommended approach for integration testing is to use automated testing.\n3. The recommended approach for integration testing is to use a combination of manual and automated testing.\n\nQ3: How can integration testing help when refactoring an integration? \nA3: Integration testing can help to ensure that any changes have not broken anything when refactoring an integration.\nWrong Answers:\n1. Integration testing can help to ensure that any changes have been implemented correctly when refactoring an integration.\n2. Integration testing can help to ensure that any changes have been tested correctly when refactoring"}
{"prompt": "Context: looked at; however, it is possible to use a real instance of the external service to verify an integration.      Containerizing Test Components: Testcontainers   It is common to build applications as containerized images, which means that many applications that your service will integrate with are also available as containerized solutions. These images can be run on your local machine as part of your testing. Not only does using local containers allow for testing communication with the external services, but also you can run the same image that is run in production.  Testcontainers is a library that integrates with your testing framework to orchestrate containers. Testcontainers will start and stop and generally organize the lifecycle of containers you use with your tests.      Case Study: Applying Testcontainers to Verify Integrations   Let\u2019s take a look at two use cases where this is helpful for the Attendee API. The first case is that the Attendee API service will support a gRPC interface as well as the RESTful interface. The gRPC interface is to be developed after the RESTful interface, but there are eager developers who want to start testing against a gRPC interface. The decision is made to provide a stub server for the gRPC interface, which will be a stub that provides a few canned responses. To achieve this goal, a bare-bones application is made that fulfills this objective. This gRPC stub is then packaged up, containerized, and published. This stub can be now used by\n\nQuestions:", "completion": "Q1. What is TestContainers?\nA1. TestContainers is a library that integrates with your testing framework to orchestrate containers.\nWrong Answers:\n1. TestContainers is a type of containerized application.\n2. TestContainers is a testing service that runs tests for applications.\n3. TestContainers is a type of stub server for the gRPC interface.\n\nQ2. What is the purpose of using local containers?\nA2. The purpose of using local containers is to allow for testing communication with the external services, as well as to run the same image that is run in production.\nWrong Answers:\n1. The purpose of using local containers is to provide a platform to run applications.\n2. The purpose of using local containers is to provide a way to easily deploy applications.\n3. The purpose of using local containers is to provide a secure environment for data storage.\n\nQ3. What is the primary benefit of using TestContainers?\nA3. The primary benefit of using TestContainers is that it will start and stop and generally organize the lifecycle of containers you use with your tests. \nWrong Answers:\n1. The primary benefit of using TestContainers is that it provides a way to containerize test components.\n2. The primary benefit of using TestContainers is that it provides a way to verify integrations.\n3. The"}
{"prompt": "Context: the developers for testing across a boundary; i.e., they can make real calls to this stub server in their tests, and this containerized stub server can run locally on their machine.  The second use case is that the Attendee API service has a connection to an external database, which is an integration to test. The options for testing integration boundaries for a database would be to mock out the database, use an in-memory database (e.g., H2), or run a local version of the database using Testcontainers. Using a real instance of the database in your test provides a lot of value because with mocks you can mock the wrong return value or make an incorrect assumption. With an in-memory DB you are assuming that the implementation matches the real DB. Using a real instance of the dependency and it being the same version that you run in production means that you get reliable testing across a boundary, which ensures that the integration will work when going to production. In Figure 2-7 you see the structure of the test to confirm a successful integration across a boundary with a database.      Figure 2-7. Testcontainers DAO test      Testcontainers is a powerful tool and should be considered when testing boundaries between any external services. Other common external services that benefit from using Testcontainers include Kafka, Redis, and NGINX. Adding this type of solution will increase the time it takes for your tests to run; however, integration tests are usually fewer and the\n\nQuestions:", "completion": "Q1. What is TestContainers used for?\nA1. TestContainers is used for testing across boundaries, such as external databases, Kafka, Redis and NGINX.\nWrong Answers: \na. TestContainers is used for unit testing.\nb. TestContainers is used for performance testing.\nc. TestContainers is used for load testing.\n\nQ2. What is the advantage of using a real instance of the database in tests?\nA2. The advantage of using a real instance of the database in tests is that it ensures reliable testing across a boundary and that the integration will work when going to production.\nWrong Answers:\na. The advantage of using a real instance of the database in tests is that it is faster than other testing methods.\nb. The advantage of using a real instance of the database in tests is that it is cheaper than other testing methods.\nc. The advantage of using a real instance of the database in tests is that it is more accurate than other testing methods.\n\nQ3. What types of external services can benefit from using TestContainers?\nA3. External services that can benefit from using TestContainers include databases, Kafka, Redis, and NGINX.\nWrong Answers:\na. External services that can benefit from using TestContainers include web services and APIs.\nb. External services that can benefit from using Test"}
{"prompt": "Context: additional confidence that is provided more often than not is a worthwhile trade-off for additional time.  Using Testcontainers raises a couple of questions. First, is this type of testing considered integration testing or is it end-to-end testing as a real instance of another service is being tested against? Second, why not just use this instead of contracts?  Using Testcontainers does not make the tests end-to-end if it stays within the integration boundary. We suggest that you use Testcontainers to test integrations; ensuring that the container has the right behavior is not your job (assuming that the owner of the image is outside your domain). For example, if I issue a statement to publish a message to a Kafka broker, I should not then subscribe to the topic to check that the item published was correct. I should trust that Kafka is doing its job and subscribers would be getting the message. If you want to verify this behavior, make it part of your end-to-end tests. That is why the boundary of what you are testing matters, so the case of the DAO to the database is not end-to-end testing, because only the interactions across the boundary are validated.  Testcontainers and integrating with a real service is a real boon and can add a lot of value to your testing, though they are not a replacement for contracts just because you can use a real version of a service. Working with a real instance is nice; however, contracts provide so much more than just a stub server\u2014they provide\n\nQuestions:", "completion": "Q1. What type of testing does Testcontainers provide? \nA1. Testcontainers provides integration testing. \nWrong Answers: Testcontainers provides unit testing, Testcontainers provides end-to-end testing, Testcontainers provides performance testing. \n\nQ2. Why should contracts be used in addition to Testcontainers? \nA2. Contracts should be used in addition to Testcontainers because they provide more confidence that the integration is working as expected. \nWrong Answers: Contracts should be used in addition to Testcontainers because they provide a more realistic environment, Contracts should be used in addition to Testcontainers because they are faster, Contracts should be used in addition to Testcontainers because they are more reliable. \n\nQ3. Is Testcontainers suitable for end-to-end testing? \nA3. No, Testcontainers is not suitable for end-to-end testing as it only tests interactions across the boundary. \nWrong Answers: Yes, Testcontainers is suitable for end-to-end testing, Testcontainers is only suitable for unit testing, Testcontainers is suitable for performance testing."}
{"prompt": "Context: all the testing, integration, and collaboration.      End-to-End Testing   The essence of end-to-end testing is to test services and their dependencies together to verify they work as expected. It is important to validate that when a request is made, and it reaches the front door (i.e., the request reaches your infrastructure), the request flows all the way through and the consumer gets the correct response. This validation gives confidence that all these systems work together as expected. For our case this is testing the legacy conference system, the new Attendee service, and the database all together.      Automating End-to-End Validation   This section concentrates on automated end-to-end tests. Automation is intended to save you time, so we will present automated testing that we believe gives you the best value. You will always need to verify that your systems work together\u2014however, you can do this manually in a testing environment before releasing software into production.      Warning   If you are building an external-facing API and you have multiple third parties that are consuming it, don\u2019t try to copy the third-party UI and replicate how it works. Doing so will mean that you spend huge amounts of time trying to replicate something out of your domain.    For end-to-end testing it is ideal is to have real versions of your services running and interacting together; however, sometimes this is not always feasible. Therefore, it is okay to stub out some entities of a\n\nQuestions:", "completion": "Q1. What is the purpose of End-to-End Testing? \nA1. The purpose of End-to-End Testing is to test services and their dependencies together to verify they work as expected. \nWrong Answers:\n-To test UI and UX of the application. \n-To identify security vulnerabilities. \n-To measure performance of the system."}
{"prompt": "Context: system that are outside your organization\u2019s domain and are provided by an external party. A hypothetical case would be if the Attendee service required the use of AWS S3. Relying on an external entity opens up concerns such as network issues or even the external provider being unavailable. Also, if your tests are not going to use an entity, there is no need to make it available for your test. For an end-to-end test of the Attendee service, the database and the Attendee service need to be launched, but this does not require the legacy conference system, as it is superfluous. This is why end-to-end tests sometimes require boundaries. The boundary for this end-to-end test is shown in Figure 2-8.      Figure 2-8. End-to-end test scope      Managing and coordinating multiple systems together is not easy to automate and end-to-end tests can be brittle. However, running end-to-end tests locally is becoming easier. As you have just seen in \u201cContainerizing Test Components: Testcontainers\u201d, containerization allows you to spin up multiple systems locally. Even though this is getting easier, you should still follow the test pyramid guidelines\u2014end-to-end tests are at the top of the test pyramid for a reason.  When writing your end-to-end tests, you should use realistic payloads. We have seen cases where tests use small and concise payloads, then when investigating why APIs are breaking it is found that the consumers are regularly sending very large payloads\u2014larger than the buffers\n\nQuestions:", "completion": "Q1: What is an example of an external system that may be used in an end-to-end test?\n\nA1: AWS S3 \nIncorrect Answers: MySQL, Java, HTML \n\nQ2: What is the boundary for an end-to-end test?\n\nA2: The database and the Attendee service \nIncorrect Answers: The legacy conference system, The external provider, The network issues"}
{"prompt": "Context: support. This is why your end-to-end testing needs to be representative of the way that a consumer uses your API.      Types of End-to-End Tests   The end-to-end tests that you write should be driven off the requirements that are most important, as you saw in \u201cTest Quadrant\u201d.  Within Q3 of the test quadrant, you see scenario testing. Scenario tests are a common form of end-to-end testing. They are for testing out typical user journeys and provide confidence that your service is performing correctly. A scenario test can be based around a single action or multiple actions. It is important that you are only testing core user journeys and not testing edge cases or exception testing. To help you write your tests, you can use Behavior Driven Development (BDD). This is a nice way to write your user stories as part of your business-facing tests. An example for the conference system would be that when an attendee is registered for a conference talk, the attendee count should have increased when the conference talk information is retrieved.  The nice thing with scenario tests and validating these core user journeys is that you are not going to be concerned if a component is slower than in production. What is being interrogated is the correct behavior and expected results. However, you need to be more careful when running performance end-to-end tests. Performance testing, Q4 in the testing quadrant, should be deployed to a like-for-like environment of your production environment. If the\n\nQuestions:", "completion": "Q1. What is the primary purpose of end-to-end testing?\nA1. The primary purpose of end-to-end testing is to provide confidence that a service is performing correctly by testing out typical user journeys.\nWrong Answers: \n1. To test the performance of an application \n2. To test the reliability of an application \n3. To test the usability of an application\n\nQ2. What is an example of a scenario test?\nA2. An example of a scenario test is to test if the attendee count has increased when a conference talk is registered. \nWrong Answers:\n1. To test the performance of an application \n2. To check for bugs in the code\n3. To test if a web page is loading correctly\n\nQ3. What type of test is best for testing edge cases and exception testing?\nA3. Unit tests are best for testing edge cases and exception testing.\nWrong Answers: \n1. Integration tests \n2. Performance tests \n3. Scenario tests"}
{"prompt": "Context: two differ, you will not get results indicative of how your services are working. This does mean that you are going to need to deploy your services to this representative hardware, and depending on your resources and environment, that can be tricky. You should take this into consideration if it is going to make your tests flaky or is going to cost more development time than the return of confidence. However, this should not put you off because we have seen this type of end-to-end testing be successful.  The performance tests that you write as part of your end-to-end testing should be focused on ensuring that you are still serving requests within your targeted SLOs. You want these performance tests to show you have not introduced any sudden lag to your services (e.g., accidentally adding in some blocking code). If volume is important, you want to verify that your service is able to handle the loads you expect. Some great tools are available for performance testing, such as Gatling, JMeter, Locust, and K6. Even if none of these appeal to you, others are available and in many different languages that you should be familiar with. The performance figures that you want should be driven from your business requirements.  As part of your end-to-end testing, you should also ensure that your security is in place (i.e., TLS is turned on and the appropriate authentication mechanisms are in place). Security should not be turned off for these tests as it does not make it representative of a\n\nQuestions:", "completion": "1. How should performance tests be written as part of end-to-end testing?\n\nCorrect Answer: Performance tests should be focused on ensuring that requests are still being served within the targeted SLOs.\n\nWrong Answers: \na. Performance tests should be focused on ensuring that the code is bug-free.\nb. Performance tests should be focused on ensuring that the user interface is intuitive.\nc. Performance tests should be focused on ensuring that the code is optimized for speed.\n\n2. What tools should be used for performance testing?\n\nCorrect Answer: Tools such as Gatling, JMeter, Locust, and K6 are great for performance testing.\n\nWrong Answers: \na. Tools such as Selenium and Postman are great for performance testing.\nb. Tools such as Jenkins and Docker are great for performance testing.\nc. Tools such as Visual Studio and Eclipse are great for performance testing.\n\n3. What should performance figures be driven from?\n\nCorrect Answer: Performance figures should be driven from business requirements.\n\nWrong Answers: \na. Performance figures should be driven from user feedback.\nb. Performance figures should be driven from competitive analysis.\nc. Performance figures should be driven from design principles."}
{"prompt": "Context: user journey, or it misrepresents metrics.  End-to-end testing is more complex than any other type of testing as it takes resources to create and maintain. Though it can be a time saver over doing end-to-end manual testing, it provides confidence in the application and evidence that services were working from a technical standpoint to meet service agreements.      ADR Guideline: End-to-End Testing   Knowing what to include, and whether end-to-end testing is worthwhile for your case, are important considerations. The ADR Guidelines in Table 2-4 should help you make a decision.  Table 2-4. ADR Guideline: End-to-End Testing Decision  As part of your testing setup, should you use automated end-to-end tests?    Discussion Points  Determine how complex your setup is to enable end-to-end testing. Do you have a good idea of end-to-end tests that you require and will provide value? Are there any specific requirements or more advanced end-to-end tests that you should add?      Recommendations  We recommend that you do perform at a minimum end-to-end testing on core user journeys. This is going to give feedback as early as possible in your development cycle that a user could be impacted with the changes that have been made. Ideally you can run these end-to-end tests locally; however, if not, then it should be part of your build pipeline.  End-to-end testing is valuable but must be balanced against the time investment you need to get it running. If it is not possible to do automated end-\n\nQuestions:", "completion": "Q1: What is the purpose of end-to-end testing?\nA1: The purpose of end-to-end testing is to provide confidence in the application and evidence that services were working from a technical standpoint to meet service agreements.\nWrong Answers:\n- To identify user journey issues\n- To measure metrics\n- To save time over manual testing"}
{"prompt": "Context: to-end testing, then you need to have a run book of manual tests that you can use. This run book should be used against a testing environment before a production release. This type of manual testing will considerably slow down your production releases and ability to deliver value to customers.      Summary   In this chapter you have learned about the core types of testing for APIs, including what should be tested and where time should be dedicated. The key takeaways are:  Stick to the fundamentals of testing and make unit testing a core of your API.    Contract testing can help you develop a consistent API and test with other APIs.    Perform service tests on your component and isolate the integrations to validate incoming and outgoing traffic.    Use end-to-end tests to replicate core user journeys to help validate that your APIs all integrate correctly.    Use the ADR Guidelines as a way to work out if you should add different tests to your API.      While we\u2019ve given you lots of information, ideas, and techniques for testing your API, this is by no means an exhaustive list of tools available. We encourage you to do some research on testing frameworks and libraries that you may want to use, to ensure you are making an informed decision.  However, no matter how much testing is done upfront, nothing is as good as seeing how an application actually runs in production. You will learn more about testing in production in Chapter 5. The next chapter will focus on exposing and\n\nQuestions:", "completion": "Q1. What type of testing should be used to validate core user journeys?\nA1. End-to-end testing \nWrong Answers: \n- Performance testing\n- Unit testing \n- Contract testing\n\nQ2. What type of testing should be used to check the consistency of an API?\nA2. Contract testing\nWrong Answers:\n- Performance testing\n- Unit testing \n- End-to-end testing\n\nQ3. What type of testing should be used to check the performance of an API?\nA3. Performance testing \nWrong Answers:\n- Contract testing \n- Unit testing \n- End-to-end testing\n\nQ4. What type of testing should be used to test individual components?\nA4. Unit testing \nWrong Answers:\n- Performance testing \n- Contract testing\n- End-to-end testing\n\nQ5. What type of guidelines should you use to determine if additional tests should be added?\nA5. ADR Guidelines \nWrong Answers:\n- Performance testing Guidelines\n- Unit testing Guidelines \n- Contract testing Guidelines"}
