{"prompt": "Context: Mastering API Architecture   Design, Operate, and Evolve API-Based Systems   James Gough, Daniel Bryant, and Matthew Auburn      Mastering API Architecture   by James Gough, Daniel Bryant, and Matthew Auburn  Copyright \u00a9 2023 James Gough Ltd, Big Picture Tech Ltd, and Matthew Auburn Ltd. All rights reserved.  Printed in the United States of America.  Published by O\u2019Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.  O\u2019Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.          Acquisitions Editor: Melissa Duffield          Development Editor: Virginia Wilson          Production Editor: Clare Laylock          Copyeditor: Kim Cofer          Proofreader: Amnet Systems LLC          Indexer: nSight, Inc.          Interior Designer: David Futato          Cover Designer: Karen Montgomery          Illustrator: Kate Dullea            October 2022: First Edition      Revision History for the First Edition           2022-10-17: First Release      See http://oreilly.com/catalog/errata.csp?isbn=9781492090632 for release details.  The O\u2019Reilly logo is a registered trademark of O\u2019Reilly Media, Inc. Mastering API Architecture, the cover image, and related trade dress are trademarks of O\u2019Reilly Media, Inc.  While the publisher and the authors have used\n\nQUESTIONS:", "completion": "Q: Who wrote Mastering API Architecture?\nA: James Gough, Daniel Bryant, and Matthew Auburn\nWA1: John Smith\nWA2: Jane Doe\nWA3: Bob Jones\n\nQ: What year was Mastering API Architecture published?\nA: 2023\nWA1: 2022\nWA2: 2021\nWA3: 2020\n\nQ: Who was the Acquisitions Editor for Mastering API Architecture?\nA: Melissa Duffield\nWA1: Virginia Wilson\nWA2: Clare Laylock\nWA3: Kim Cofer"}
{"prompt": "Context: good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.    978-1-492-09063-2  [LSI]      Dedication   This book is dedicated to Alex Blewitt, who unfortunately passed away before publication. We would like to thank Alex for his candid feedback, constant support, and warm friendship over the years.  \u2014The Authors      Foreword   When I built my first APIs at the Financial Times, over a decade ago, there weren\u2019t too many of them. We were building on a monolithic architecture, and the APIs were solely there for external third parties to get access to our content.  Now though, APIs are everywhere and they are core to your success when building a system.  That\u2019s because, over the last decade, a couple of things have combined to change the way many of us do software development.  Firstly, the technology available for us changed. The rise of cloud computing gave us self-service, on-demand provisioning. Automated\n\nQUESTIONS:", "completion": "Q: What is the purpose of this work?\nA: To provide accurate information and instructions.\nWA1: To provide incorrect information and instructions.\nWA2: To provide entertainment.\nWA3: To provide advice.\n\nQ: Who is the book dedicated to?\nA: Alex Blewitt.\nWA1: Alex Brown.\nWA2: Alex Smith.\nWA3: Alex Johnson.\n\nQ: What has changed in the last decade regarding software development?\nA: The technology available for us changed.\nWA1: The workforce changed.\nWA2: The platforms changed.\nWA3: The demand increased."}
{"prompt": "Context: build and deployment pipelines allowed us to do continuous integration and deployment, and containers and associated technologies like orchestration let us run large numbers of small, independent services as a distributed system.  Why are we doing that? Because of the second thing: the research showing that successful software development organizations have loosely coupled architectures and autonomous, empowered teams. Successful here is defined in terms of a positive impact on the business: increased market share, productivity, and profitability.  Our architectures now tend to be more loosely coupled, distributed, and built around APIs. You want your APIs to be discoverable, consistent, and unlikely to cause problems to the consumers even if they change unexpectedly or disappear. Anything else will couple work together and slow down your teams.  In this book, James, Daniel and Matthew provide a comprehensive and practical guide to building effective API architectures. They cover a lot of ground, from how to build and test an individual API, through the ecosystem you deploy them into, the ways to release and operate them effectively, and perhaps most importantly, how to use APIs to evolve your architecture. Those first APIs I built at the Financial Times don\u2019t exist anymore, and we built those systems again from scratch. That\u2019s costly. James, Daniel, and Matthew provide a template for how to deal with inevitable change and evolve your systems, using APIs as a key tool.\n\nQUESTIONS:", "completion": "Q: What is the benefit of using build and deployment pipelines?\nA: Continuous integration and deployment.\nWA1: Automated bug fixing.\nWA2: Efficiency in development process.\nWA3: Increased market share.\n\nQ: What are the positive impacts of successful software development organizations?\nA: Increased market share, productivity, and profitability.\nWA1: Faster development process.\nWA2: Automated bug fixing.\nWA3: Increased customer satisfaction.\n\nQ: What are the qualities of an API that will help teams work faster?\nA: Discoverable, consistent, and unlikely to cause problems to the consumers even if they change unexpectedly or disappear.\nWA1: Easily testable.\nWA2: Easy to develop.\nWA3: High performance."}
{"prompt": "Context: Software architecture has been defined as those decisions that are both important and hard to change. These are the decisions that will make your project succeed\u2014or fail.  The authors\u2019 focus is not on architecture in the abstract, but on how you apply architecture within your own organizations. Deciding to adopt an API gateway or a service mesh, and which one, is exactly the kind of hard-to-undo decision that you should approach with caution and evaluate carefully. James, Daniel, and Matthew give strong, opinionated guidance where they feel it is appropriate, and where the options are less clearcut, they provide a framework to help you make the best choice for your circumstances.  They illustrate throughout with a practical and realistic case study that takes the concepts and shows how you actually make them work in practice. Their case study evolves throughout the book, in the same way real systems do. The authors show that you don\u2019t have to do everything upfront; you can evolve your architecture piece by piece, extracting services and adding tools like API gateways and service meshes as you find you need them.  When I built my first APIs, I made a lot of mistakes. I wish I\u2019d had a book like this, to help me understand where I might trip up, and to guide me towards sensible decisions.  I recommend this book to anyone leading work on systems where APIs play a major role. With it, you should be able to develop a consistent set of tools and standards to support every team\n\nQUESTIONS:", "completion": "Q: What is the main focus of the authors in this book?\nA: How to apply architecture within organizations\nWA1: To develop systems with APIs\nWA2: To provide opinionated guidance\nWA3: To illustrate with a case study\n\nQ: What is the authors' advice on making difficult decisions?\nA: Approach with caution and evaluate carefully\nWA1: Make the decision immediately\nWA2: Do not make the decision at all\nWA3: Make the decision without research"}
{"prompt": "Context: building APIs in your organization.  Sarah Wells, Co-Chair of the QCon London conference, independent consultant, and former Technical Director at the Financial Times, Reading, UK, September 2022      Preface    Why Did We Write This Book?   In early 2020 we attended O\u2019Reilly Software Architecture in New York, where Jim and Matt gave a workshop on APIs and a presentation on API gateways. Jim and Daniel know each other from the London Java Community, and like at many architecture events, we got together to talk about our thoughts and understanding around API architectures. As we were talking on the hallway track, several conference delegates came up to us and chatted about their experiences with APIs. People were asking for our thoughts and guidance on their API journey. It was at this point that we thought writing a book on the topic of APIs would help share our discussions from conferences with other architects.      Why Should You Read This Book?   This book has been designed to provide a complete picture on designing, operating, and evolving an API architecture. We have shared our experience and advice through both our writing and an accompanying case study that mimics a real-life event-management conference system that enables attendees to view and book presentation sessions. The case study runs throughout the book, with the goal of you exploring how abstract concepts sometimes translate into practical application. If you want a high-level overview of the evolution of the\n\nQUESTIONS:", "completion": "Q: What was the purpose of writing this book?\nA: To share discussions from conferences and provide a complete picture on designing, operating, and evolving an API architecture.\nWA1: To provide a high-level overview of the evolution of the API industry.\nWA2: To give advice to conference attendees on their API journey.\nWA3: To introduce Jim and Matt's workshop on APIs and API Gateways.\n\nQ: What is the goal of the accompanying case study?\nA: To explore how abstract concepts sometimes translate into practical application.\nWA1: To provide an example of a real-life event-management conference system.\nWA2: To help the reader understand the evolution of the API industry.\nWA3: To give the reader an understanding of Jim and Matt's workshop on APIs and API Gateways."}
{"prompt": "Context: case study, you can find this in Chapter 10.  We also believe in allowing you to make your own decisions. To support this, we will:  Be clear when we have a strong recommendation or guidance.    Highlight areas of caution and problems that you may encounter.    Supply an Architecture Decision Record (ADR) Guideline1 to help inform the best possible decision given the circumstances of your architecture and provide guidance on what to consider (because sometimes the answer is \u201cit depends\u201d).    Highlight references and useful articles where you can find more in-depth content.      The book is not just a greenfield technology book. We felt that covering existing architectures with an evolutionary approach toward more suitable API architectures would provide the most benefit for you. We also tried to balance this with looking forward to newer technologies and developments in the API architecture domain.      Who This Book Is For   Although we had an initial persona in mind when creating this book, during the writing and reviewing process three key personas emerged: the developer, an accidental architect, and the solutions or enterprise architect. We have outlined these personas in the following sections, with the aim that you not only identify with at least one of them, but also so that you can look at each chapter through the different lens these personas provide.      Developer   You have most likely been coding professionally for several years and have a good understanding of\n\nQUESTIONS:", "completion": "Q: What is the main focus of the book?\nA: To provide guidance on making the best possible decision given the circumstances of one's architecture.\nWA1: To provide an overview of greenfield technology.\nWA2: To provide an overview of existing architectures.\nWA3: To provide an overview of newer technologies and developments.\n\nQ: Who is the book for?\nA: Developers, accidental architects, and solutions or enterprise architects.\nWA1: Business analysts.\nWA2: Software engineers.\nWA3: Project managers."}
{"prompt": "Context: common software development challenges, patterns, and best practices. You are increasingly realizing that the software industry\u2019s march toward building service-oriented architecture (SOA) and adopting cloud services means that building and operating APIs is fast becoming a core skill. You are keen to learn more about designing effective APIs and testing them. You want to explore the various implementation choices (e.g., synchronous versus asynchronous communication) and technologies and learn how to ask the right questions and evaluate which approach is best for a given context.      Accidental Architect   You have most likely been developing software for many years and have often operated as a team lead or resident software architect (even if you don\u2019t have the official titles). You understand core architectural concepts, such as designing for high cohesion and loose coupling, and apply these to all aspects of software development, including design, testing, and operating systems.  You realize that your role is increasingly focused on combining systems to meet customer requirements. This could include internally built applications and third-party SaaS-type offerings. APIs play a big part in successfully integrating your systems with external systems. You want to learn more about the supporting technologies (e.g., API gateway, service mesh, etc.) and also understand how to operate and secure API-based systems.      Solutions/Enterprise Architect   You have been designing and\n\nQUESTIONS:", "completion": "Q: What technology is increasingly becoming a core skill for software developers?\nA: Building and operating APIs\nWA1: Writing code\nWA2: Developing websites\nWA3: Designing for high cohesion\n\nQ: What is the role of APIs in successfully integrating systems with external systems?\nA: APIs play a big part in successfully integrating systems with external systems\nWA1: Connecting systems with internal applications\nWA2: Automating processes\nWA3: Creating service-oriented architecture"}
{"prompt": "Context: building enterprise software systems for several years and most likely have the word architect in your job title or role description. You are responsible for the big picture of software delivery and typically work within the context of a large organization or a series of large interconnected organizations.  You recognize the changes that the latest iteration of service-based architectural styles are having on the design, integration, and governance of software, and you see APIs are pivotal to the success of your organization\u2019s software strategy. You are keen to learn more about evolutionary patterns and understand how the choice of API design and implementation will impact this. You also want to focus on the cross-functional \u201cilities\u201d\u2014usability, maintainability, scalability, and availability\u2014and understand how to build API-based systems that exhibit such properties, as well as provide security.      What You Will Learn   After reading this book you will understand:  The fundamentals of REST APIs and how to best build, version, and test APIs    The architectural patterns involved in building an API platform    The differences in managing API traffic at ingress and within service-to-service communication, and how to apply patterns and technologies such as API gateways and service meshes    Threat modeling and key security considerations for APIs, such as authentication, authorization, and encryption    How to evolve existing systems toward APIs and different target deployments,\n\nQUESTIONS:", "completion": "Q: What is the purpose of this book?\nA: After reading this book you will understand the fundamentals of REST APIs and how to best build, version, and test APIs, the architectural patterns involved in building an API platform, the differences in managing API traffic at ingress and within service-to-service communication, and how to apply patterns and technologies such as API gateways and service meshes, threat modeling and key security considerations for APIs, such as authentication, authorization, and encryption, and how to evolve existing systems toward APIs and different target deployments.\nWA1: To provide an introduction to software engineering\nWA2: To teach users how to design and develop web applications\nWA3: To provide an overview of cloud computing technologies\n\nQ: Who is this book most relevant to?\nA: Individuals who have been building enterprise software systems for several years and most likely have the word architect in their job title or role description. \nWA1: Beginner-level software developers\nWA2: Experienced web developers\nWA3: Data analysts"}
{"prompt": "Context: such as the cloud      And you will be able to:  Design, build, and test API-based systems    Help to implement and drive an organization\u2019s API program from an architectural perspective    Deploy, release, and configure key components of an API platform    Deploy gateways and service meshes based on case studies    Identify vulnerabilities in API architecture and implement measured security mitigations    Contribute to emerging API trends and the associated communities      What This Book Is Not   We realize that APIs encompass a vast market space and we want to be clear what this book will not cover. That doesn\u2019t mean to say that we believe these topics are not important; however, if we tried to cover everything we wouldn\u2019t be able to share our knowledge effectively with you.  We will cover application patterns for migration and modernization that will include taking advantage of cloud platforms, but the book is not wholly focused on cloud technologies. Many of you will have hybrid architectures or even have all of your systems hosted in data centers. We want to ensure that we cover the design and operational factors of API architectures that support both approaches.  The book is not tied to a specific language but will use some lightweight examples to demonstrate approaches to building/designing APIs and their corresponding infrastructure. The book will focus more on the approach, and code examples will be available in the accompanying GitHub repository.  The book does not\n\nQUESTIONS:", "completion": "Q: What will you be able to do after reading this book?\nA: Design, build, and test API-based systems; Help to implement and drive an organizations API program from an architectural perspective; Deploy, release, and configure key components of an API platform; Deploy gateways and service meshes based on case studies; Identify vulnerabilities in API architecture and implement measured security mitigations; Contribute to emerging API trends and the associated communities\nWA1: Create complex software systems\nWA2: Develop a secure API\nWA3: Design a cloud-based system\n\nQ: What topics will this book not cover?\nA: We will cover application patterns for migration and modernization that will include taking advantage of cloud platforms, but the book is not wholly focused on cloud technologies. Many of you will have hybrid architectures or even have all of your systems hosted in data centers. We want to ensure that we cover the design and operational factors of API architectures that support both approaches.\nWA1: Cloud technologies \nWA2: Design and operational factors of API architectures \nWA3: Application patterns for modernization"}
{"prompt": "Context: favor one style of architecture over another, however we will discuss situations in which architectural approaches may cause limitations to the API offering presented.      Conventions Used in This Book   The following typographical conventions are used in this book:  Italic  Indicates new terms, URLs, email addresses, filenames, and file extensions.    Constant width  Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.    Constant width bold  Shows commands or other text that should be typed literally by the user.    Constant width italic  Shows text that should be replaced with user-supplied values or by values determined by context.      Tip   This element signifies a tip or suggestion.      Note   This element signifies a general note.      Warning   This element indicates a warning or caution.      Using Code Examples   Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/masteringapi.  If you have a technical question or a problem using the code examples, please send an email to bookquestions@oreilly.com.  This book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for permission unless you\u2019re reproducing a significant portion of the code. For example, writing\n\nQUESTIONS:", "completion": "Q: What typographical conventions are used in this book?\nA: Italic, constant width, constant width bold, and constant width italic.\nWA1: Dotted lines, bold letters, italic typeface, and underlining.\nWA2: Bold, italic, underline, and strikethrough.\nWA3: Italic, bold, and underlined.\n\nQ: How can readers get supplemental material for this book?\nA: Download it from https://github.com/masteringapi.\nWA1: Contact the publisher for a copy.\nWA2: Send an email to bookquestions@oreilly.com.\nWA3: Purchase it from an online store."}
{"prompt": "Context: a program that uses several chunks of code from this book does not require permission. Selling or distributing examples from O\u2019Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a significant amount of example code from this book into your product\u2019s documentation does require permission.  We appreciate, but generally do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: \u201cMastering API Architecture by James Gough, Daniel Bryant, and Matthew Auburn (O\u2019Reilly). Copyright 2023 James Gough Ltd, Big Picture Tech Ltd, and Matthew Auburn Ltd, 978-1-492-09063-2.\u201d  If you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com.      O\u2019Reilly Online Learning    Note   For more than 40 years, O\u2019Reilly Media has provided technology and business training, knowledge, and insight to help companies succeed.    Our unique network of experts and innovators share their knowledge and expertise through books, articles, and our online learning platform. O\u2019Reilly\u2019s online learning platform gives you on-demand access to live training courses, in-depth learning paths, interactive coding environments, and a vast collection of text and video from O\u2019Reilly and 200+ other publishers. For more information, visit https://oreilly.com.      How to Contact Us   Please address\n\nQUESTIONS:", "completion": "Q: Does using code from OReilly books require permission?\nA: Selling or distributing examples from OReilly books does require permission.\nWA1: No permission is needed.\nWA2: Attribution is necessary.\nWA3: No permission is needed but attribution is required.\n\nQ: Is attribution usually required when using OReilly code?\nA: We appreciate, but generally do not require, attribution.\nWA1: No attribution is necessary.\nWA2: Attribution is necessary.\nWA3: Attribution is optional."}
{"prompt": "Context: comments and questions concerning this book to the publisher:  O\u2019Reilly Media, Inc.  1005 Gravenstein Highway North  Sebastopol, CA 95472  800-998-9938 (in the United States or Canada)  707-829-0515 (international or local)  707-829-0104 (fax)    We have a web page for this book, where we list errata, examples, and any additional information. You can access this page at https://oreil.ly/Mastering-API-Architecture.  Email bookquestions@oreilly.com to comment or ask technical questions about this book.  For news and information about our books and courses, visit https://oreilly.com.  Find us on LinkedIn: https://linkedin.com/company/oreilly-media.  Follow us on Twitter: https://twitter.com/oreillymedia.  Watch us on YouTube: https://www.youtube.com/oreillymedia.      Acknowledgments   As with almost all technical books, only three names may be listed as authors on the front of this book, but the reality is that many people have contributed, either directly in the form of feedback as the book was written, or indirectly by their teaching and guidance over the years.  Although we can\u2019t possibly list everyone who has helped us during this journey, we would like to explicitly thank the people who took time out of their busy schedules to provide extensive discussions, feedback, and support.  Our technical reviewers: Sam Newman, Dov Katz, Sarah Wells, Antoine Cailliau, Stefania Chaplin, Matt McLarty, and Neal Ford.  For general detail, encouragement, advice, and introductions: Charles\n\nQUESTIONS:", "completion": "Q: What is the website for the publisher of this book?\nA: https://oreil.ly/Mastering-API-Architecture\nWA1: https://www.youtube.com/oreillymedia\nWA2: https://linkedin.com/company/oreilly-media\nWA3: https://twitter.com/oreillymedia\n\nQ: What is the email address to comment or ask technical questions about this book?\nA: bookquestions@oreilly.com\nWA1: info@oreilly.com\nWA2: oreilly@gmail.com\nWA3: bookquestions@gmail.com"}
{"prompt": "Context: Humble, Richard Li, Simon Brown, Nick Ebbitt, Jason Morgan, Nic Jackson, Cliff Tiltman, Elspeth Minty, George Ball, Benjamin Evans, and Martijn Verberg.  The O\u2019Reilly Team: Virginia Wilson, Melissa Duffield, and Nicole Tache.      James Gough   I would like to thank my incredible family: Megan, Emily, and Anna. Writing would not have been possible without their help and support. I\u2019d also like to thank my parents, Heather and Paul, for encouraging me to learn and for their constant support.  I\u2019d like to thank my coauthors Daniel and Matt; writing a book is a challenge and, like architecture, is never perfect. It has been a fun journey and we have all learned a lot from each other and our amazing reviewers. Finally, I\u2019d like to thank Jon Daplyn, Ed Safo, David Halliwell, and Dov Katz for providing me support, opportunities, and encouragement throughout my career.      Daniel Bryant   I would like to thank my entire family for their love and support, both during the authoring process and throughout my career. I would also like to thank Jim and Matt for being great partners on this writing journey. We started this book in early 2020, just as the pandemic hit. Our weekly Wednesday morning calls were not only useful for collaboration but also a great source of fun and support as our worlds rapidly changed. Finally, I would like to thank everyone involved in the London Java Community (LJC), the InfoQ/QCon teams, and everyone at Ambassador Labs. These three communities have provided\n\nQUESTIONS:", "completion": "Q: Who are the authors of the book?\nA: Humble, Richard Li, Simon Brown, Nick Ebbitt, Jason Morgan, Nic Jackson, Cliff Tiltman, Elspeth Minty, George Ball, Benjamin Evans, and Martijn Verberg.\nWA1: Virginia Wilson, Melissa Duffield, and Nicole Tache.\nWA2: James Gough\nWA3: Daniel Bryant\n\nQ: Who are the editors of the book?\nA: Virginia Wilson, Melissa Duffield, and Nicole Tache.\nWA1: Humble, Richard Li, Simon Brown, Nick Ebbitt, Jason Morgan, Nic Jackson, Cliff Tiltman, Elspeth Minty, George Ball, Benjamin Evans, and Martijn Verberg.\nWA2: James Gough\nWA3: Daniel Bryant\n\nQ: Who are the people thanked by the author?\nA: Megan, Emily, and Anna, Heather and Paul, Daniel and Matt, Jon Daplyn, Ed Safo, David Halliwell, and Dov Katz\nWA1: Virginia Wilson, Melissa Duffield, and Nicole Tache.\nWA2: James Gough\nWA3: Humble, Richard Li, Simon Brown, Nick Ebbitt, Jason Morgan, Nic Jackson, Cliff Tiltman, Elspeth Minty, George Ball, Benjamin Evans, and Martijn Verberg."}
{"prompt": "Context: me with access to mentors, guidance, and so many opportunities. I hope to someday pay all of this forward.      Matthew Auburn   I would like to thank my amazing wife Hannah\u2014without your support I would not have been able to write this book. Thank you to both of my parents; you have shown me that anything is possible and never stopped believing in me. This book has been such an amazing experience to go through, and Jim and Dan, you have both been excellent mentors. Both of you have taught me so much and helped me write the best material possible. An additional thanks to Jim\u2014you introduced me to speaking and have helped me more than anyone else in my career. Finally and most importantly, I want to thank my son Joshi: you are an absolute joy.      1 You will learn more about ADRs and their importance to making and documenting architectural decisions in the Introduction.      Introduction   In this introduction, you will discover the basics of APIs and their potential to be part of the architecture journey. We will introduce a lightweight definition for APIs and their use in and out of process. In order to demonstrate the importance of APIs, we will introduce the conference system case study, a running example that will evolve throughout the book. Out-of-process APIs allow you to think beyond a simple three-tiered architecture; we will introduce traffic patterns and their importance to demonstrate this. We will outline a summary of the case study steps, allowing you to skip\n\nQUESTIONS:", "completion": "Q: Who introduced Matthew Auburn to speaking?\nA: Jim\nWA1: Hannah\nWA2: Dan\nWA3: Joshi\n\nQ: What will be introduced in the Introduction?\nA: ADRs and their importance to making and documenting architectural decisions\nWA1: APIs and their potential to be part of the architecture journey\nWA2: A lightweight definition for APIs\nWA3: Traffic patterns and their importance"}
{"prompt": "Context: ahead if an area is of interest to you straight away.  In order to present APIs and their associated ecosystem, we will use a series of important artifacts. We will introduce the case study with C4 model diagrams and revisit the specifics and logic behind the approach. You will also learn about the use of Architecture Decision Records (ADRs) and the value of clearly defining decisions across the software lifecycle. As the introduction closes, we will outline ADR Guidelines\u2014our approach to help you make decisions when the answer is \u201cit depends.\u201d      The Architecture Journey   Anyone who has taken a long journey will no doubt have encountered the question (and possibly persistently) \u201care we there yet?\u201d For the first few inquiries, you look at the GPS or a route planner and provide an estimate\u2014hoping that you don\u2019t encounter any delays along the way. Similarly, the journey to building API-based architectures can be complex for developers and architects to navigate; even if there was an Architecture GPS, what would your destination be?  Architecture is a journey without a destination, and you cannot predict how technologies and architectural approaches will change. For example, you may not have been able to predict service mesh technology would become so widely used, but once you learn about its capabilities it may cause you to think about evolving your existing architecture. It is not only technologies that influence change in architecture; new business requirements and\n\nQUESTIONS:", "completion": "Q: What is the main purpose of the introduction?\nA: To outline ADR Guidelines and provide an approach to help make decisions when the answer is it depends.\nWA1: To introduce C4 model diagrams\nWA2: To present APIs and their associated ecosystem\nWA3: To provide an estimate of the journey to building API-based architectures\n\nQ: What is the difference between the architecture journey and a long journey?\nA: The architecture journey does not have a destination.\nWA1: The architecture journey is longer than a long journey.\nWA2: The architecture journey is guided by GPS.\nWA3: The architecture journey does not encounter any delays."}
{"prompt": "Context: constraints also drive change in architectural direction.  The culminating effect of delivering incremental value combined with new emerging technologies leads to the concept of evolutionary architecture. Evolutionary architecture is an approach to incrementally changing an architecture, focusing on the ability to change with speed and reducing the risk of negative impacts. Along the way, we ask you to keep the following advice in approaching API architecture in mind:  Though architects like to be able to strategically plan for the future, the constantly changing software development ecosystem makes that difficult. Since we can\u2019t avoid change, we need to exploit it.  Building Evolutionary Architectures by Neal Ford, Rebecca Parsons, and Patrick Kua (O\u2019Reilly)    In many projects APIs themselves are evolutionary, requiring change as more systems and services are integrated. Most developers have built services that focus on a single function without considering the broader API reuse from a consumer perspective.  API-First design is an approach where developers and architects consider the functionality of their service and design an API in a consumer-centric manner. The API consumer could be a mobile application, another service, or even an external customer. In Chapter 1 we will review design techniques to support an API-First approach and discover how we build APIs that are durable to change and deliver value to a broad consumer base.  The good news is that you can start an\n\nQUESTIONS:", "completion": "Q: What is an evolutionary architecture?\nA: An approach to incrementally changing an architecture, focusing on the ability to change with speed and reducing the risk of negative impacts.\nWA1: A type of architecture that requires no change.\nWA2: A type of architecture that is designed to be static.\nWA3: A type of architecture that is designed to be fast.\n\nQ: What is the goal of API-First design?\nA: To design an API in a consumer-centric manner.\nWA1: To design an API with no focus on the consumer.\nWA2: To design an API with no consideration of the consumer.\nWA3: To design an API with no regard to the consumer."}
{"prompt": "Context: API-driven architecture journey at any point. If you are responsible for preexisting technical inventory, we will show you techniques to evolve your architecture to promote the use of APIs in your platform. On the other hand, if you are lucky and have a blank canvas to work with, we will share with you the benefit of adopting API architectures based on our years of experience, while also highlighting key factors in decision making.      A Brief Introduction to APIs   In the field of software architecture, there are a handful of terms that are incredibly difficult to define. The term API, which stands for application programming interface, falls into this categorization, as the concept first surfaced as many as 80 years ago. Terms that have been around for a significant amount of time end up being overused and having multiple meanings in different problem spaces. We consider an API to mean the following:  An API represents an abstraction of the underlying implementation.    An API is represented by a specification that introduces types. Developers can understand the specifications and use tooling to generate code in multiple languages to implement an API consumer (software that consumes an API).    An API has defined semantics or behavior to effectively model the exchange of information.    Effective API design enables extension to customers or third parties for a business integration.      Broadly speaking, APIs can be broken into two general categories depending on whether\n\nQUESTIONS:", "completion": "Q: What are the two general categories of APIs? \nA: Internal APIs and External APIs\nWA1: Software APIs\nWA2: Web APIs\nWA3: Machine APIs\n\nQ: What does API stand for?\nA: Application Programming Interface\nWA1: Automatic Programming Interface\nWA2: Application Protocol Interface\nWA3: Automated Programming Interface"}
{"prompt": "Context: the API invocation is in process or out of process. The process being referred to here is an operating system (OS) process. For example, a Java method invocation from one class to another is an in-process API invocation, as the call is handled by the same process from which the call was made. A .NET application invocating an external REST-like API using an HTTP library is an out-of-process API invocation, as the call is handled by an additional external process other than the process from which the call was made. Typically, an out-of-process API call will involve data traversing a network, potentially a local network, virtual private cloud (VPC) network, or the internet. We will focus on the latter style of APIs; however, architects will often encounter the requirement to remodel an in-process API to an out-of-process API. In order to demonstrate this concept (and others), we will create a running case study that will evolve throughout the book.      Running Example: Conference System Case Study   We have chosen to model a conference system for our case study because the domain is easily recognizable but also provides enough complexity for modeling an evolutionary architecture. Figure I-1 visualizes the conference system at the top level, allowing us to set the context of the architecture under discussion. The system is used by an external customer to create their attendee account, review the conference sessions available, and book their attendance.      Figure I-1. C4\n\nQUESTIONS:", "completion": "Q: What is an example of an in-process API invocation? \nA: A Java method invocation from one class to another.\nWA1: A .NET application invoking an external REST-like API.\nWA2: Data traversing a network.\nWA3: Modelling an evolutionary architecture.\n\nQ: What is the running case study used throughout the book?\nA: A conference system.\nWA1: An operating system process.\nWA2: A virtual private cloud network.\nWA3: An external customer."}
{"prompt": "Context: conference system context diagram      Let\u2019s zoom in to the conference system box in Figure I-2. Expanding the conference system provides us more detail about its major technical building blocks. The customer interacts with the web application, which invokes APIs on the conference application. The conference application uses SQL to query the backing datastore.      Figure I-2. C4 conference system container diagram      Figure I-2 reveals that from an API perspective the most interesting functionality is within the conference application container. Figure I-3 zooms in to this specific container, allowing you to explore the structure and interactions.  Four major components and the database are involved in the current system. The API Controller faces all incoming traffic from the UI and makes a decision about where to route the request in the system. This component would also be responsible for marshaling from the on the wire network-level representation to an object or representation in code. The API Controller component is intriguing from the perspective of in-process routing and acting as a junction point or front controller pattern. For API requests and processing, this is an important pattern; all requests pass through the controller, which makes a decision on where the request is directed. In Chapter 3 we will look at the potential for taking the controller out of process.  The Attendee, Booking, and Session components are involved in translating the requests into\n\nQUESTIONS:", "completion": "Q: What is the purpose of the API Controller component in Figure I-2?\nA: To route incoming requests from the UI and make decisions on where the request is directed.\nWA1: To provide the customer with the web application. \nWA2: To use SQL to query the backing datastore.\nWA3: To translate requests into objects or representations in code.\n\nQ: What pattern is the API Controller component responsible for?\nA: The front controller pattern.\nWA1: The network-level representation pattern. \nWA2: The junction point pattern.\nWA3: The in-process routing pattern."}
{"prompt": "Context: queries and execute SQL against the database out of process. In the existing architecture, the database is an important component, potentially enforcing relationships\u2014for example, constraints between bookings and sessions.      Figure I-3. C4 conference system component diagram      Now that we have drilled down to the appropriate level of detail, let\u2019s revisit the types of API interactions in the case study at this point.      Types of APIs in the Conference Case Study   In Figure I-3 the Web Application to API Controller arrow is an out-of-process call, whereas the API Controller to Attendee Component arrow is an example of an in-process call. All interactions within the Conference Application boundary are examples of in-process calls. The in-process invocation is well defined and restricted by the programming language used to implement the Conference Application. The invocation is compile-time safe (the conditions under which the exchange mechanism are enforced at the time of writing code).      Reasons for Changing the Conference System   The current architectural approach has worked for the conference system for many years, however the conference owner has asked for three improvements, which are driving architectural change:  The conference organizers would like to build a mobile application.    The conference organizers plan to go global with their system, running tens of conferences instead of one per year. In order to facilitate this expansion, they would like to\n\nQUESTIONS:", "completion": "Q: What type of API interactions are present in the conference case study?\nA: In-process and out-of-process calls.\nWA1: Only in-process calls.\nWA2: Only out-of-process calls.\nWA3: No API interactions.\n\nQ: What is the current architectural approach of the conference system?\nA: The database is an important component, potentially enforcing relationships.\nWA1: The application is an important component.\nWA2: The API is an important component.\nWA3: The server is an important component."}
{"prompt": "Context: integrate with an external Call for Papers (CFP) system for managing speakers and their application to present sessions at the conference.    The conference organizers would like to decommission their private data center and instead run the conference system on a cloud platform with global reach.      Our goal is to migrate the conference system to be able to support the new requirements, without impacting the existing production system or rewriting everything in one go.      From Tiered Architecture to Modeling APIs   The starting point of the case study is a typical three-tier architecture, composed of a UI, a server-side processing tier, and a datastore. To begin to discuss an evolutionary architecture we need a model to think about the way API requests are processed by the components. We need a model/abstraction that will work for both the public cloud, virtual machines in a data center and a hybrid approach.  The abstraction of traffic will allow us to consider out-of-process interactions between an API consumer and an API service, sometimes referred to as the API producer. With architectural approaches like service-oriented architecture (SOA) and microservices-based architecture, the importance of modeling API interactions is critical. Learning about API traffic and the style of communication between components will be the difference between realizing the advantages of increased decoupling or creating a maintenance nightmare.      Warning   Traffic patterns are used by\n\nQUESTIONS:", "completion": "Q: What is the goal of the case study?\nA: To migrate the conference system to be able to support the new requirements, without impacting the existing production system or rewriting everything in one go.\nWA1: To decommission their private data center and run the conference system on a cloud platform.\nWA2: To build a three-tier architecture.\nWA3: To create a maintenance nightmare.\n\nQ: What is the starting point of the case study?\nA: A typical three-tier architecture, composed of a UI, a server-side processing tier, and a datastore.\nWA1: Modeling APIs.\nWA2: An API consumer.\nWA3: An API service."}
{"prompt": "Context: data center engineers to describe network exchanges within data centers and between low-level applications. At the API level we are using traffic patterns to describe flows between groups of applications. For the purposes of this book, we are referring to application and API-level traffic patterns.      Case Study: An Evolutionary Step   To start to consider traffic pattern types, it will be useful to take a small evolutionary step in our case study architecture. In Figure I-4 a step has been taken to refactor the Attendee component into an independent service, as opposed to a package or module within the legacy conference system. The conference system now has two traffic flows: the interaction between the customer and the legacy conference system and the interaction between the legacy system and the attendee system.      Figure I-4. C4 conference system context\u2014evolutionary step      North\u2013south traffic   In Figure I-4 interaction between the customer and the legacy conference system is referred to a north\u2013south traffic, and it represents an ingress flow. The customer is using the UI, which is sending requests to the legacy conference system over the internet. This represents a point in our network that is exposed publicly and will be accessed by the UI.1 This means that any component handling north\u2013south traffic must make concrete checks about client identity and also include appropriate challenges before allowing traffic to progress through the system. Chapter 7 will go\n\nQUESTIONS:", "completion": "Q: What type of traffic does the northsouth flow represent in Figure I-4?\nA: An ingress flow\nWA1: An egress flow\nWA2: A lateral flow\nWA3: A vertical flow\n\nQ: What type of checks must be made by components handling northsouth traffic?\nA: Concrete checks about client identity and appropriate challenges\nWA1: Performance checks\nWA2: Accessibility checks\nWA3: Network security checks"}
{"prompt": "Context: into detail about securing north\u2013south API traffic.      East\u2013west traffic   The new interaction between the legacy conference system and the Attendee service introduces an east\u2013west traffic flow to our system. East\u2013west traffic can be thought of as service-to-service style of communication within a group of applications. Most east\u2013west traffic, particularly if the origin is within your wider infrastructure, can be trusted to some degree. Although we can trust the source of the traffic, it is still necessary to consider securing east\u2013west traffic.      API Infrastructure and Traffic Patterns   There are two key infrastructure components present in API-based architectures, which are key to controlling traffic. Controlling and coordinating traffic is often described as traffic management. Generally north\u2013south traffic will be controlled by API gateways, the key subject for Chapter 3.  East\u2013west traffic will often be handled by infrastructure components like Kubernetes or service mesh, the key subject for Chapter 4. Infrastructure components like Kubernetes and service mesh use network abstractions to route to services, requiring services to run inside a managed environment. In some systems east\u2013west traffic is managed by the application itself and service discovery techniques are implemented to locate other systems.      Roadmap for the Conference Case Study   Throughout the course of the book you will observe the following changes or applications of technology to the case\n\nQUESTIONS:", "completion": "Q: What is eastwest traffic?\nA: Eastwest traffic can be thought of as service-to-service style of communication within a group of applications.\nWA1: Eastwest traffic is communication between two machines.\nWA2: Eastwest traffic is communication between two users.\nWA3: Eastwest traffic is communication between two networks.\n\nQ: What is the key subject for Chapter 3?\nA: API gateways.\nWA1: Service mesh.\nWA2: Kubernetes.\nWA3: Service discovery techniques."}
{"prompt": "Context: study:  In Chapter 1 you will explore the design and specification of the Attendee API. We will also present the importance of versioning and modeling exchanges for performance of the Attendee API.    In Chapter 2 you will explore contract and component testing to verify behavior of the Attendee service. You will also see how Testcontainers can help with integration testing.    In Chapter 3 you will look at exposing the Attendee service to consumers using an API gateway. We will also demonstrate how to evolve the conference system using an API gateway on Kubernetes.    In Chapter 4 we will refactor the sessions functionality out of the legacy conference system using a service mesh. You will also learn about how service mesh helps with routing, observability, and security.    In Chapter 5 we will discuss feature flagging and how this can help to evolve the conference system and avoid a coupled deployment and release. You will also explore approaches for modeling releases in the conference system and we will demonstrate the use of Argo Rollouts for the Attendee service.    In Chapter 6 you will explore how to apply threat modeling and mitigate OWASP concerns in the Attendee service.    In Chapter 7 you will look at authentication and authorization and how this is implemented for the Attendee service.    In Chapter 8 you will look at establishing the Attendee service domain boundaries and how different service patterns can help.    In Chapter 9 you will look at cloud adoption\n\nQUESTIONS:", "completion": "Q: What will be discussed in Chapter 1?\nA: The design and specification of the Attendee API, the importance of versioning and modeling exchanges for performance of the Attendee API.\nWA1: Contract and component testing\nWA2: Refactoring the sessions\nWA3: Feature flagging\n\nQ: What will be discussed in Chapter 2?\nA: Contract and component testing to verify behavior of the Attendee service, how Testcontainers can help with integration testing.\nWA1: The design and specification of the Attendee API\nWA2: Refactoring the sessions\nWA3: Feature flagging\n\nQ: What will be discussed in Chapter 3?\nA: Exposing the Attendee service to consumers using an API gateway, evolving the conference system using an API gateway on Kubernetes.\nWA1: Contract and component testing\nWA2: Refactoring the sessions\nWA3: Authentication and authorization\n\nQ: What will be discussed in Chapter 4?\nA: Refactoring the sessions functionality out of the legacy conference system using a service mesh, how service mesh helps with routing, observability, and security.\nWA1: Exposing the Attendee service to consumers using an API gateway\nWA2: Contract and component testing\nWA3: Feature flagging"}
{"prompt": "Context: and how to move the Attendee service to the cloud and consider replatforming.      The case study and the planned roadmap require us to visualize architectural change and record decisions. These are important artifacts that help to explain and plan changes in software projects. We believe that C4 diagrams and Architecture Decision Records (ADRs) represent a clear way of recording change.      Using C4 Diagrams   As part of introducing the case study, we revealed three types of C4 diagrams from the C4 model. We believe C4 is the best documentation standard for communicating architecture, context, and interactions to a diverse set of stakeholders. You may be wondering what about UML? The Unified Modeling Language (UML) provides an extensive dialect for communicating software architectures. A major challenge is that the majority of what UML provides is not committed to memory by architects and developers, and people quickly revert to boxes/circles/diamonds. It becomes a real challenge to understand the structure of diagrams before getting into the technical content of the discussion. Many diagrams are only committed to a project history if someone accidentally uses a permanent marker instead of dry wipe marker by mistake. The C4 model provides a simplified set of diagrams that act as a guide to your project architecture at various levels of detail.      C4 Context Diagram   Figure I-1 is represented using a C4 context diagram from the C4 model. The intention of this diagram is\n\nQUESTIONS:", "completion": "Q: What is the C4 model?\nA: The C4 model is a documentation standard for communicating architecture, context, and interactions to a diverse set of stakeholders.\nWA1: The C4 model is a programming language.\nWA2: The C4 model is a system for planning software projects.\nWA3: The C4 model is a system of communication between stakeholders.\n\nQ: What is the Unified Modeling Language (UML)?\nA: The Unified Modeling Language (UML) is a dialect for communicating software architectures.\nWA1: The Unified Modeling Language (UML) is a programming language.\nWA2: The Unified Modeling Language (UML) is a system for planning software projects.\nWA3: The Unified Modeling Language (UML) is a system of communication between stakeholders."}
{"prompt": "Context: to set context for both a technical and nontechnical audience. Many architecture conversations dive straight into the low-level details and miss setting the context of the high-level interactions. Consider the implications of getting a system context diagram wrong\u2014the benefit of summarizing the approach may save months of work to correct a misunderstanding.      C4 Container Diagram   While Figure I-1 provides the big picture of the conference system, a container diagram helps describe the technical breakout of the major participants in the architecture. A container in C4 is defined as \u201csomething that needs to be running in order for the overall system to work\u201d (for example, the conference database). Container diagrams are technical in nature and build on the higher-level system context diagram. Figure I-2, a container diagram, documents the detail of a customer interacting with the conference system.      Note   The conference application container in Figure I-2 is documented as simply software. Normally a C4 container would provide more detail into the type of container (e.g., Java Spring Application). However in this book we will be avoiding technology specifics, unless it helps to demonstrate a specific solution. The advantage of APIs and indeed modern applications is that there is a significant amount of flexibility in the solution space.      C4 Component Diagram   The C4 component diagram in Figure I-3 helps to define the roles and responsibilities within each\n\nQUESTIONS:", "completion": "Q: What is a C4 Container Diagram?\nA: A container diagram that helps describe the technical breakout of the major participants in the architecture.\nWA1: A diagram that documents the detail of a customer interacting with the conference system.\nWA2: A diagram that provides the big picture of the conference system.\nWA3: A diagram that defines the roles and responsibilities within each.\n\nQ: What is the advantage of APIs and modern applications?\nA: There is a significant amount of flexibility in the solution space.\nWA1: There is a significant amount of detail in the solution space.\nWA2: There is a significant amount of speed in the solution space.\nWA3: There is a significant amount of reliability in the solution space."}
{"prompt": "Context: container, along with the internal interactions. This diagram is useful if the detail of a container is queried, and it also provides a very useful map to the codebase. Think about the first time starting work on a new project: browsing a self-documenting codebase is one approach\u2014but it can be difficult to piece everything together. A component diagram reveals the detail of the language/stack you are using to build your software. In order to remain technology agnostic, we have used the term package/module.      Using Architecture Decision Records   As developers, architects, and indeed humans, we have all been in the position where we ask the question \u201cwhat were they thinking??\u201d If you have ever driven on the M62 between Leeds and Manchester in the United Kingdom, you may have been baffled by the construction of the motorway. As you climb the hill on the three-lane highway, it starts to deviate away from the traffic contraflow, until eventually Scott Hall Farm emerges surrounded by around 15 acres of farming land nestled between the carriages. Local legend of what happened described the owner of the land as stubborn and refusing to move or hand over his land, so the engineers simply built around him.2 Fifty years later a documentary surfaced revealing that the real reason for this was a geological fault beneath the land, meaning the motorway had to be built that way. When people guess why something was done in a particular way, expect rumor, humor, and criticism to emerge.\n\nQUESTIONS:", "completion": "Q: What is a component diagram useful for?\nA: A component diagram reveals the detail of the language/stack used to build software. \nWA1: It provides a very useful map to the codebase.\nWA2: It is used to query the detail of a container.\nWA3: It is used to remain technology agnostic.\n\nQ: What was the reason for the construction of the M62 between Leeds and Manchester in the UK?\nA: There was a geological fault beneath the land, meaning the motorway had to be built that way.\nWA1: The owner of the land was stubborn and refused to move or hand over his land.\nWA2: The engineers simply built around the land.\nWA3: It was to create a three-lane highway."}
{"prompt": "Context: In software architecture there will be many constraints that we have to build around, so it is important to ensure our decisions are recorded and transparent. ADRs help make decisions clear in software architecture.  One of the hardest things to track during the life of a project is the motivation behind certain decisions. A new person coming on to a project may be perplexed, baffled, delighted, or infuriated by some past decision.  Michael Nygard, creator of the ADR concept    There are four key sections in an ADR: status, context, decision, and consequences. An ADR is created in a proposed status and based on discussion will usually be either accepted or rejected. It is also possible that the decision may be superseded later by a new ADR. The context helps to set the scene and describe the problem or the bounds in which the decision will be made. Although creating a blog post ahead of the ADR and then linking from the ADR helps the community to follow your work, the context is not intended to be a blog post or detailed description. The decision clearly sets out what you plan to do and how you plan to do it. All decisions carry consequences or trade-offs in architecture, and these can sometimes be incredibly costly to get wrong.  When reviewing an ADR it is important to see if you agree with the decision in the ADR or if there is an alternative approach. An alternative approach that has not been considered may cause the ADR to be rejected. There is a lot of value in a\n\nQUESTIONS:", "completion": "Q: What is an ADR?\nA: An ADR is an architectural decision record which helps to make decisions clear in software architecture.\nWA1: An ADR is an automatic documentation resource.\nWA2: An ADR is a type of project management software.\nWA3: An ADR is a type of architecture design language.\n\nQ: What are the four key sections in an ADR?\nA: The four key sections in an ADR are status, context, decision, and consequences.\nWA1: The four key sections in an ADR are status, context, implementation, and consequences.\nWA2: The four key sections in an ADR are status, context, decision, and implementation.\nWA3: The four key sections in an ADR are objectives, context, decision, and consequences.\n\nQ: What is the purpose of the context section in an ADR?\nA: The context section in an ADR helps to set the scene and describe the problem or the bounds in which the decision will be made.\nWA1: The context section in an ADR provides a detailed description of the decision.\nWA2: The context section in an ADR is used to describe the implementation of the decision.\nWA3: The context section in an ADR is used to create a blog post about the decision."}
{"prompt": "Context: rejected ADR and most teams choose to keep ADRs immutable to capture the change in perspective. ADRs work best when they are presented in a location where key participants can view them, comment, and help move the ADR to accepted.      Tip   A question we often get asked is at what point should the team create an ADR? It is useful to ensure that there has been discussion ahead of the ADR and the record is a result of collective thinking in the team. Publishing an ADR to the wider community allows the opportunity for feedback beyond the immediate team.      Attendees Evolution ADR   In Figure I-4 we made the decision to take an evolutionary step in the conference system architecture. This is a major change and would warrant an ADR. Table I-1 is an example ADR that might have been proposed by the engineering team owning the conference system.  Table I-1. ADR001 separating attendees from the legacy conference system Status Proposed  Context  The conference owners have requested two new major features to the current conference system that need to be implemented without disrupting the current system. The conference system will need to be evolved to support a mobile application and an integration with an external CFP system. Both the mobile application and the external CFP system need to be able to access attendees to log in users to the third-party service.    Decision  We will take an evolutionary step as documented in Figure I-4 to split out the Attendee component into a\n\nQUESTIONS:", "completion": "Q: At what point should the team create an ADR?\nA: After there has been discussion ahead of the ADR and the record is a result of collective thinking in the team.\nWA1: Before any discussion has taken place.\nWA2: After the ADR has been published to the wider community.\nWA3: When the engineering team has proposed the ADR.\n\nQ: What is the purpose of publishing an ADR to the wider community?\nA: To allow the opportunity for feedback beyond the immediate team.\nWA1: To allow for discussion about the ADR.\nWA2: To keep the ADR immutable.\nWA3: To capture the change in perspective."}
{"prompt": "Context: standalone service. This will allow API-First development against the Attendee service and allow the API to be invoked from the legacy conference service. This will also support the ability to design for direct access to the Attendee service to provide user information to the external CFP system.    Consequences  The call to the Attendee service will not be out of process and may introduce a latency that will need to be tested. The Attendee service could become a single point of failure in the architecture and we may need to take steps to mitigate the potential impact of running a single Attendee service. With the planned multiple consumer model for the Attendee service, we will need to ensure good design, versioning, and testing to reduce accidental breaking changes.      Some of the consequences in the ADR are fairly major and definitely require further discussion. We are going to defer some of the consequences to later chapters.      Mastering API: ADR Guidelines   Within Mastering API Architecture we will be supplying ADR Guidelines to help collect important questions to ask when making decisions on the topic we are covering. Making decisions about an API-based architecture can be really tough, and in a lot of situations the answer is \u201cit depends.\u201d Rather than say it depends without context, the ADR Guidelines will help describe what it depends on and help inform your decisions. The ADR Guidelines can be used as a reference point to come back to or to read ahead to if\n\nQUESTIONS:", "completion": "Q: What is the purpose of the standalone service in the text?\nA: To allow API-First development against the Attendee service and to allow the API to be invoked from the legacy conference service.\nWA1: To provide user information to the external CFP system.\nWA2: To reduce accidental breaking changes.\nWA3: To provide ADR Guidelines.\n\nQ: What type of latency may be introduced by the standalone service?\nA: A latency that will need to be tested.\nWA1: No latency.\nWA2: A lower latency.\nWA3: A higher latency."}
{"prompt": "Context: you\u2019re facing a specific challenge. Table I-2 outlines the format for the ADR Guidelines and what you should expect from them.  Table I-2. ADR Guideline: Format Decision  Describes a decision that you might need to make when considering an aspect of this book.    Discussion Points  This section helps to identify the key discussions that you should be having when making a decision about your API architecture.  In this section we will reveal some of our experiences that may have influenced the decision. We will help you to identify the key information to inform your decision making process.    Recommendations  We will make specific recommendations that you should consider when creating your ADR, explaining the rationale behind why we are making a specific recommendation.      Summary   In this introduction we have provided a foundation with both the case study and the approach we will take to discussing API-driven architectures:  Architecture is an endless journey and APIs can play a major part in helping it evolve.    APIs are an abstraction of the implementation and can either be in process or out of process. Often architects find themselves in a position of evolving to out-of-process APIs, the onward focus of this book.    The conference case study is to describe and explain concepts. In this introduction you have seen a small evolutionary step to break out the Attendee service to address the upcoming business requirements.    You have seen the first three levels of C4\n\nQUESTIONS:", "completion": "Q: What is the format of the ADR Guidelines?\nA: Decision, Discussion Points, Recommendations, Summary\nWA1: Discussion Points, Decision, Summary, Recommendations\nWA2: Discussion Points, Summary, Recommendations, Decision\nWA3: Summary, Recommendations, Decision, Discussion Points\n\nQ: What is the focus of the book?\nA: Out-of-process APIs\nWA1: In-process APIs\nWA2: Process APIs\nWA3: Abstract APIs"}
{"prompt": "Context: diagrams and their importance in sharing and communicating architecture.    ADRs provide a valuable record for making decisions and have both present and historical value in the lifetime of a project.    You have seen the structure for ADR Guidelines that will be used throughout the book to help facilitate decision making.      With the decision made to break the Attendee service out from the conference system, we will now explore the options for designing and specifying the Attendee API.      1 The intention is it will be the UI accessing the ingress point. However, it is open for potential exploit.  2 Local stubborn traits fueled this likely explanation.      Part I. Designing, Building, and Testing APIs   This section provides the foundational building blocks for API-driven architectures.  In Chapter 1 you will learn about REST and Remote Procedure Call (RPC)\u2013based APIs. We will explore specifications and schemas, recommended standards, strategies for versioning, and how to choose the right API for your system.  In Chapter 2 you will learn about testing APIs and how different test styles are best applied to API-driven architectures.      Chapter 1. Design, Build, and Specify APIs   You will be presented with many options when designing and building APIs. It is incredibly fast to build a service with modern technologies and frameworks, but creating a durable approach requires careful thought and consideration. In this chapter we will explore REST and RPC to model the\n\nQUESTIONS:", "completion": "Q: What is the purpose of ADRs?\nA: ADRs provide a valuable record for making decisions and have both present and historical value in the lifetime of a project.\nWA1: To make sure decisions are made quickly\nWA2: To help with the design of APIs\nWA3: To provide a record of a project's lifetime\n\nQ: What will be explored in Chapter 1 of Part I?\nA: REST and Remote Procedure Call (RPC)based APIs, specifications and schemas, recommended standards, strategies for versioning, and how to choose the right API for your system.\nWA1: How to build an API\nWA2: How to test an API\nWA3: How to design a UI"}
{"prompt": "Context: producer and consumer relationships in the case study.  You will discover how standards can help to shortcut design decisions and navigate away from potential compatibility issues. You will look at OpenAPI Specifications, the practical uses for teams, and the importance of versioning.  RPC-based interactions are specified using a schema; to compare and contrast with a REST approach, we will explore gRPC. With both REST and gRPC in mind, we will look at the different factors to consider in how we model exchanges. We will look at the possibility of providing both a REST and RPC API in the same service and whether this is the right thing to do.      Case Study: Designing the Attendee API   In the Introduction we decided to migrate our legacy conference system and move toward a more API-driven architecture. As a first step to making this change, we are going to create a new Attendee service, which will expose a matching Attendee API. We also provided a narrow definition of an API. In order to design effectively, we need to consider more broadly the exchange between the producer and consumer, and more importantly who the producer and consumer are. The producer is owned by the attendee team. This team maintains two key relationships:  The attendee team owns the producer, and the legacy conference team owns the consumer. There is a close relationship between these two teams and any changes in structure are easily coordinated. A strong cohesion between the producer/consumer services\n\nQUESTIONS:", "completion": "Q: Who owns the producer?\nA: The attendee team\nWA1: The legacy conference team\nWA2: The API team\nWA3: The design team\n\nQ: What is the purpose of OpenAPI Specifications?\nA: To shortcut design decisions and navigate away from potential compatibility issues\nWA1: To provide a REST API in the same service\nWA2: To compare and contrast with a gRPC approach\nWA3: To look at different factors to consider in how to model exchanges"}
{"prompt": "Context: is possible to achieve.    The attendee team owns the producer, and the external CFP system team owns the consumer. There is a relationship between the teams, but any changes need to be coordinated to not break the integration. A loose coupling is required and breaking changes would need to be carefully managed.      We will compare and contrast the approaches to designing and building the Attendee API throughout this chapter.      Introduction to REST   REpresentation State Transfer (REST) is a set of architectural constraints, most commonly applied using HTTP as the underlying transport protocol. Roy Fielding\u2019s dissertation \u201cArchitectural Styles and the Design of Network-based Software Architectures\u201d provides a complete definition of REST. From a practical perspective, to be considered RESTful your API must ensure that:  A producer-to-consumer interaction is modeled where the producer models resources the consumer can interact with.    Requests from producer to consumer are stateless, meaning that the producer doesn\u2019t cache details of a previous request. In order to build up a chain of requests on a given resource, the consumer must send any required information to the producer for processing.    Requests are cachable, meaning the producer can provide hints to the consumer where this is appropriate. In HTTP this is often provided in information contained in the header.    A uniform interface is conveyed to the consumer. You will explore the use of verbs, resources, and\n\nQUESTIONS:", "completion": "Q: What is REST?\nA: REpresentation State Transfer (REST) is a set of architectural constraints, most commonly applied using HTTP as the underlying transport protocol.\nWA1: REST is a software development methodology. \nWA2: REST is a database language.\nWA3: REST is a security protocol.\n\nQ: How do producers and consumers interact in REST?\nA: A producer-to-consumer interaction is modeled where the producer models resources the consumer can interact with.\nWA1: Consumers and producers interact directly.\nWA2: Consumers and producers do not interact with each other.\nWA3: Consumers send requests to the producer."}
{"prompt": "Context: other patterns shortly.    It is a layered system, abstracting away the complexity of systems sitting behind the REST interface. For example, the consumer should not know or care if they\u2019re interacting with a database or other services.      Introduction to REST and HTTP by Example   Let\u2019s see an example of REST over HTTP. The following exchange is a GET request, where GET represents the method or verb. A verb such as GET describes the action to take on a particular resource; in this example, we consider the attendees resource. An Accept header is passed to define the type of content the consumer would like to retrieve. REST defines the notion of a representation in the body and allows for representation metadata to be defined in the headers.  In the examples in this chapter, we represent a request above the --- separator and a response below:   GET http://mastering-api.com/attendees Accept: application/json --- 200 OK Content-Type: application/json { \"displayName\": \"Jim\", \"id\": 1 }  The response includes the status code and message from the server, which enables the consumer to interrogate the result of the operation on the server-side resource. The status code of this request was a 200 OK, meaning the request was successfully processed by the producer. In the response body a JSON representation containing the conference attendees is returned. Many content types are valid for return from a REST, however it is important to consider if the content type is parsable by the\n\nQUESTIONS:", "completion": "Q: What is REST?\nA: REST is a layered system that abstracts away the complexity of systems sitting behind the REST interface.\nWA1: REST is a type of content.\nWA2: REST is a type of programming language.\nWA3: REST is a type of database.\n\nQ: What is an example of a GET request?\nA: A GET request is when a verb such as GET describes the action to take on a particular resource.\nWA1: A GET request is when a consumer sends a request to the server.\nWA2: A GET request is when a consumer sends a response to the server.\nWA3: A GET request is when a consumer sends a request to another consumer."}
{"prompt": "Context: consumer. For example, returning application/pdf is valid but would not represent an exchange that could easily be used by another system. We will explore approaches to modeling content types, primarily looking at JSON, later in this chapter.      Note   REST is relatively straightforward to implement because the client and server relationship is stateless, meaning no client state is persisted by the server. The client must pass the context back to the server in subsequent requests; for example, a request for http://mastering-api.com/attendees/1 would retrieve more information on a specific attendee.      The Richardson Maturity Model   Speaking at QCon in 2008, Leonard Richardson presented his experiences of reviewing many REST APIs. Richardson found levels of adoption that teams apply to building APIs from a REST perspective. Martin Fowler also covered Richardson\u2019s maturity heuristics on his blog. Table 1-1 explores the different levels represented by Richardson\u2019s maturity heuristics and their application to RESTful APIs.  Table 1-1. Richardson maturity heuristics Level 0 - HTTP/RPC  Establishes that the API is built using HTTP and has the notion of a single URI. Taking our preceding example of /attendees and not applying a verb to specify intent, we would open up an endpoint for exchange. Essentially this represents an RPC implementation over the REST protocol.    Level 1 - Resources  Establishes the use of resources and starts to bring in the idea of modeling resources in\n\nQUESTIONS:", "completion": "Q: What is an example of a valid exchange that would not be easily used by another system?\nA: Returning application/pdf \nWA1: Returning application/plaintext\nWA2: Returning application/html\nWA3: Returning application/json\n\nQ: What is the Richardson Maturity Model?\nA: A model presented by Leonard Richardson of different levels of adoption teams apply to building APIs from a REST perspective. \nWA1: A model presented by Martin Fowler of different levels of adoption teams apply to building APIs from a REST perspective. \nWA2: A model presented by Leonard Richardson of different levels of adoption teams apply to building APIs from a SOAP perspective. \nWA3: A model presented by Martin Fowler of different levels of adoption teams apply to building APIs from a SOAP perspective."}
{"prompt": "Context: the context of the URI. In our example, if we added GET /attendees/1 returning a specific attendee, it would start to look like a level 1 API. Martin Fowler draws an analogy to the classic object-oriented world of introducing identity.    Level 2 - Verbs (Methods)  Starts to introduce the correct modeling of multiple resource URIs accessed by different request methods (also known as HTTP verbs) based on the effect of the resources on the server. An API at level 2 can make guarantees around GET methods not impacting server state and presenting multiple operations on the same resource URI. In our example adding DELETE /attendees/1, PUT /attendees/1 would start to add the notion of a level 2\u2013compliant API.    Level 3 - Hypermedia Controls  This is the epitome of REST design and involves navigable APIs by the use of HATEOAS (Hypertext As The Engine Of Application State). In our example, when we call GET /attendees/1, the response would contain the actions that are possible on the object returned from the server. This would include the option to be able to update the attendee or delete the attendee and what the client is required to invoke in order to do so. In practical terms level 3 is rarely used in modern RESTful HTTP services, and although the navigation is a benefit in flexible UI style systems, it doesn\u2019t suit interservice API calls. Using HATEOAS would be a chatty experience and is often short-circuited by having a complete specification of possible interactions up front\n\nQUESTIONS:", "completion": "Q: What is the difference between a level 1 and level 2 API?\nA: Level 1 introduces identity, whereas level 2 introduces the notion of multiple resource URIs accessed by different request methods.\nWA1: Level 1 introduces the notion of multiple resource URIs accessed by different request methods, whereas level 2 introduces identity.\nWA2: Level 1 introduces the use of Hypertext As The Engine Of Application State, whereas level 2 introduces identity.\nWA3: Level 1 introduces the use of Hypertext As The Engine Of Application State, whereas level 2 introduces the notion of multiple resource URIs accessed by different request methods.\n\nQ: What is the epitome of REST design?\nA: Level 3 - Hypermedia Controls.\nWA1: Level 1 - Identity.\nWA2: Level 2 - Verbs (Methods).\nWA3: Level 4 - Hypertext As The Engine Of Application State."}
{"prompt": "Context: while programming against the producer.      When designing API exchanges, the different levels of Richardson Maturity are important to consider. Moving toward level 2 will enable you to project an understandable resource model to the consumer, with appropriate actions available against the model. In turn, this reduces coupling and hides the full detail of the backing service. Later we will also see how this abstraction is applied to versioning.  If the consumer is the CFP team, modeling an exchange with low coupling and projecting a RESTful model would be a good starting point. If the consumer is the legacy conference team, we may still choose to use a RESTful API, but there is also another option with RPC. In order to start to consider this type of traditionally east\u2013west modeling, we will explore RPC.      Introduction to Remote Procedure Call (RPC) APIs   A Remote Procedure Call (RPC) involves calling a method in one process but having it execute code in another process. While REST can project a model of the domain and provides an abstraction from the underlying technology to the consumer, RPC involves exposing a method from one process and allows it to be called directly from another.  gRPC is a modern open source high-performance RPC. gRPC is under stewardship of the Linux Foundation and is the de facto standard for RPC across most platforms. Figure 1-1 describes an RPC call in gRPC, which involves the legacy conference service invoking the remote method on the Attendee\n\nQUESTIONS:", "completion": "Q: What is the purpose of moving toward level 2 when designing API exchanges?\nA: To project an understandable resource model to the consumer, with appropriate actions available against the model.\nWA1: To reduce coupling and increase the full detail of the backing service.\nWA2: To provide the CFP team with a RESTful model.\nWA3: To apply abstraction to versioning.\n\nQ: What is the purpose of Remote Procedure Call (RPC) APIs?\nA: To call a method in one process but have it execute code in another process.\nWA1: To provide an abstraction from the underlying technology to the consumer.\nWA2: To project a model of the domain.\nWA3: To use gRPC as a high-performance RPC."}
{"prompt": "Context: service. The gRPC Attendee service starts and exposes a gRPC server on a specified port, allowing methods to be invoked remotely. On the client side (the legacy conference service), a stub is used to abstract the complexity of making the remote call into the library. gRPC requires a schema to fully cover the interaction between producer and consumer.      Figure 1-1. Example C4 component diagram using gRPC      A key difference between REST and RPC is state. REST is by definition stateless\u2014with RPC state depends on the implementation. RPC-based integrations in certain situations can also build up state as part of the exchange. This buildup of state has the convenience of high performance at the potential cost of reliability and routing complexities. With RPC the model tends to convey the exact functionality at a method level that is required from a secondary service. This optionality in state can lead to an exchange that is potentially more coupled between producer and consumer. Coupling is not always a bad thing, especially in east\u2013west services where performance is a key consideration.      A Brief Mention of GraphQL   Before we explore REST and RPC styles in detail, we would be remiss not to mention GraphQL and where it fits into the API world. RPC offers access to a series of individual functions provided by a producer but does not usually extend a model or abstraction to the consumer. REST, on the other hand, extends a resource model for a single API provided by the\n\nQUESTIONS:", "completion": "Q: What is the key difference between REST and RPC?\nA: State - REST is by definition stateless, while state in RPC depends on the implementation.\nWA1: GraphQL is the key difference between REST and RPC.\nWA2: Complexity is the key difference between REST and RPC.\nWA3: Authentication is the key difference between REST and RPC.\n\nQ: What does RPC require?\nA: A schema to fully cover the interaction between producer and consumer.\nWA1: A library to abstract the complexity of making the remote call.\nWA2: A client-side stub to make the remote call.\nWA3: A server-side stub to make the remote call."}
{"prompt": "Context: producer. It is possible to offer multiple APIs on the same base URL using API gateways. We will explore this notion further in Chapter 3. If we offer multiple APIs in this way, the consumer will need to query sequentially to build up state on the client side. The consumer also needs to understand the structure of all services involved in the query. This approach is wasteful if the consumer is only interested in a subset of fields on the response. Mobile devices are constrained by smaller screens and network availability, so GraphQL is excellent in this scenario.  GraphQL introduces a technology layer over existing services, datastores, and APIs that provides a query language to query across multiple sources. The query language allows the client to ask for exactly the fields required, including fields that span across multiple APIs. GraphQL uses the GraphQL schema language, to specify the types in individual APIs and how APIs combine. One major advantage of introducing a GraphQL schema in your system is the ability to provide a single version across all APIs, removing the need for potentially complex version management on the consumer side.  GraphQL excels when a consumer requires uniform API access over a wide range of interconnected services. The schema provides the connection and extends the domain model, allowing the customer to specify exactly what is required on the consumer side. This works extremely well for modeling a user interface and also reporting systems or data\n\nQUESTIONS:", "completion": "Q: What is the advantage of introducing a GraphQL schema in your system? \nA: The ability to provide a single version across all APIs, removing the need for potentially complex version management on the consumer side.\nWA1: To make the APIs more efficient.\nWA2: To make the APIs easier to use.\nWA3: To make the APIs more secure.\n\nQ: How can GraphQL help mobile devices?\nA: GraphQL is excellent in this scenario as it allows the client to ask for exactly the fields required, including fields that span across multiple APIs.\nWA1: By enabling faster response times.\nWA2: By providing more secure connections.\nWA3: By offering unrestricted access."}
{"prompt": "Context: warehousing\u2013style systems. In systems where vast amounts of data are stored across different subsystems, GraphQL can provide an ideal solution to abstracting away internal system complexity.  It is possible to place GraphQL over existing legacy systems and use this as a facade to hide away the complexity, though providing GraphQL over a layer of well-designed APIs often means the facade is simpler to implement and maintain. GraphQL can be thought of as a complementary technology and should be considered when designing and building APIs. GraphQL can also be thought of as a complete approach to building up an entire API ecosystem.  GraphQL shines in certain scenarios and we would encourage you to take a look at Learning GraphQL (O\u2019Reilly) and GraphQL in Action (O\u2019Reilly) for a deeper dive into this topic.      REST API Standards and Structure   REST has some very basic rules, but for the most part the implementation and design is left as an exercise for the developer. For example, what is the best way to convey errors? How should pagination be implemented? How do you accidentally avoid building an API where compatibility frequently breaks? At this point, it is useful to have a more practical definition around APIs to provide uniformity and expectations across different implementations. This is where standards or guidelines can help, however there are a variety of sources to choose from.  For the purposes of discussing design, we will use the Microsoft REST API Guidelines, which\n\nQUESTIONS:", "completion": "Q: What is GraphQL used for?\nA: GraphQL is used to abstract complexity away in systems that store vast amounts of data across different subsystems.\nWA1: To design and build APIs\nWA2: To provide a uniform implementation of REST APIs\nWA3: To provide a layer of well-designed APIs\n\nQ: What does Microsoft REST API Guidelines provide?\nA: It provides uniformity and expectations across different implementations.\nWA1: It provides a layer of well-designed APIs\nWA2: It provides an ideal solution to abstracting away internal system complexity\nWA3: It provides a complete approach to building up an entire API ecosystem"}
{"prompt": "Context: represent a series of internal guidelines that have been open sourced. The guidelines use RFC-2119, which defines terminology for standards such as MUST, SHOULD, SHOULD NOT, MUST NOT, etc., allowing the developer to determine whether requirements are optional or mandatory.      Tip   As REST API standards are evolving, an open list of API standards are available on the book\u2019s Github page. Please contribute via pull request any open standards you think would be useful for other readers to consider.    Let\u2019s consider the design of the Attendee API using the Microsoft REST API Guidelines and introduce an endpoint to create a new attendee. If you are familiar with REST, the thought will immediately be to use POST:   POST http://mastering-api.com/attendees { \"displayName\": \"Jim\", \"givenName\": \"James\", \"surname\": \"Gough\", \"email\": \"jim@mastering-api.com\" } --- 201 CREATED Location: http://mastering-api.com/attendees/1  The Location header reveals the location of the new resource created on the server, and in this API we are modeling a unique ID for the user. It is possible to use the email field as a unique ID, however the Microsoft REST API Guidelines recommend in section 7.9 that personally identifiable information (PII) should not be part of the URL.      Warning   The reason for removing sensitive data from the URL is that paths or query parameters might be inadvertently cached in the network\u2014for example, in server logs or elsewhere.    Another aspect of APIs that can be\n\nQUESTIONS:", "completion": "Q: What is the purpose of the Microsoft REST API Guidelines?\nA: The guidelines provide developers with terminology to determine whether requirements are optional or mandatory.\nWA1: To provide developers with guidelines to create APIs.\nWA2: To standardize the design of APIs.\nWA3: To provide developers with a source of open standards.\n\nQ: What is the recommended approach to creating an endpoint to create a new attendee?\nA: POST\nWA1: PUT\nWA2: GET\nWA3: DELETE"}
{"prompt": "Context: difficult is naming. As we will discuss in \u201cAPI Versioning\u201d, something as simple as changing a name can break compatibility. There is a short list of standard names that should be used in the Microsoft REST API Guidelines, however teams should expand this to have a common domain data dictionary to supplement the standards. In many organizations it is incredibly helpful to proactively investigate the requirements around data design and in some cases governance. Organizations that provide consistency across all APIs offered by a company present a uniformity that enables consumers to understand and connect responses. In some domains there may already be widely known terminology\u2014use them!      Collections and Pagination   It seems reasonable to model the GET /attendees request as a response containing a raw array. The following source snippet shows an example of what that might look like as a response body:   GET http://mastering-api.com/attendees --- 200 OK [ { \"displayName\": \"Jim\", \"givenName\": \"James\", \"surname\": \"Gough\", \"email\": \"jim@mastering-api.com\", \"id\": 1, }, ... ]  Let\u2019s consider an alternative model to the GET /attendees request that nests the array of attendees inside an object. It may seem strange that an array response is returned in an object, however the reason for this is that it allows for us to model bigger collections and pagination. Pagination involves returning a partial result, while providing instructions for how the consumer can request the next set of\n\nQUESTIONS:", "completion": "Q: What is the recommended approach for naming in the Microsoft REST API Guidelines?\nA: Use the standard names provided, and supplement them with a common domain data dictionary.\nWA1: Come up with new names each time\nWA2: Use a completely different set of names than the standards\nWA3: Use the same names for all APIs\n\nQ: What is the purpose of providing consistency across all APIs offered by a company?\nA: It enables consumers to understand and connect responses.\nWA1: It allows for easier maintenance of the APIs\nWA2: It reduces the amount of code needed\nWA3: It allows for faster implementation of changes"}
{"prompt": "Context: results. This is reaping the benefits of hindsight; adding pagination later and converting from an array to an object in order to add a @nextLink (as recommended by the standards) would break compatibility:   GET http://mastering-api.com/attendees --- 200 OK { \"value\": [ { \"displayName\": \"Jim\", \"givenName\": \"James\", \"surname\": \"Gough\", \"email\": \"jim@mastering-api.com\", \"id\": 1, } ], \"@nextLink\": \"{opaqueUrl}\" }      Filtering Collections   Our conference is looking a little lonely with only one attendee, however when collections grow in size we may need to add filtering in addition to pagination. The filtering standard provides an expression language within REST to standardize how filter queries should behave, based upon the OData Standard. For example, we could find all attendees with the displayName Jim using:   GET http://mastering-api.com/attendees?$filter=displayName eq 'Jim'  It is not necessary to complete all filtering and searching features from the start. However, designing an API in line with the standards will allow the developer to support an evolving API architecture without breaking compatibility for consumers. Filtering and querying is a feature that GraphQL is really good at, especially if querying and filtering across many of your services becomes relevant.      Error Handling   An important consideration when extending APIs to consumers is defining what should happen in various error scenarios. Error standards are useful to define upfront and share with\n\nQUESTIONS:", "completion": "Q: What is an example of a filter query based upon the OData Standard?\nA: GET http://mastering-api.com/attendees?$filter=displayName eq 'Jim'\nWA1: GET http://mastering-api.com/attendees?$filter=email eq 'Jim'\nWA2: GET http://mastering-api.com/attendees?$filter=givenName eq 'Jim'\nWA3: GET http://mastering-api.com/attendees?$filter=surname eq 'Jim'\n\nQ: What is an important consideration when extending APIs to consumers?\nA: Defining what should happen in various error scenarios\nWA1: Adding pagination later\nWA2: Converting from an array to an object\nWA3: Using the GraphQL standard"}
{"prompt": "Context: producers to provide consistency. It is important that errors describe to the consumer exactly what has gone wrong with the request, as this will avoid increasing the support required for the API.  The guidelines state \u201cFor non-success conditions, developers SHOULD be able to write one piece of code that handles errors consistently.\u201d An accurate status code must be provided to the consumer, because often consumers will build logic around the status code provided in the response. We have seen many APIs that return errors in the body along with a 2xx type of response, which is used to indicate success. 3xx status codes for redirects are actively followed by some consuming library implementations, enabling providers to relocate and access external sources. 4xx usually indicates a client-side error; at this point the content of the message field is extremely useful to the developer or end user. 5xx usually indicates a failure on the server side and some client libraries will retry on these types of failures. It is important to consider and document what happens in the service based on an unexpected failure\u2014for example, in a payment system does a 500 mean the payment has gone through or not?      Warning   Ensure that the error messages sent back to an external consumer do not contain stack traces and other sensitive information. This information can help a hacker aiming to compromise the system. The error structure in the Microsoft guidelines has the concept of an InnerError,\n\nQUESTIONS:", "completion": "Q: How should developers structure their code to handle errors consistently?\nA: Developers SHOULD be able to write one piece of code that handles errors consistently.\nWA1: Developers MUST be able to write one piece of code that handles errors consistently.\nWA2: Developers MAY be able to write one piece of code that handles errors consistently.\nWA3: Developers DO NOT NEED to write one piece of code that handles errors consistently.\n\nQ: What should be sent back to an external consumer in the case of an error?\nA: An accurate status code must be provided to the consumer.\nWA1: Stack traces and other sensitive information must be sent back to an external consumer.\nWA2: All errors must be sent back to an external consumer.\nWA3: No information must be sent back to an external consumer."}
{"prompt": "Context: which could be useful in which to place more detailed stack traces/descriptions of issues. This would be incredibly helpful for debugging but must be stripped prior to an external consumer.    We have just scratched the surface on building REST APIs, but clearly there are many important decisions to be made when beginning to build an API. If we combine the desire to present intuitive APIs that are consistent and allow for an evolving and compatible API, it is worth adopting an API standard early.      ADR Guideline: Choosing an API Standard   To make your decision on API standards, the guideline in Table 1-2 lists important topics to consider. There are a range of guidelines to choose from, including the Microsoft guidelines discussed in this section, and finding one that best matches the styles of APIs being produced is a key decision.  Table 1-2. API Standards Guideline Decision  Which API standard should we adopt?    Discussion Points  Does the organization already have other standards within the company? Can we extend those standards to external consumers?  Are we using any third-party APIs that we will need to expose to a consumer (e.g., Identity Services) that already have a standard?  What does the impact of not having a standard look like for our consumers?    Recommendations  Pick an API standard that best matches the culture of the organization and formats of APIs you may already have in the inventory.  Be prepared to evolve and add to a standard any\n\nQUESTIONS:", "completion": "Q: What is the ADR Guideline?\nA: Choosing an API Standard\nWA1: Developing REST APIs\nWA2: Debugging\nWA3: Intuitive APIs\n\nQ: What should be taken into consideration when making a decision on API standards?\nA: Discussion Points and Recommendations\nWA1: Impact of not having a standard\nWA2: Third-party APIs\nWA3: Company culture"}
{"prompt": "Context: domain/industry-specific amendments.  Start with something early to avoid having to break compatibility later for consistency.  Be critical of existing APIs. Are they in a format that consumers would understand or is more effort required to offer the content?      Specifying REST APIs Using OpenAPI   As we\u2019re beginning to see, the design of an API is fundamental to the success of an API platform. The next consideration we\u2019ll discuss is sharing the API with developers consuming our APIs.  API marketplaces provide a public or private listing of APIs available to a consumer. A developer can browse documentation and quickly try out an API in the browser to explore the API behavior and functionality. Public and private API marketplaces have placed REST APIs prominently into the consumer space. The success of REST APIs has been driven by both the technical landscape and the low barrier to entry for both the client and server.  As the number of APIs grew, it quickly became necessary to have a mechanism to share the shape and structure of APIs with consumers. This is why the OpenAPI Initiative was formed by API industry leaders to construct the OpenAPI Specification (OAS). Swagger was the original reference implementation of the OpenAPI Specifications, but most tooling has now converged on using OpenAPI.  The OpenAPI Specifications are JSON- or YAML-based representations of the API that describe the structure, the domain objects exchanged, and any security requirements of the API. In\n\nQUESTIONS:", "completion": "Q: What is the OpenAPI Initiative?\nA: The OpenAPI Initiative is a organization formed by API industry leaders to construct the OpenAPI Specification (OAS).\nWA1: The OpenAPI Initiative is an open source toolkit for building and testing APIs.\nWA2: The OpenAPI Initiative is a tool for creating and sharing documentation of APIs. \nWA3: The OpenAPI Initiative is a platform for hosting and sharing APIs.\n\nQ: What is the OpenAPI Specification?\nA: The OpenAPI Specification is a JSON- or YAML-based representation of the API that describes the structure, the domain objects exchanged, and any security requirements of the API.\nWA1: The OpenAPI Specification is a tool for creating and sharing documentation of APIs.\nWA2: The OpenAPI Specification is a platform for hosting and sharing APIs.\nWA3: The OpenAPI Specification is an open source toolkit for building and testing APIs."}
{"prompt": "Context: addition to the structure, they also convey metadata about the API, including any legal or licensing requirements, and also carry documentation and examples that are useful to developers consuming the API. OpenAPI Specifications are an important concept surrounding modern REST APIs, and many tools and products have been built around its usage.      Practical Application of OpenAPI Specifications   Once an OAS is shared, the power of the specification starts to become apparent. OpenAPI.Tools documents a full range of available open and closed source tools. In this section we will explore some of the practical applications of tools based on their interaction with the OpenAPI Specification.  In the situation where the CFP team is the consumer, sharing the OAS enables the team to understand the structure of the API. Using some of the following practical applications can help both improve the developer experience and ensure the health of the exchange.      Code Generation   Perhaps one of the most useful features of an OAS is allowing the generation of client-side code to consume the API. As discussed earlier, we can include the full details of the server, security, and of course the API structure itself. With all this information we can generate a series of model and service objects that represent and invoke the API. The OpenAPI Generator project supports a wide range of languages and toolchains. For example, in Java you can choose to use Spring or JAX-RS and in TypeScript you\n\nQUESTIONS:", "completion": "Q: What is the purpose of OpenAPI Specifications?\nA: OpenAPI Specifications are used to convey metadata about an API, including documentation, legal and licensing requirements, and examples.\nWA1: To generate client-side code to consume the API.\nWA2: To document a full range of available open and closed source tools.\nWA3: To improve the developer experience and ensure the health of the exchange.\n\nQ: What is the OpenAPI Generator project?\nA: The OpenAPI Generator project supports a wide range of languages and toolchains to generate client-side code to consume the API.\nWA1: A closed source tool used to generate client-side code.\nWA2: A tool used to document a full range of available open and closed source tools.\nWA3: A tool used to improve the developer experience and ensure the health of the exchange."}
{"prompt": "Context: can choose a combination of TypeScript with your favorite framework. It is also possible to generate the API implementation stubs from the OAS.  This raises an important question about what should come first\u2014the specification or the server-side code? In Chapter 2, we discuss \u201ccontract tracing,\u201d which presents a behavior-driven approach to testing and building APIs. The challenge with OpenAPI Specifications is that alone they only convey the shape of the API. OpenAPI Specifications do not fully model the semantics (or expected behavior) of the API under different conditions. If you are going to present an API to external users, it is important that the range of behaviors is modeled and tested to help avoid having to drastically change the API later.  APIs should be designed from the perspective of the consumer and consider the requirement to abstract the underlying representation to reduce coupling. It is important to be able to freely refactor components behind the scenes without breaking API compatibility, otherwise the API abstraction loses value.      OpenAPI Validation   OpenAPI Specifications are useful for validating the content of an exchange to ensure the request and response match the expectations of the specification. At first it might not seem apparent where this would be useful\u2014if code is generated, surely the exchange will always be right? One practical application of OpenAPI validation is in securing APIs and API infrastructure. In many organizations a zonal\n\nQUESTIONS:", "completion": "Q: What should come first when developing an API- the specification or the server-side code?\nA: The specification\nWA1: The server-side code\nWA2: The consumer \nWA3: The underlying representation\n\nQ: What is the challenge with OpenAPI Specifications?\nA: They only convey the shape of the API.\nWA1: They fully model the semantics.\nWA2: They are difficult to use.\nWA3: They are not secure.\n\nQ: What is one practical application of OpenAPI validation?\nA: Securing APIs and API infrastructure.\nWA1: Generating the API implementation stubs from the OAS.\nWA2: Modeling the range of behaviors.\nWA3: Refactoring components behind the scenes."}
{"prompt": "Context: architecture is common, with a notion of a demilitarized zone (DMZ) used to shield a network from inbound traffic. A useful feature is to interrogate messages in the DMZ and terminate the traffic if the specification does not match. We will cover security in more detail in Chapter 6.  Atlassian, for example, open sourced a tool called the swagger-request-validator, which is capable of validating JSON REST content. The project also has adapters that integrate with various mocking and testing frameworks to help ensure that API Specifications are conformed to as part of testing. The tool has an OpenApiInteractionValidator, which is used to create a ValidationReport on an exchange. The following code demonstrates building a validator from the specification, including any basePathOverrides\u2014which may be necessary if deploying an API behind infrastructure that alters the path. The validation report is generated from analyzing the request and response at the point where validation is executed:   //Using the location of the specification create an interaction validator //The base path override is useful if the validator will be used //behind a gateway/proxy final OpenApiInteractionValidator validator = OpenApiInteractionValidator .createForSpecificationUrl(specUrl) .withBasePathOverride(basePathOverride) .build; //Requests and Response objects can be converted or created using a builder final ValidationReport report = validator.validate(request, response); if (report.hasErrors()) { //\n\nQUESTIONS:", "completion": "Q: What is a DMZ used for?\nA: To shield a network from inbound traffic.\nWA1: To secure a network from outbound traffic.\nWA2: To protect a network from malicious software.\nWA3: To provide remote access to a network.\n\nQ: What is the purpose of the swagger-request-validator?\nA: To validate JSON REST content.\nWA1: To secure a network from outbound traffic.\nWA2: To encrypt data for secure transfer.\nWA3: To detect and eliminate malware.\n\nQ: What is the OpenApiInteractionValidator used for?\nA: To create a ValidationReport on an exchange.\nWA1: To secure a network from outbound traffic.\nWA2: To encrypt data for secure transfer.\nWA3: To detect and eliminate malware."}
{"prompt": "Context: Capture or process error information }      Examples and Mocking   The OAS can provide example responses for the paths in the specification. Examples, as we\u2019ve discussed, are useful for documentation to help developers understand the expected API behavior. Some products have started to use examples to allow the user to query the API and return example responses from a mock service. This can be really useful in features such as a developer portal, which allows developers to explore documentation and invoke APIs. Another useful feature of mocks and examples is the ability to share ideas between the producer and consumer ahead of committing to build the service. Being able to \u201ctry out\u201d the API is often more valuable than trying to review if a specification would meet your requirements.  Examples can potentially introduce an interesting problem, which is that this part of the specification is essentially a string (in order to model XML/JSON, etc.). openapi-examples-validator validates that an example matches the OAS for the corresponding request/response component of the API.      Detecting Changes   OpenAPI Specifications can also be helpful in detecting changes in an API. This can be incredibly useful as part of a DevOps pipeline. Detecting changes for backward compatibility is very important, but first we need to understand versioning of APIs in more detail.      API Versioning   We have explored the advantages of sharing an OAS with a consumer, including the speed of\n\nQUESTIONS:", "completion": "Q: What is the purpose of using examples in an OAS?\nA: Examples are useful for documentation to help developers understand the expected API behavior. \nWA1: To detect changes in an API.\nWA2: To share ideas between the producer and consumer ahead of committing to build the service.\nWA3: To validate that an example matches the OAS for the corresponding request/response component.\n\nQ: What is the purpose of a mock service?\nA: A mock service allows the user to query the API and return example responses. \nWA1: To detect changes in an API.\nWA2: To validate that an example matches the OAS for the corresponding request/response component.\nWA3: To share ideas between the producer and consumer ahead of committing to build the service."}
{"prompt": "Context: integration. Consider the case where multiple consumers start to operate against the API. What happens when there is a change to the API or one of the consumers requests the addition of new features to the API?  Let\u2019s take a step back and think about if this was a code library built into our application at compile time. Any changes to the library would be packaged as a new version and until the code is recompiled and tested against the new version, there would be no impact to production applications. As APIs are running services, we have a few upgrade options that are immediately available to us when changes are requested:  Release a new version and deploy in a new location.  Older applications continue to operate against the older version of the APIs. This is fine from a consumer perspective, as the consumer only upgrades to the new location and API if they need the new features. However, the owner of the API needs to maintain and manage multiple versions of the API, including any patching and bug fixing that might be necessary.    Release a new version of the API that is backward compatible with the previous version of the API.  This allows additive changes without impacting existing users of the API. There are no changes required by the consumer, but we may need to consider downtime or availability of both old and new versions during the upgrade. If there is a small bug fix that changes something as small as an incorrect field name, this would break compatibility.    Break\n\nQUESTIONS:", "completion": "Q: What are the upgrade options when changes are requested to an API?\nA: Release a new version and deploy in a new location, release a new version of the API that is backward compatible with the previous version of the API, or break compatibility.\nWA1: Ignore the changes\nWA2: Recompile and test the code\nWA3: Do nothing\n\nQ: What might happen when multiple consumers start to operate against the API?\nA: Changes to the API or new features requested by one of the consumers might require an upgrade.\nWA1: The API will become unstable\nWA2: The application will crash\nWA3: The API will be shut down"}
{"prompt": "Context: compatibility with the previous API and all consumers must upgrade code to use the new API.  This seems like an awful idea at first, as that would result in things breaking unexpectedly in production.1 However, a situation may present itself where we cannot avoid breaking compatibility with older versions. This type of change can trigger a whole-system lockstep change that requires coordination of downtime.      The challenge is that all of these different upgrade options offer advantages but also drawbacks either to the consumer or the producer. The reality is that we want to be able to support a combination of all three options. In order to do this we need to introduce rules around versioning and how versions are exposed to the consumer.      Semantic Versioning   Semantic versioning offers an approach that we can apply to REST APIs to give us a combination of the preceding upgrade options. Semantic versioning defines a numerical representation attributed to an API release. That number is based on the change in behavior in comparison to the previous version, using the following rules:  A major version introduces noncompatible changes with previous versions of the API. In an API platform, upgrading to a new major version is an active decision by the consumer. There is likely going to be a migration guide and tracking as consumers upgrade to the new API.    A minor version introduces a backward compatible change with the previous version of the API. In an API service\n\nQUESTIONS:", "completion": "Q: What is the goal of introducing rules around versioning and how versions are exposed to the consumer?\nA: To support a combination of all three upgrade options.\nWA1: To introduce noncompatible changes with previous versions of the API.\nWA2: To introduce a backward compatible change with the previous version of the API.\nWA3: To avoid breaking compatibility with older versions.\n\nQ: What is Semantic Versioning?\nA: An approach that we can apply to REST APIs to give us a combination of the preceding upgrade options.\nWA1: A numerical representation attributed to an API release.\nWA2: A way to upgrade to a new major version.\nWA3: A migration guide and tracking as consumers upgrade to the new API."}
{"prompt": "Context: platform, it is acceptable for consumers to receive minor versions without making an active change on the client side.    A patch version does not change or introduce new functionality but is used for bug fixes on an existing Major.Minor version of functionality.      Formatting for semantic versioning can be represented as Major.Minor.Patch. For example, 1.5.1 would represent major version 1, minor version 5, with patch upgrade of 1. In Chapter 5 you will explore how semantic versioning connects with the concept of API lifecycle and releases.      OpenAPI Specification and Versioning   Now that we have explored versioning we can look at examples of breaking changes and nonbreaking changes using the Attendee API specification. There are several tools to choose from to compare specifications, and in this example we will use openapi-diff from OpenAPITools.  We will start with a breaking change: we will change the name of the givenName field to firstName. This is a breaking change because consumers will be expecting to parse givenName, not firstName. We can run the diff tool from a docker container using the following command:   $docker run --rm -t \\ -v $(pwd):/specs:ro \\ openapitools/openapi-diff:latest /specs/original.json /specs/first-name.json ========================================================================== ... - GET /attendees Return Type: - Changed 200 OK Media types: - Changed */* Schema: Broken compatibility Missing property: [n].givenName (string)\n\nQUESTIONS:", "completion": "Q: What is the format used for semantic versioning?\nA: Major.Minor.Patch\nWA1: Major.Minor\nWA2: Minor.Patch\nWA3: Major.Minor.Minor\n\nQ: What is the command used to compare OpenAPI specifications?\nA: $docker run --rm -t \\ -v $(pwd):/specs:ro \\ openapitools/openapi-diff:latest /specs/original.json /specs/first-name.json\nWA1: $docker run --rm -t \\ -v $(pwd):/specs:ro \\ openapitools/openapi-diff\nWA2: $docker run --rm -v $(pwd):/specs:ro \\ openapitools/openapi-diff:latest /specs/original.json\nWA3: $docker run -t \\ -v $(pwd):/specs:ro \\ openapitools/openapi-diff:latest /specs/original.json /specs/first-name.json"}
{"prompt": "Context: -------------------------------------------------------------------------- -- Result -- -------------------------------------------------------------------------- API changes broke backward compatibility --------------------------------------------------------------------------  We can try to add a new attribute to the /attendees return type to add an additional field called age. Adding new fields does not break existing behavior and therefore does not break compatibility:   $ docker run --rm -t \\ -v $(pwd):/specs:ro \\ openapitools/openapi-diff:latest --info /specs/original.json /specs/age.json ========================================================================== ... - GET /attendees Return Type: - Changed 200 OK Media types: - Changed */* Schema: Backward compatible -------------------------------------------------------------------------- -- Result -- -------------------------------------------------------------------------- API changes are backward compatible --------------------------------------------------------------------------  It is worth trying this out to see which changes would be compatible and which would not. Introducing this type of tooling as part of the API pipeline is going to help avoid unexpected noncompatible changes for consumers. OpenAPI Specifications are an important part of an API program, and when combined with tooling, versioning, and lifecycle, they are invaluable.      Warning   Tools are often OpenAPI version\u2013specific, so it is important\n\nQUESTIONS:", "completion": "Q: What can be done to add an additional field to the /attendees return type without breaking compatibility?\nA: Adding new fields does not break existing behavior and therefore does not break compatibility.\nWA1: Deleting existing fields.\nWA2: Removing existing behavior.\nWA3: Making changes to the OpenAPI Specifications.\n\nQ: What is an important part of an API program?\nA: OpenAPI Specifications.\nWA1: Versioning.\nWA2: Testing Tools.\nWA3: Documentation."}
{"prompt": "Context: to check whether the tool supports the specification you are working with. In the preceding example we tried the diff tool with an earlier version of a spec and no breaking changes were detected.      Implementing RPC with gRPC   East\u2013west services such as Attendee tend to be higher traffic and can be implemented as microservices used across the architecture. gRPC may be a more suitable tool than REST for east\u2013west services, owing to the smaller data transmission and speed within the ecosystem. Any performance decisions should always be measured in order to be informed.  Let\u2019s explore using a Spring Boot Starter to rapidly create a gRPC server. The following .proto file models the same attendee object that we explored in our OpenAPI Specification example. As with OpenAPI Specifications, generating code from a schema is quick and supported in multiple languages.  The attendees .proto file defines an empty request and returns a repeated Attendee response. In protocols used for binary representations, it is important to note that the position and order of fields is critical, as they govern the layout of the message. Adding a new service or new method is backward compatible as is adding a field to a message, but care is required. Any new fields that are added must not be mandatory fields, otherwise backward compatibility would break.  Removing a field or renaming a field will break compatibility, as will changing the data type of a field. Changing the field number is also an\n\nQUESTIONS:", "completion": "Q: What should be taken into consideration when making performance decisions?\nA: Measuring performance\nWA1: Guessing performance\nWA2: Making assumptions about performance\nWA3: Consulting colleagues about performance \n\nQ: What should be done when adding a new field to a message in binary representations?\nA: The field should not be mandatory\nWA1: The field should be mandatory\nWA2: The field should be optional\nWA3: The field should be ignored"}
{"prompt": "Context: issue as field numbers are used to identify fields on the wire. The restrictions of encoding with gRPC mean the definition must be very specific. REST and OpenAPI are quite forgiving as the specification is only a guide.2 Extra fields and ordering do not matter in OpenAPI, and therefore versioning and compatibility is even more important when it comes to gRPC:   syntax = \"proto3\"; option java_multiple_files = true; package com.masteringapi.attendees.grpc.server; message AttendeesRequest { } message Attendee { int32 id = 1; string givenName = 2; string surname = 3; string email = 4; } message AttendeeResponse { repeated Attendee attendees = 1; } service AttendeesService { rpc getAttendees(AttendeesRequest) returns (AttendeeResponse); }  The following Java code demonstrates a simple structure for implementing the behavior on the generated gRPC server classes:   @GrpcService public class AttendeesServiceImpl extends AttendeesServiceGrpc.AttendeesServiceImplBase { @Override public void getAttendees(AttendeesRequest request, StreamObserver<AttendeeResponse> responseObserver) { AttendeeResponse.Builder responseBuilder = AttendeeResponse.newBuilder(); //populate response responseObserver.onNext(responseBuilder.build()); responseObserver.onCompleted(); } }  You can find the Java service modeling this example on this book\u2019s GitHub page. gRPC cannot be queried directly from a browser without additional libraries, however you can install gRPC UI to use the browser for testing. grpcurl\n\nQUESTIONS:", "completion": "Q: What syntax is used in gRPC?\nA: Syntax = \"proto3\";\nWA1: Syntax = \"proto2\";\nWA2: Syntax = \"JSON\";\nWA3: Syntax = \"XML\";\n\nQ: What does the following Java code demonstrate?\nA: A simple structure for implementing the behavior on the generated gRPC server classes\nWA1: A simple structure for implementing the behavior on the generated REST server classes\nWA2: A simple structure for implementing the behavior on the generated OpenAPI server classes\nWA3: A simple structure for implementing the behavior on the generated GraphQL server classes\n\nQ: How can gRPC be tested from a browser?\nA: Install gRPC UI\nWA1: Use Postman\nWA2: Use cURL\nWA3: Use SOAP UI"}
{"prompt": "Context: also provides a command-line tool:   $ grpcurl -plaintext localhost:9090 \\ com.masteringapi.attendees.grpc.server.AttendeesService/getAttendees { \"attendees\": [ { \"id\": 1, \"givenName\": \"Jim\", \"surname\": \"Gough\", \"email\": \"gough@mail.com\" } ] }  gRPC gives us another option for querying our service and defines a specification for the consumer to generate code. gRPC has a more strict specification than OpenAPI and requires methods/internals to be understood by the consumer.      Modeling Exchanges and Choosing an API Format   In the Introduction we discussed the concept of traffic patterns and the difference between requests originating from outside the ecosystem and requests within the ecosystem. Traffic patterns are an important factor in determining the appropriate format of API for the problem at hand. When we have full control over the services and exchanges within our microservices-based architecture, we can start to make compromises that we would not be able to make with external consumers.  It is important to recognize that the performance characteristics of an east\u2013west service are likely to be more applicable than a north\u2013south service. In a north\u2013south exchange, traffic originating from outside the producer\u2019s environment will generally involve the exchange using the internet. The internet introduces a high degree of latency, and an API architecture should always consider the compounding effects of each service. In a microservices-based architecture it is likely that\n\nQUESTIONS:", "completion": "Q: What command-line tool does gRPC provide?\nA: $ grpcurl -plaintext localhost:9090 \\ com.masteringapi.attendees.grpc.server.AttendeesService/getAttendees\nWA1: $ grpcurl -plaintext localhost:9090 \\ com.masteringapi.attendees.grpc.server.AttendeeService/getAttendees\nWA2: $ grpcurl -plaintext localhost:9090 \\ com.masteringapi.attendees.grpc.server.AttendanceService/getAttendees\nWA3: $ grpcurl -plaintext localhost:8000 \\ com.masteringapi.attendees.grpc.server.AttendeesService/getAttendees\n\nQ: What should API architecture always consider?\nA: The compounding effects of each service.\nWA1: The performance characteristics of an eastwest service.\nWA2: The interactions between services.\nWA3: The internet introducing a high degree of latency."}
{"prompt": "Context: one north\u2013south request will involve multiple east\u2013west exchanges. High east\u2013west traffic exchange needs to be efficient to avoid cascading slow-downs propagating back to the consumer.      High-Traffic Services   In our example, Attendees is a central service. In a microservices-based architecture, components will keep track of an attendeeId. APIs offered to consumers will potentially retrieve data stored in the Attendee service, and at scale it will be a high-traffic component. If the exchange frequency is high between services, the cost of network transfer due to payload size and limitations of one protocol versus another will be more profound as usage increases. The cost can present itself in either monetary costs of each transfer or the total time taken for the message to reach the destination.      Large Exchange Payloads   Large payload sizes may also become a challenge in API exchanges and are susceptible to decreasing transfer performance across the wire. JSON over REST is human readable and will often be more verbose than a fixed or binary representation fuelling an increase in payload sizes.      Tip   A common misconception is that \u201chuman readability\u201d is quoted as a primary reason to use JSON in data transfers. The number of times a developer will need to read a message versus the performance consideration is not a strong case with modern tracing tools. It is also rare that large JSON files will be read from beginning to end. Better logging and error handling can\n\nQUESTIONS:", "completion": "Q: What is the cost of network transfer due to payload size and limitations of one protocol versus another?\nA: The cost can present itself in either monetary costs of each transfer or the total time taken for the message to reach the destination.\nWA1: Increased data storage costs\nWA2: Increased API call frequency\nWA3: Increased bandwidth usage\n\nQ: What is a common misconception regarding the use of JSON in data transfers?\nA: A common misconception is that human readability is quoted as a primary reason to use JSON in data transfers.\nWA1: JSON is always the most efficient data format\nWA2: JSON is not suitable for large file transfers\nWA3: JSON is not machine readable"}
{"prompt": "Context: mitigate the human-readable argument.    Another factor in large payload exchanges is the time it takes components to parse the message content into language-level domain objects. Performance time of parsing data formats varies vastly depending on the language a service is implemented in. Many traditional server-side languages can struggle with JSON compared to a binary representation, for example. It is worth exploring the impact of parsing and include that consideration when choosing an exchange format.      HTTP/2 Performance Benefits   Using HTTP/2-based services can help to improve performance of exchanges by supporting binary compression and framing. The binary framing layer is transparent to the developer, but behind the scenes will split and compress the message into smaller chunks. The advantage of binary framing is that it allows for a full request and response multiplexing over a single connection. Consider processing a list in another service and the requirement is to retrieve 20 different attendees; if we retrieved these as individual HTTP/1 requests it would require the overhead of creating 20 new TCP connections. Multiplexing allows us to perform 20 individual requests over a single HTTP/2 connection.  gRPC uses HTTP/2 by default and reduces the size of exchange by using a binary protocol. If bandwidth is a concern or cost, then gRPC will provide an advantage, in particular as content payloads increase significantly in size. gRPC may be beneficial compared to\n\nQUESTIONS:", "completion": "Q: What is a factor in large payload exchanges?\nA: The time it takes components to parse the message content into language-level domain objects.\nWA1: The size of the payload.\nWA2: The speed of the connection.\nWA3: The type of language used.\n\nQ: How can HTTP/2 help improve performance?\nA: It supports binary compression and framing.\nWA1: It creates multiple connections.\nWA2: It reduces the size of the payload.\nWA3: It increases connection speed.\n\nQ: What is the advantage of binary framing?\nA: It allows for a full request and response multiplexing over a single connection.\nWA1: It increases connection speed.\nWA2: It reduces the size of the payload.\nWA3: It increases bandwidth."}
{"prompt": "Context: REST if payload bandwidth is a cumulative concern or the service exchanges large volumes of data. If large volumes of data exchanges are frequent, it is also worth considering some of the asynchronous capabilities of gRPC.      Tip   HTTP/3 is on the way and it will change everything. HTTP/3 uses QUIC, a transport protocol built on UDP. You can find out more in HTTP/3 explained.      Vintage Formats   Not all services in an architecture will be based on a modern design. In Chapter 8 we will look at how to isolate and evolve vintage components, as older components will be an active consideration for evolving architectures. It is important that those involved with an API architecture understand the overall performance impact of introducing vintage components.      Guideline: Modeling Exchanges   When the consumer is the legacy conference system team, the exchange is typically an east\u2013west relationship. When the consumer is the CFP team, the exchange is typically a north\u2013south relationship. The difference in coupling and performance requirements will require the teams to consider how the exchange is modeled. You will see some aspects for consideration in the guideline shown in Table 1-3.  Table 1-3. Modeling exchanges guideline Decision  What format should we use to model the API for our service?    Discussion Points  Is the exchange a north\u2013south or east\u2013west exchange? Are we in control of the consumer code?  Is there a strong business domain across multiple services or do we\n\nQUESTIONS:", "completion": "Q: What is HTTP/3?\nA: HTTP/3 uses QUIC, a transport protocol built on UDP.\nWA1: HTTP/3 is a communication protocol\nWA2: HTTP/3 is a data storage system\nWA3: HTTP/3 is a web browser\n\nQ: What should be considered when introducing vintage components?\nA: The overall performance impact of introducing vintage components.\nWA1: The security impact of introducing vintage components.\nWA2: The design impact of introducing vintage components.\nWA3: The cost impact of introducing vintage components."}
{"prompt": "Context: want to allow consumers to construct their own queries?  What versioning considerations do we need to have?  What is the deployment/change frequency of the underlying data model?  Is this a high-traffic service where bandwidth or performance concerns have been raised?    Recommendations  If the API is consumed by external users, REST is a low barrier to entry and provides a strong domain model. External users also usually means that a service with loose coupling and low dependency is desirable.  If the API is interacting between two services under close control of the producer or the service is proven to be high traffic, consider gRPC.      Multiple Specifications   In this chapter we have explored a variety of API formats to consider in an API architecture, and perhaps the final question is \u201cCan we provide all formats?\u201d The answer is yes, we can support an API that has a RESTful presentation, a gRPC service, and connections into a GraphQL schema. However, it is not going to be easy and may not be the right thing to do. In this final section, we will explore some of the options available for a multiformat API and the challenges it can present.      Does the Golden Specification Exist?   The .proto file for attendees and the OpenAPI Specification do not look too dissimilar; they contain the same fields and both have data types. Is it possible to generate a .proto file from an OAS using the openapi2proto tool? Running openapi2proto --spec spec-v2.json will output the .proto\n\nQUESTIONS:", "completion": "Q: What versioning considerations do we need to have? \nA: Consider providing multiple versions of the API to accommodate different versions of the underlying data model.\nWA1: Different versions of the API should not be supported.\nWA2: Versioning of the underlying data model is not necessary.\nWA3: Changes to the underlying data model do not need to be reflected in the API.\n\nQ: Is it possible to generate a .proto file from an OAS using the openapi2proto tool?\nA: Yes, it is possible to generate a .proto file from an OAS using the openapi2proto tool.\nWA1: No, it is not possible to generate a .proto file from an OAS.\nWA2: The .proto file for attendees and the OpenAPI Specification cannot look too dissimilar.\nWA3: The openapi2proto tool is not necessary to generate a .proto file from an OAS."}
{"prompt": "Context: file with fields ordered alphabetically by default. This is fine until we add a new field to the OAS that is backward compatible and suddenly the ID of all fields changes, breaking backward compatibility.  The following sample .proto file shows that adding a_new_field would be alphabetically added to the beginning, changing the binary format and breaking existing services:   message Attendee { string a_new_field = 1; string email = 2; string givenName = 3; int32 id = 4; string surname = 5; }      Note   Other tools are available to solve the specification conversion problem, however it is worth noting that some tools only support OpenAPI Specification version 2. The time taken to move between versions 2 and 3 in some of the tools built around OpenAPI has led to many products needing to support both versions of the OAS.    An alternative option is grpc-gateway, which generates a reverse proxy providing a REST facade in front of the gRPC service. The reverse proxy is generated at build time against the .proto file and will produce a best-effort mapping to REST, similar to openapi2proto. You can also supply extensions within the .proto file to map the RPC methods to a nice representation in the OAS:   import \"google/api/annotations.proto\"; //... service AttendeesService { rpc getAttendees(AttendeesRequest) returns (AttendeeResponse) {         option(google.api.http) = {             get: \"/attendees\"       }; }  Using grpc-gateway gives us another option for presenting both a\n\nQUESTIONS:", "completion": "Q: What is the best approach to keep backward compatibility when introducing a new field in an OAS?\nA: Use a tool that supports OpenAPI Specification version 3.\nWA1: Add the new field at the end of the OAS.\nWA2: Use openapi2proto.\nWA3: Use a reverse proxy generated against the .proto file.\n\nQ: What tool can generate a reverse proxy providing a REST facade in front of the gRPC service?\nA: grpc-gateway.\nWA1: openapi2proto.\nWA2: OpenAPI Specification version 3.\nWA3: A .proto file."}
{"prompt": "Context: REST and gRPC service. However, grpc-gateway involves several commands and a setup that would only be familiar to developers who work with the Go language or build environment.      Challenges of Combined Specifications   It\u2019s important to take a step back here and consider what we are trying to do. When converting from OpenAPI we are effectively trying to convert our RESTful representation into a gRPC series of calls. We are trying to convert an extended hypermedia domain model into a lower-level function-to-function call. This is a potential conflation of the difference between RPC and APIs and is likely going to result in wrestling with compatibility.  With converting gRPC to OpenAPI we have a similar issue; the objective is trying to take gRPC and make it look like a REST API. This is likely going to create a difficult series of issues when evolving the service.  Once specifications are combined or generated from one another, versioning becomes a challenge. It is important to be mindful of how both the gRPC and OpenAPI Specifications maintain their individual compatibility requirements. An active decision should be made as to whether coupling the REST domain to an RPC domain makes sense and adds overall value.  Rather than generate RPC for east\u2013west from north\u2013south, what makes more sense is to carefully design the microservices-based architecture (RPC) communication independently from the REST representation, allowing both APIs to evolve freely. This is the choice we\n\nQUESTIONS:", "completion": "Q: What is the challenge of combining REST and gRPC service?\nA: Its important to consider the difference between RPC and APIs and how the REST domain will interact with the RPC domain.\nWA1: Understanding the Go language and build environment.\nWA2: Generating RPC for eastwest from northsouth.\nWA3: Making the gRPC look like a REST API.\n\nQ: What is the best approach to tackling the challenge?\nA: Carefully design the microservices-based architecture (RPC) communication independently from the REST representation.\nWA1: Generating gRPC to OpenAPI.\nWA2: Versioning the gRPC and OpenAPI Specification.\nWA3: Combining or generating specifications from one another."}
{"prompt": "Context: have made for the conference case study and would be recorded as an ADR in the project.      Summary   In this chapter we have covered how to design, build, and specify APIs and the different circumstances under which you may choose REST or gRPC. It is important to remember that it is not REST versus gRPC, but rather given the situations, which is the most appropriate choice for modeling the exchange. The key takeaways are:  The barrier to building REST- and RPC-based APIs is low in most technologies. Carefully considering the design and structure is an important architectural decision.    When choosing between REST and RPC models, consider the Richardson Maturity Model and the degree of coupling between the producer and consumer.    REST is a fairly loose standard. When building APIs, conforming to an agreed API standard ensures your APIs are consistent and have the expected behavior for your consumers. API standards can also help to short-circuit potential design decisions that could lead to an incompatible API.    OpenAPI Specifications are a useful way of sharing API structure and automating many coding-related activities. You should actively select OpenAPI features and choose what tooling or generation features will be applied to projects.    Versioning is an important topic that adds complexity for the producer but is necessary to ease API usage for the consumer. Not planning for versioning in APIs exposed to consumers is dangerous. Versioning should be an active\n\nQUESTIONS:", "completion": "Q: What is the key takeaway from the chapter?\nA: Carefully considering the design and structure is an important architectural decision.\nWA1: Designing APIs is an unnecessary task.\nWA2: It is always better to use REST over gRPC.\nWA3: OpenAPI Specifications are not useful for sharing API structure.\n\nQ: What is the Richardson Maturity Model?\nA: The Richardson Maturity Model is a model used to consider the degree of coupling between the producer and consumer when choosing between REST and RPC models.\nWA1: The Richardson Maturity Model is a model used for versioning APIs.\nWA2: The Richardson Maturity Model is a model used to analyze the reliability of an API.\nWA3: The Richardson Maturity Model is a model used to assess the performance of an API."}
{"prompt": "Context: decision in the product feature set and a mechanism to convey versioning to consumers should be part of the discussion.    gRPC performs incredibly well in high-bandwidth exchanges and is an ideal option for east\u2013west exchanges. Tooling for gRPC is powerful and provides another option when modeling exchanges.    Modeling multiple specifications starts to become quite tricky, especially when generating from one type of specification to another. Versioning complicates matters further but is an important factor to avoid breaking changes. Teams should think carefully before combining RPC representations with RESTful API representations, as there are fundamental differences in terms of usage and control over the consumer code.      The challenge for an API architecture is to meet the requirements from a consumer business perspective, to create a great developer experience around APIs, and to avoid unexpected compatibility issues. In Chapter 2 you will explore testing, which is essential in ensuring services meet these objectives.      1 We have been in this situation many times, usually first thing on a Monday!  2 Validation of OpenAPI Specifications at runtime helps enforce a greater strictness.      Chapter 2. Testing APIs   Chapter 1 covered the different types of APIs and the value that they provide to your architecture. This chapter closes out the Designing, Building, and Testing APIs section of this book by reviewing approaches to testing APIs. The new Attendee API that was\n\nQUESTIONS:", "completion": "Q: What is gRPC used for?\nA: gRPC performs incredibly well in high-bandwidth exchanges and is an ideal option for eastwest exchanges.\nWA1: gRPC is used for modeling multiple specifications.\nWA2: gRPC is used to create a great developer experience around APIs.\nWA3: gRPC is used to avoid unexpected compatibility issues.\n\nQ: What is the challenge for an API architecture?\nA: The challenge for an API architecture is to meet the requirements from a consumer business perspective, to create a great developer experience around APIs, and to avoid unexpected compatibility issues.\nWA1: The challenge for an API architecture is to avoid breaking changes.\nWA2: The challenge for an API architecture is to create a great developer experience around APIs.\nWA3: The challenge for an API architecture is to enforce a greater strictness."}
{"prompt": "Context: extracted within the Introduction should obviously be tested and validated. We believe that testing is core to building APIs. It helps provide a high level of confidence to you that your service is working as expected, which will help you deliver a quality product to consumers of your API. It is only by testing your API under varying conditions that you will gain the confidence that it is operating correctly.  When building APIs, as with creating any product, the only way to verify that the product works as expected is to test it. In the case of a mouthguard, this can mean stretching, hitting, pushing, and pulling the product, or even running simulations.1  As discussed in \u201cSpecifying REST APIs Using OpenAPI\u201d, an API should not return anything that differs from what is documented. It is also frustrating when an API introduces breaking changes or causes network timeouts due to the large duration of time to retrieve a result. These types of issues drive customers away and are entirely preventable by creating quality tests around the API service. Any API built should be ready to fulfill a variety of requirements, including sending useful feedback to users who provide a bad input, being secure, and returning results within a specified service-level objective (SLO) based on our service-level indicators (SLIs) that are agreed.2  In this chapter we will introduce the different types of testing that you can apply to your API to help avoid these issues from occurring.  We will\n\nQUESTIONS:", "completion": "Q: What is the purpose of testing an API?\nA: To verify that the product works as expected and provide a high level of confidence that the service is operating correctly.\nWA1: To build the API.\nWA2: To introduce breaking changes.\nWA3: To return results quickly.\n\nQ: What should an API not do?\nA: Return anything that differs from what is documented.\nWA1: Send useful feedback to users.\nWA2: Be secure.\nWA3: Follow service-level objectives."}
{"prompt": "Context: highlight the positives and the negatives of each type of testing, so you can decide where best to invest your time. We are going to focus on testing APIs and where we believe that you will gain the most value; we will not be covering generic testing of services. The chapter will contain additional resources for those readers seeking to gain a significantly more in-depth and specialist knowledge about testing.      Conference System Scenario for This Chapter   In \u201cAttendees Evolution ADR\u201d, we explained the reasons to separate the Attendee API from the rest of the conference system. The separation of the Attendee API introduces new interactions. The Attendee API will be used by the external CFP system and legacy conference system, as is shown in Figure 2-1. You will spend this chapter covering the testing needed for the Attendee service and how testing can help verify the interactions between the legacy conference system and the Attendee API. As a collective we have seen enough APIs that become inconsistent or produce accidental breaking changes as new releases are made, and this is primarily due to a lack of testing. For the new Attendee API it is important to ensure that it avoids these pitfalls by providing confidence that the correct results will always be returned, and the only way this can happen is by investing in the right levels of testing.      Figure 2-1. Scenario for the chapter      Testing can be applied at different levels of an API, starting with the individual\n\nQUESTIONS:", "completion": "Q: What is the purpose of the text?\nA: The text explains the importance of investing in testing for an API, and provides a scenario for the chapter.\nWA1: The text provides a deep and specialist knowledge about testing.\nWA2: The text explains how to separate the Attendee API from the rest of the conference system.\nWA3: The text explains how to design an API.\n\nQ: What is the main objective of testing?\nA: To provide confidence that the correct results will always be returned.\nWA1: To ensure the API is consistent.\nWA2: To ensure the API is secure. \nWA3: To ensure the API is bug-free."}
{"prompt": "Context: building blocks that make up the service, going all the way to verifying that it works as part of the entire ecosystem. Before showing you some tools and frameworks that are available for API testing, it is important to understand the strategies that can be used.      Testing Strategies   Testing is important; it ensures that you are building a working application. However, you don\u2019t want something that just works, you want something that also has the right behavior. Realistically, though, you have limited time and resources to write tests, so you will want to ensure that you are not wasting cycles writing tests that provide little to no value. After all, the value for customers is when you are running in production. Therefore, you need to be smart about deciding upon the coverage and proportions of the types of tests you should be using. Avoid creating irrelevant tests, duplicating tests, and any tests that are going to take more time and resources than the value they provide (i.e., flaky tests). Not all the testing that is introduced needs to be implemented to be able to release an API, as it may not be feasible due to time constraints and business demands.  To guide you to getting the right balance and the right tests for your case, we will introduce the test quadrant and test pyramid. These will give you focus on identifying the testing you should be implementing.      Test Quadrant   The test quadrant was first introduced by Brian Marick in his blog series on agile\n\nQUESTIONS:", "completion": "Q: What is the purpose of testing?\nA: Testing ensures that you are building a working application with the right behavior.\nWA1: To ensure the application is running in production.\nWA2: To create irrelevant tests.\nWA3: To duplicate tests.\n\nQ: What are the two strategies introduced to guide you to getting the right balance and the right tests for your case?\nA: The test quadrant and test pyramid.\nWA1: The test cube and test triangle.\nWA2: The test pyramid and test cube.\nWA3: The test quadrant and test triangle."}
{"prompt": "Context: testing. This became popularized in the book Agile Testing by Lisa Crispin and Janet Gregory (Addison-Wesley). The technology side of building an API cares that it has been built correctly, that its pieces (e.g., functions or endpoints) respond as expected, and that it is resilient and continues to behave under abnormal circumstances. The business cares that the right service is being developed (i.e., in our case, that the Attendee API provides the right functionality). To clarify, the term \u201cthe business\u201d means someone who has a clear understanding of the product and the features and the functionality that should be developed; they need not have a technical understanding.  The test quadrant brings together tests that help technology and business stakeholders alike\u2014each perspective will have different opinions on priorities. The popular image of the test quadrant is shown in Figure 2-2.      Figure 2-2. Agile Test Quadrants from Agile Testing (Addison-Wesley) by Lisa Crispin and Janet Gregory      The test quadrant does not depict any order. The quadrants are labeled for convenience and this is a common source of confusion that Lisa describes in one of her blog posts. The four quadrants can be generally described as follows:  Q1  Unit and component tests for technology. These should verify that the service that has been created works, and this verification should be performed using automated testing.    Q2  Tests with the business. These ensure what is being built is serving a\n\nQUESTIONS:", "completion": "Q: What is the purpose of the test quadrant?\nA: The test quadrant brings together tests that help technology and business stakeholders alike.\nWA1: To clarify the term business\nWA2: To verify the service created \nWA3: To provide a technical understanding\n\nQ: Who popularized the term testing?\nA: Lisa Crispin and Janet Gregory popularized the term testing.\nWA1: Agile Testing\nWA2: Addison-Wesley\nWA3: Technology side of building an API"}
{"prompt": "Context: purpose. This is verified with automated testing and can also include manual testing.    Q3  Testing for the business. This is about ensuring that functional requirements are met and also includes exploratory testing. When Figure 2-2 was originally created, this type of testing was manual; now it is possible to perform automated testing in this area as well.      Q4  Ensuring that what exists works from a technical standpoint. From Q1 you know that what has been built works; however, when the product is being used, is it performing as expected? Examples of performing correctly from a technical standpoint could include security enforcement, SLA integrity, and autoscaling.      The left side of quadrant (Q1, Q2) is all about supporting the product. It helps guide the product and prevent defects. The right side (Q3, Q4) is about critiquing the product and finding defects. The top of the quadrant (Q2, Q3) is the external quality of your product, making sure that it meets your users\u2019 expectations. This is what the business finds important. The bottom of the quadrant (Q1, Q4) is the technology-facing tests to maintain the internal quality of your application.3  The test quadrant does not say where you should start testing; it helps guide you on the tests that you might want. This is something that you must decide and should be based on the factors important to you. For example, a ticketing system must handle large traffic spikes, so it may be best to start with ensuring that your\n\nQUESTIONS:", "completion": "Q1: What is the purpose of automated testing?\nA: Verifying that functional requirements are met.\nWA1: To support the product.\nWA2: Critiquing the product and finding defects.\nWA3: Ensuring that the product is performing as expected.\n\nQ2: What is the left side of the test quadrant about?\nA: Supporting the product and preventing defects.\nWA1: Ensuring that what exists works from a technical standpoint.\nWA2: Testing for the business.\nWA3: Critiquing the product and finding defects.\n\nQ3: What is the right side of the test quadrant about?\nA: Critiquing the product and finding defects.\nWA1: Ensuring that what exists works from a technical standpoint.\nWA2: Testing for the business.\nWA3: Supporting the product and preventing defects."}
{"prompt": "Context: ticket system is resilient (e.g., performance testing). This would be part of Q4.      Test Pyramid   In addition to the test quadrants, the test pyramid (also known as the test automation pyramid) can be used as part of your strategy for test automation. The test pyramid was first introduced in the book Succeeding with Agile by Mike Cohn. This pyramid illustrates a notion of how much time should be spent on a given test area, its corresponding difficulty to maintain, and the value it provides in terms of additional confidence. The test pyramid at its core has remained unchanged. It has unit tests as its foundation, service tests in the middle block, and UI tests at the peak of the pyramid. Figure 2-3 shows the areas of the test pyramid you will explore.      Figure 2-3. The test pyramid, showing the proportion of tests desired      The test automation pyramid shows the trade-offs that exist in terms of confidence, isolation, and scope. By testing small parts of the codebase, you have better isolation and faster tests; however, this does not give confidence that the whole application is working. By testing the entire application in its ecosystem, the opposite is true. The tests give you more confidence that the application is working, but the scope of the test will be large as many pieces will be interacting together. This also makes it more difficult to maintain and slow. The following defines each of the core elements of the test pyramid:  Unit tests are at the bottom of\n\nQUESTIONS:", "completion": "Q: What is the test pyramid?\nA: The test pyramid is a concept for test automation strategy that illustrates the trade-offs that exist in terms of confidence, isolation, and scope.\nWA1: The test pyramid is a concept for performance testing strategy.\nWA2: The test pyramid is a concept for manual testing strategy.\nWA3: The test pyramid is a concept for usability testing strategy.\n\nQ: What is the origin of the test pyramid?\nA: The test pyramid was first introduced in the book Succeeding with Agile by Mike Cohn.\nWA1: The test pyramid was first introduced in the book Automating Software Quality by Mark Fewster.\nWA2: The test pyramid was first introduced in the book Agile Testing by Lisa Crispin.\nWA3: The test pyramid was first introduced in the book Software Testing Fundamentals by Paul Ammann."}
{"prompt": "Context: the pyramid; they form the foundation of your testing. They test small, isolated units of your code to ensure that your defined unit is running as expected.4 If your test is going to escape the boundaries of your unit, you can use test doubles. Test doubles are objects that look like real versions of an external entity; however, they are under your control.5 Because unit tests form the foundation of your pyramid, there should be more unit tests than any other type of test; we recommend using TDD as a practice.6 TDD is about writing tests before you write the logic. Unit tests fit into Q1 of the test quadrant and are used to provide quality to the internals of the application. Unit testing will not be covered further as we will be focusing on tests that verify your API from an external consumer standpoint as opposed to the internals of an API.    Service tests make up the middle tier of the pyramid. They will provide you with more confidence that your API is working correctly than unit tests, though they are more expensive. The expense comes from the tests having a larger scope and less isolation, which incurs a higher maintenance and development cost. Service tests include some of the following cases: the verification that multiple units are working together, that behavior is as expected, and that the application itself is resilient. Therefore, service tests fit into Q1, Q2, and Q4 of the test quadrant.    UI tests sit at the top of the test pyramid. In the old days a\n\nQUESTIONS:", "completion": "Q: What are test doubles?\nA: Test doubles are objects that look like real versions of an external entity but are under the control of the tester.\nWA1: Test doubles are the same as unit tests.\nWA2: Test doubles are used to test the internals of the application.\nWA3: Test doubles are used to verify the API from an external consumer. \n\nQ: What is TDD?\nA: TDD is about writing tests before you write the logic.\nWA1: TDD is a form of unit testing.\nWA2: TDD is used to provide quality to the externals of the application.\nWA3: TDD is a type of UI test."}
{"prompt": "Context: majority of applications being built for the web were LAMP stacks, and the only way to test your application from the front to the backend was through the Web UI. There is a UI with APIs: it is just not graphical, so these tests will now be referred to as end-to-end tests. They cover the same ground of a request flowing from a start to an end point but do not necessarily imply or assume that traffic originates from a Web UI. End-to-end tests are the most complex. They have the largest scope and are slow to run; however, they will verify entire modules are working together so they provide lots of confidence. End-to-end tests will generally sit in Q2, Q3, Q4 of the quadrant. Tooling for testing has improved and become more advanced, and it is now making more and more of Q3 available for automation.      One type of test is not better than another\u2014the test pyramid is a guide to the proportions of each type of testing you should aim to implement. It can be tempting to ignore the test pyramid and concentrate on end-to-end testing as this gives a high degree of confidence. However, this is a fallacy and instead gives a false sense of security that these higher-level tests are of higher quality/value than unit tests. The fallacy gives rise to the ice cream cone representation of testing, which is the opposite of a test pyramid. For a robust argument on this topic, please read Steve Smith\u2019s blog post \u201cEnd-to-End Testing considered harmful\u201d. You may also consider implementing other\n\nQUESTIONS:", "completion": "Q: What is an end-to-end test?\nA: An end-to-end test covers the same ground of a request flowing from a start to an end point but do not necessarily imply or assume that traffic originates from a Web UI.\nWA1: An end-to-end test is a test that is used to verify entire modules are working together.\nWA2: An end-to-end test is a type of unit test.\nWA3: An end-to-end test is a test that is only used for web applications.\n\nQ: What is the test pyramid?\nA: The test pyramid is a guide to the proportions of each type of testing you should aim to implement.\nWA1: The test pyramid is a type of unit test.\nWA2: The test pyramid is a type of end-to-end test.\nWA3: The test pyramid is a type of integration test."}
{"prompt": "Context: proportions of tests, though it is not recommended. Martin Fowler wrote an updated piece on testing shapes and covered why he feels that testing that is guided by any shape other than the test pyramid is incorrect.      ADR Guideline for Testing Strategies   To help you decide on the testing strategy that you should use, the ADR Guideline in Table 2-1 should help you make an informed decision.  Table 2-1. ADR Guideline: Testing strategies Decision  When building your API, which testing strategy should be made part of the development process?    Discussion Points  Do all parties that have a stake in the API have the time and the availability to regularly discuss how the API should be working? If you are unable to effectively communicate with the stakeholders, you could end up stalling your product waiting for a decision to be made.  Are the skills and experience available to effectively use these testing strategies? Not everyone has used these practices before, so you need to weigh if you have the time resources to train everyone on them.  Are there other practices within your workplace that are recommended and should be used? Sometimes there can be internal strategies to building software that work for an organization or are required due to the nature of the business.    Recommendations  We recommend using test quadrants and the test pyramid.  The test quadrant is very valuable to ensure that your customers are getting the right product. The test quadrant coupled with the\n\nQUESTIONS:", "completion": "Q: What is the ADR Guideline for Testing Strategies?\nA: To help you decide on the testing strategy that you should use.\nWA1: To help you decide on the development process.\nWA2: To help you decide on the API should be working.\nWA3: To help you decide on the time resources.\n\nQ: What are the recommendations for testing strategies?\nA: We recommend using test quadrants and the test pyramid.\nWA1: We recommend using proportions of tests.\nWA2: We recommend using other practices within the workplace.\nWA3: We recommend using only the test pyramid."}
{"prompt": "Context: test pyramid will help you build a great API.  We do recognize that using the test quadrant in its truest form by having someone readily available from the business to help guide your testing is not always possible. However, at a minimum, use the test pyramid as this concentrates the automated side of the test quadrant. This at the very least will ensure that you find bugs early in your development cycle.  Whatever the case, you will always need someone to help guide the product direction.      Contract Testing   Contract testing has two entities: a consumer and a producer. A consumer requests data from an API (e.g., web client, terminal shell), and a producer (also known as a provider) responds to the API requests, i.e., it is producing data, such as a RESTful web service. A contract is a definition of an interaction between the consumer and producer. It is a statement to say if a consumer makes a request that matches the contract request definition, then the producer will return a response that matches the contract response definition. In the case of the Attendee API, it is a producer and the consumer is the legacy conference system. The legacy conference system is a consumer as it is calling the Attendee API.7 So why use contracts? What do they offer you?      Why Contract Testing Is Often Preferable   As you learned in \u201cSpecifying REST APIs Using OpenAPI\u201d, APIs should have a specification, and it is important that your API responses conform to the API specification that\n\nQUESTIONS:", "completion": "Q: What is the purpose of the test pyramid?\nA: The test pyramid helps you build a great API.\nWA1: To provide automated testing in the test quadrant.\nWA2: To ensure bugs are found early in the development cycle.\nWA3: To provide a guide to product direction.\n\nQ: What is the relationship between a consumer and a producer in contract testing?\nA: The consumer requests data from an API, and the producer responds to the API requests.\nWA1: The consumer and producer are interchangeable.\nWA2: The consumer produces data and the producer requests data.\nWA3: The consumer and producer both produce data."}
{"prompt": "Context: you have laid out. Having a written definition of these interactions that must be adhered to by the producer ensures that consumers can keep using your API and makes it possible to generate tests. The contract defines what a request and response should look like and these can be used to verify that the producer (the API) is fulfilling the contract. If you break a contract test, then it means that the producer is not fulfilling the contract anymore, which means that consumers will be broken.  As the contract has the response definition, it is also possible to generate a stub server.8 This stub server can be used by consumers to verify that they can call the producer correctly and parse the response from the producer. Contract testing can be performed locally\u2014it is not required to launch additional services, which makes it part of your service tests. Contracts will evolve and the consumers and producers pick up these changes as they are made available, which ensures that they are able to continually integrate with the latest contract.  There is already a lot of value here about why you would want to use contracts. Additionally, contract testing has a well-developed ecosystem. There are established methodologies that guide what the contract should be, as well as frameworks and test integrations to generate contracts and provide effective ways to distribute them. We believe that contracts are the best way to define interactions between the service you implement and a consumer.\n\nQUESTIONS:", "completion": "Q: What does contract testing ensure?\nA: It ensures that consumers can keep using your API\nWA1: It ensures that producers are fulfilling the contract\nWA2: It ensures that consumers are calling the producer correctly\nWA3: It ensures that contracts evolve\n\nQ: What is a stub server?\nA: It is a server that can be used to verify that consumers can call the producer correctly and parse the response from the producer\nWA1: It is a server that can be used to generate tests\nWA2: It is a server that can be used to verify that the producer is fulfilling the contract\nWA3: It is a server that can be used to distribute contracts"}
{"prompt": "Context: Other tests are important and should be implemented as well, but these offer the most bang for the buck.      Note   It is important to note that contract testing is not the same as saying that an API conforms to a schema. A system is either compatible with a schema (like OpenAPI Spec) or it is not; a contract is about a defined interaction between parties and provides examples. Matt Fellows has an excellent piece on this titled \u201cSchema-based contract testing with JSON schemas and Open API (Part 1)\u201d.      How a Contract Is Implemented   As mentioned, a contract is a shared definition of how a producer and consumer interact. The following example shows a contract for a GET request to the endpoint /conference/{conference-id}/attendees. It states that the expected response has a property called value that contains an array of values about the attendees. In this sample definition of a contract, you can see that it is defining an interaction, which is used to generate the tests and stub server:   Contract.make { request { description('Get a list of all the attendees at a conference') method GET() url '/conference/1234/attendees' headers { contentType('application/json') } } response { status OK() headers { contentType('application/json') } body( value: [ $( id: 123456, givenName: 'James', familyName: 'Gough' ), $( id: 123457, givenName: 'Matthew', familyName: 'Auburn' ) ] ) } }  In Figure 2-4 you see how the generated tests are used by the consumer and producer.      Figure 2-4.\n\nQUESTIONS:", "completion": "Q: What is contract testing?\nA: Contract testing is a shared definition of how a producer and consumer interact.\nWA1: Contract testing is a way to check if an API conforms to a schema.\nWA2: Contract testing is an automated unit testing process.\nWA3: Contract testing is a way to check if two systems are compatible.\n\nQ: What is an example of a contract?\nA: A contract is a GET request to the endpoint /conference/{conference-id}/attendees with a response body of a property called value containing an array of values about the attendees.\nWA1: A contract is a POST request to the endpoint /conference/{conference-id}/attendees with a response body of a property called value containing an array of values about the attendees.\nWA2: A contract is a PUT request to the endpoint /conference/{conference-id}/attendees with a response body of a property called value containing an array of values about the attendees.\nWA3: A contract is a DELETE request to the endpoint /conference/{conference-id}/attendees with a response body of a property called value containing an array of values about the attendees.\n\nQ: What is the purpose of generated tests?\nA: The generated tests are used by the consumer and producer.\nWA1: The generated tests are used to generate the tests"}
{"prompt": "Context: Generated stub server and tests from a contract      Warning   It is tempting to use contracts for scenario tests. For example:  Step 1: add an attendee to a conference.  Step 2: get the list of attendees of the conference and check that the attendee was added correctly.    Frameworks do support this but also discourage it. Contracts are about defining interaction; if you wish to test this type of behavior, then use component tests.    A key benefit of using contracts is that once the producer agrees to implement a contract, this decouples the dependency of building the consumer and producer.      Tip   We have used generated stub servers to run demos for stakeholders. This was useful as the producer was still implementing the logic; however, they had agreed to the contracts.    The consumer has a stub server to develop against and the producer has tests to ensure that they are building the right interaction. The contract test process saves time, as when both the consumer and producer are deployed, they should integrate seamlessly.      Note   The generated tests need to be executed against your running API (producer). When your API launches, you should use test doubles for external dependencies. You do not want to be testing integrations with other services as part of your generated contract tests against the consumer.    To understand how contracts are agreed upon, let\u2019s look at the two main contract methodologies.      Producer contracts   Producer contract testing is when\n\nQUESTIONS:", "completion": "Q: What type of tests should be used to test interactions between components?\nA: Component tests\nWA1: Integration tests\nWA2: Contract tests\nWA3: Scenario tests\n\nQ: What is a key benefit of using contracts?\nA: It decouples the dependency of building the consumer and producer.\nWA1: It increases the speed of building the consumer and producer.\nWA2: It ensures seamless integration when both the consumer and producer are deployed.\nWA3: It saves time in the contract test process."}
{"prompt": "Context: a producer defines its own contracts. This practice is commonly utilized when your API is being used outside your immediate organization (i.e., external third parties). When you\u2019re developing an API for an external audience, the API needs to maintain its integrity, because the interface cannot make breaking changes without a migration plan, as you learned in \u201cAPI Versioning\u201d. Though interactions will be updated and improved, no individual consumer is likely to be able to ask for changes that affect the whole API and receive a quick change, because these changes need to be carefully orchestrated.  A real-world example of such an API is the Microsoft Graph API. Microsoft has thousands of consumers of this API from companies all over the world. Having companies or individuals adjust contracts for the Graph API with what they believe the contract should look like isn\u2019t feasible. That is not to say that changes should not be suggested to Microsoft, as they definitely are. However, even if a change is agreed to it will not be made quickly as the change will need to be verified and tested carefully.  If the Attendee API is going to be made available for public consumption, then the same concerns occur. What is important for the Attendee API is to use contracts to ensure that the interactions do not diverge and that the data returned is consistent.  Another reason to use producer contracts is that it is easier to get started. It is a good way to introduce contracts to your APIs.\n\nQUESTIONS:", "completion": "Q: What is a producer contract?\nA: A producer contract is a practice utilized when an API is being used outside of an immediate organization, where the API needs to maintain its integrity, as changes to the interface cannot be made quickly without a migration plan.\nWA1: A producer contract is when the consumer of an API can make changes to the interface quickly.\nWA2: A producer contract is a tool used to introduce contracts to the API.\nWA3: A producer contract is a legal document between two parties.\n\nQ: What is an example of an API that is utilized outside of an immediate organization?\nA: The Microsoft Graph API is an example of an API that is utilized outside of an immediate organization. \nWA1: The Attendee API is an example of an API that is utilized outside of an immediate organization.\nWA2: The Google Maps API is an example of an API that is utilized outside of an immediate organization.\nWA3: The Twitter API is an example of an API that is utilized outside of an immediate organization.\n\nQ: What is the purpose of a producer contract?\nA: The purpose of a producer contract is to ensure that the interactions do not diverge and that the data returned is consistent.\nWA1: The purpose of a producer contract is to introduce contracts to the API.\nWA2: The purpose of a producer contract is to make changes to the interface quickly.\nWA3"}
{"prompt": "Context: Having contracts is far more beneficial than not having them. However, when consumers and producers are both in the same organization, we suggest that you use the consumer-driven contracts methodology.      Consumer-driven contracts   Consumer-driven contracts (CDCs), by definition, are implemented by a consumer driving the functionality that they wish to see in an interaction. Consumers submit contracts, or changes to a contract, to the producer for new or additional API functionality. When the new/updated contract is submitted to the producer, a discussion about the change will begin, which will result in accepting or rejecting this change.  CDC is very much an interactive and social process. The owners of the applications that are consumers and producers should be within reach (e.g., in the same organization as one another). When a consumer would like a new interaction (e.g., API call) or have an interaction updated (e.g., a new property added), then they submit a request for that feature.      Case study: Applying CDC   In our case, this may mean that a pull request is submitted from the legacy conference system to the new Attendee API service. The request for the new interaction is then reviewed and a discussion takes place about this new functionality. This discussion is to ensure that this is something that the Attendee service should and will fulfill. For example, if a contract is suggested for a PUT request, a discussion can take place, as it may be preferable to\n\nQUESTIONS:", "completion": "Q: What is the definition of Consumer-driven contracts (CDCs)?\nA: Consumer-driven contracts (CDCs), by definition, are implemented by a consumer driving the functionality that they wish to see in an interaction. \nWA1: Consumer-driven contracts (CDCs) are contracts between consumers and producers. \nWA2: Consumer-driven contracts (CDCs) are contracts between consumers and companies. \nWA3: Consumer-driven contracts (CDCs) are contracts between producers and companies. \n\nQ: What is an example of a contract suggested for a PUT request?\nA: A pull request from the legacy conference system to the new Attendee API service.\nWA1: A request for a new API call. \nWA2: A request for a new property added. \nWA3: A discussion about the change."}
{"prompt": "Context: have this as a PATCH request.  This is where a good part of the value of contracts comes from: this discussion for both parties about what the problem is, and using a contract to assert that this is what the two parties accept and agree to. Once the contract is agreed to, the producer (Attendee service) accepts the contract as part of the project and can start fulfilling it.      Contracts methodology overview   These methodologies should hopefully give an overview of how to use contracts as part of the development process. This should not be taken as gospel, as variations do exist on the exact steps. For example, one process may request that the consumer\u2014when writing the contract\u2014also creates a basic implementation of producer code to fulfill the contract. In another example, the consumer should TDD the functionality they require and then create the contract before submitting the pull request. The exact process that is put in place may vary by team. Once you understand the core concepts and patterns of CDC, the exact process that is used is just an implementation detail.  If you are starting out on a journey to add contracts, you should note that there is a cost\u2014the setup time to incorporate contracts into a project and also the cost of writing the contracts. It is worth looking at tooling that can create contracts for you based on an OpenAPI Specification.9      Contract testing frameworks   It is likely that when it comes to contract testing frameworks for HTTP, you will\n\nQUESTIONS:", "completion": "Q: What is the value of contracts?\nA: Contracts provide a discussion for both parties about what the problem is and a way to assert that both parties agree to the same thing.\nWA1: Contracts provide a way to argue with each other.\nWA2: Contracts provide a way to keep secrets.\nWA3: Contracts provide a way to assign tasks.\n\nQ: How can contracts be used in a development process?\nA: Contracts can be used as part of the development process by having the consumer write the contract and create a basic producer code to fulfill the contract, or by having the consumer TDD the functionality and create the contract before submitting the pull request.\nWA1: Contracts can be used as part of the development process by having the consumer write the contract and create a basic consumer code to fulfill the contract.\nWA2: Contracts can be used as part of the development process by having the producer write the contract and create a basic producer code to fulfill the contract.\nWA3: Contracts can be used as part of the development process by having the consumer TDD the functionality and submit the pull request before creating the contract."}
{"prompt": "Context: want to look at Pact. Pact has evolved into the default contract testing framework due to the ecosystem that has been built around it and the sheer number of languages it supports. Other contract testing frameworks are available, and they can be opinionated. Pact is opinionated; it enforces that you should perform CDC and is specifically designed for that. A test is written by a consumer and that test generates a contract, which takes the form of an intermediate representation of the interaction. This language-agnostic intermediate representation is why Pact has such wide language usage. Other frameworks have differing opinions; for example, Spring Cloud Contracts does not have a strong opinion on CDC or producer contracts, and either can be achieved. This is possible as with Spring Cloud Contracts you write the contracts by hand as opposed to having them generated. Though Spring Cloud Contracts is language agnostic by using a containerized version of the product, to get the most out of it you need to be using the Spring and JVM ecosystem.10  There are options for contract testing for other protocols; it is not exclusively for HTTP communications.      API contracts storage and publishing   Having seen how contracts work and methodologies of incorporating them into the development process, the next consideration becomes where contracts are stored and how they should be published.  There are a few options for storing and publishing contracts and these again depend on the setup\n\nQUESTIONS:", "completion": "Q: What is Pact?\nA: Pact is a contract testing framework that is opinionated and enforces consumer-driven contract testing.\nWA1: Pact is a language-specific testing framework.\nWA2: Pact is an application-specific testing framework. \nWA3: Pact is a protocol-specific testing framework.\n\nQ: What is the intermediate representation of an interaction in Pact?\nA: The intermediate representation of an interaction in Pact is a language-agnostic representation.\nWA1: The intermediate representation of an interaction in Pact is a language-specific representation.\nWA2: The intermediate representation of an interaction in Pact is a protocol-agnostic representation.\nWA3: The intermediate representation of an interaction in Pact is a product-agnostic representation.\n\nQ: What is the difference between Pact and Spring Cloud Contracts?\nA: Pact is opinionated and enforces consumer-driven contract testing, while Spring Cloud Contracts does not have a strong opinion on CDC or producer contracts and either can be achieved.\nWA1: Pact is language-agnostic while Spring Cloud Contracts is language-specific.\nWA2: Pact is opinionated while Spring Cloud Contracts is not opinionated.\nWA3: Pact is specifically designed for consumer-driven contract testing while Spring Cloud Contracts is designed for producer contracts."}
{"prompt": "Context: that is available to you and your organization.  Contracts can be stored alongside the producer code in version control (e.g., Git). They can also be published alongside your build into an artifact repository such as Artifactory.  Ultimately the contracts need to be obtainable by the producer and the consumer. The storage point also needs to allow for the submission of new contracts. The producer should have control over which contracts are accepted in the project and can ensure that undesired changes aren\u2019t made or additional contracts are added. The downside to this approach is that in a large organization it can be difficult to find all the API services that use contracts.  Another option is to store all the contracts in a centralized location to enable visibility into other API interactions that are available. This central location could be a Git repository, but the downside to this approach is that unless organized and set up correctly, it is possible and likely that contracts get pushed into a module that the producer has no intention of fulfilling.  Yet another option for storing contracts is to use a broker. The Pact contract framework has a broker product that can be used as a central location to host contracts. A broker can show all contracts that have been validated by the producer as the producer will publish those contracts that have been fulfilled. A broker can also see who is using a contract to produce a network diagram, integrate with CI/CD pipelines, and\n\nQUESTIONS:", "completion": "Q: What does Pact contract framework provide?\nA: A broker product that can be used as a central location to host contracts.\nWA1: A centralized location to store all contracts\nWA2: A network diagram\nWA3: CI/CD pipelines\n\nQ: What is the downside to storing contracts in a large organization?\nA: It can be difficult to find all the API services that use contracts.\nWA1: It is possible and likely that contracts get pushed into a module that the producer has no intention of fulfilling.\nWA2: It can be difficult to ensure that undesired changes arent made or additional contracts are added.\nWA3: It can be difficult to organize and set up correctly."}
{"prompt": "Context: provide even more valuable information. This is the most comprehensive solution available and if you use a framework that is compatible with the Pact Broker, then it is recommended.      ADR Guideline: Contract Testing   To understand if applying contract testing is valid for your case and weighing the pros and cons of using contracts, the ADR Guideline in Table 2-2 should help guide you to a decision.  Table 2-2. ADR Guideline: Contract testing Decision  When building an API should you use contract testing and, if so, should you use consumer-driven contracts or producer contracts?    Discussion Points  Determine whether you are ready to include contract testing as part of your API testing.  Do you want to add an extra layer of testing to your API that developers will be required to learn about?      If contracts have not been used before, then it requires time to decide how you will use them.  Should contracts be centralized or in a project?    Do additional tools and training need to be provided to help people with contracts?      If deciding to use contracts, then which methodology should be used\u2014CDC or producer contracts?  Do you know who will use this API?    Will this API be used just within your organization?    Does the API have consumers that are willing to engage with you to help drive your functionality?      Recommendations  We recommend using contract testing when building an API. Even if there is a developer learning curve and you are deciding how you are going\n\nQUESTIONS:", "completion": "Q: When building an API, should you use contract testing?\nA: Yes, we recommend using contract testing when building an API.\nWA1: No, contract testing is not necessary.\nWA2: Contract testing is optional.\nWA3: Contract testing is not recommended.\n\nQ: Should contracts be centralized or in a project?\nA: It depends on the situation and needs to be decided.\nWA1: Contracts should not be used at all.\nWA2: Contracts should always be centralized.\nWA3: Contracts should always be in a project."}
{"prompt": "Context: to set up your contracts for the first time, we believe it is worth the effort. Defined interactions that are tested save so much time when integrating services together.  If you are exposing your API to a large external audience, it is important to use producer contracts. Again, having defined interactions that help ensure that your API does not break backward compatibility is crucial.  If you\u2019re building an internal API, the ideal is to work toward CDC, even if you have to start with producer contracts and evolve over to CDC.  If contract testing is not feasible, then for a producer you need alternatives to ensure that your API is conforming your agreed interactions and provide a way that consumers can test. This means that you have to be very careful with your tests that the responses and requests match with what is expected, which can be tricky and time-consuming.      API Component Testing   Component testing can be used to validate that multiple units work together and should be used to validate behavior\u2014they are service tests in the test pyramid in Figure 2-3. An example of a component test is sending a request to your API and verifying the response. At a high level it will require that your application can read the request, perform authentication and authorization, deserialize a payload, perform the business logic, serialize the payload, and respond. That is a lot of units being tested, and it would be difficult to point to exactly where a bug could be. Where this\n\nQUESTIONS:", "completion": "Q: What is the ideal approach for building an internal API?\nA: Work toward CDC, even if you have to start with producer contracts and evolve over to CDC. \nWA1: Use producer contracts\nWA2: Use component testing\nWA3: Use API Component Testing\n\nQ: What is important to consider when exposing an API?\nA: Use producer contracts to ensure backward compatibility. \nWA1: Use component testing\nWA2: Use API Component Testing\nWA3: Ensure the responses and requests match what is expected"}
{"prompt": "Context: example differs from a contract test is that you should be checking that the service had the correct behavior; for example, if this was creating a new attendee, you want to verify that the service made a call to the (mocked) database. You are not just checking the shape of the response like contract tests do. As component tests verify multiple units together, they are (normally) slower running than unit tests. Component tests should not call out to external dependencies. Like contract testing, you are not using these tests to verify external integration points. The type of tests that you want to trigger in this scope varies based on the business case; however, for APIs you would be looking to validate cases such as:  Is the correct status code returned when a request is made?    Does the response contain the correct data?    Is an incoming payload rejected if a null or empty parameter is passed in?    When I send a request where the accepted content type is XML, will the data return the expected format?    If a request is made by a user who does not have the correct entitlements, what will the response be?    What will happen if an empty dataset is returned? Is this a 404 or is it an empty array?    When creating a resource, does the location header point to the new asset created?      From this selection of tests, you can see how these bleed into two areas of the test quadrant. This includes Q1, where you are confirming that the API being built works (i.e., it is producing\n\nQUESTIONS:", "completion": "Q: What is the difference between an example and a contract test?\nA: An example test checks the behavior of a service, while a contract test checks the shape of the response.\nWA1: Example tests call out to external dependencies.\nWA2: Contract tests are used to verify external integration points.\nWA3: Example tests are faster running than component tests.\n\nQ: What are component tests used to do?\nA: Component tests verify multiple units together.\nWA1: Component tests should call out to external dependencies.\nWA2: Component tests are used to verify external integration points.\nWA3: Component tests are used to confirm that the API is producing."}
{"prompt": "Context: results), and Q2, where you test to verify that the responses of the Attendee API are correct.      Contract Testing Versus Component Testing   If contract testing is not available, you should use API component tests to verify that your API conforms to your agreed interactions, i.e., your API specification. Using API component tests to verify that your API conforms to an interaction is not ideal\u2014for a start, it is much more likely to be error-prone and is tedious to write. You should make contracts your golden source of agreed interactions, as the generated tests ensure that the shape of your API is accurate.      Case Study: Component Test to Verify Behavior   Let\u2019s look at an example of a case for our Attendee API for the endpoint /conference/{conference-id}/attendees. This endpoint returns a list of the attendees at a conference event. For this component test, a mock is used to represent our external database dependency, and as seen in Figure 2-5, in this case that is the DAO.  Some things to test this endpoint for are:  Requests that are successful have response of 200 (OK)    Users without the right level of access will return a status of 403 (Forbidden)    When a conference has no attendees, an empty array will be returned      Figure 2-5. API Component test with mocked DAO      A library or testing framework that wraps a request client can be really useful. Here REST-Assured is used to call the Attendee API endpoint and to verify these test cases:11   @Test void\n\nQUESTIONS:", "completion": "Q: What should be used when contract testing is not available?\nA: API component tests\nWA1: Manual testing\nWA2: Manual verification\nWA3: Manual scripting\n\nQ: What is the response code of a successful request to the /conference/{conference-id}/attendees endpoint?\nA: 200 (OK)\nWA1: 201 (Created)\nWA2: 400 (Bad Request)\nWA3: 403 (Not Authorized)\n\nQ: What should be the golden source of agreed interactions?\nA: Contracts\nWA1: Manual testing\nWA2: Manual verification\nWA3: Manual scripting"}
{"prompt": "Context: response_for_attendees_should_be_200() { given() .header(\"Authorization\", VALID_CREDENTIAL) .when() .get(\"/conference/conf-1/attendees\") .then() .statusCode(HttpStatus.OK.value()); } @Test void response_for_attendees_should_be_403() { given() .header(\"Authorization\", INVALID_CREDENTIAL) .when() .get(\"/conference/conf-1/attendees\") .then() .statusCode(HttpStatus.FORBIDDEN.value()); ... }  Running this type of test gives us confidence that our API is behaving correctly.      API Integration Testing   Integration tests in our definition are tests across boundaries between the module being developed and any external dependencies. Integration tests are a type of service test and can be seen in the test pyramid image in Figure 2-3.  When performing integration testing, you want to confirm that the communication across the boundary is correct; i.e., your service can correctly communicate with another service that is external to it.  The types of things you want to verify are the following:  Ensuring that an interaction is being made correctly; e.g., for a RESTful service, this may be specifying the correct URL or that the payload body is correct.    Can the unit that is interacting with an external service handle the responses that are being returned?      In our case the legacy conference system needs to verify that it can make a request to the new Attendee API and can interpret the response.      Using Stub Servers: Why and How   If you are using contract tests, the generated stub\n\nQUESTIONS:", "completion": "Q: What type of test gives us confidence that our API is behaving correctly?\nA: Integration tests\nWA1: Unit tests\nWA2: Contract tests\nWA3: Stress tests\n\nQ: What do you want to verify when performing integration testing?\nA: Ensuring that an interaction is being made correctly\nWA1: Ensuring that there are no bugs in the code\nWA2: Ensuring that the code is well-written\nWA3: Ensuring that the system is secure"}
{"prompt": "Context: servers can be used to verify that the consumer can communicate with the producer. The legacy conference system has a generated stub server and can use this to test against. This will keep testing local, and the stub server will be accurate. This is the preferred option for testing an external boundary.  However, a generated stub server from a contract is not always available and other options are required, as in the case of testing with an external API, such as the Microsoft Graph API, or within your organization when contracts are not used. The simplest one is to hand roll a stub server that mimics the requests and responses of the service you interact with. This is certainly a viable option, as in your chosen language and framework it is usually very easy for a developer to create a stub server with canned responses that integrate with tests.  The key considerations when hand rolling a stub server is to make sure that the stub is accurate. It can be very easy to make mistakes, such as inaccurately portraying the URL or making mistakes in the response property names and values. Can you see the errors in this handtyped response?12   { \"values\": [ { \"id\": 123456, \"givenName\": \"James\", \"familyName\": \"Gough\" }, { \"id\": 123457, \"givenName\": \"Matthew\", \"familyNane\": \"Auburn\" }, { \"id\": 123456, \"givenName\": \"Daniel\", \"familyName\": \"Bryant\" } ] }  This should still not put you off as this is a good solution. One of the authors had great success with this approach after a\n\nQUESTIONS:", "completion": "Q: What is the preferred option for testing an external boundary?\nA: The legacy conference system has a generated stub server and can use this to test against.\nWA1: Making a contract\nWA2: Hand rolling a stub server\nWA3: Using Microsoft Graph API\n\nQ: What key considerations must be made when hand rolling a stub server?\nA: Make sure that the stub is accurate.\nWA1: Ensure the request is accurate\nWA2: Ensure the response is correct\nWA3: Ensure the URL is accurate"}
{"prompt": "Context: requirement for a project meant he had to hand roll a stub server for a login service.  A way to avoid these inaccuracies and to ensure that requests to URLs are accurately captured along with the responses is to use a recorder. It is possible to use a tool that will record the requests and responses to an endpoint and generate files that can be used for stubbing. Figure 2-6 shows how this works.      Figure 2-6. How a consumer of the Attendee API would use a recorder to capture a request/response for test data      These generated files are mappings that can then be used for tests to accurately portray requests and responses, and as they are not hand rolled they are guaranteed to be accurate at the point of generation. To use these generated files, a stub server is launched that is capable of reading in the mappings files. When a request is made to the stub server, it checks to see if the request matches any of the expected requests in the mappings file. If it matches, then the mapped response will be returned.13 Recording calls to APIs will produce more accurate stubs than that of hand rolling a stub. If you do use recordings, then you need to make sure they stay updated and in sync; also, if you make recordings against production, you need to watch that no PII is saved into the mapping files.      ADR Guideline: Integration Testing   Integration testing is important, so to help you understand what types of integration testing you need, see the ADR Guideline in Table 2-3.\n\nQUESTIONS:", "completion": "Q: What is a recorder?\nA: A recorder is a tool that can capture requests and responses to an endpoint and generate files that can be used for stubbing.\nWA1: A recorder is a tool that can be used to accurately portray requests and responses. \nWA2: A recorder is a tool that can be used to update and sync files.\nWA3: A recorder is a tool that can be used to store PII in mapping files.\n\nQ: What is the purpose of a stub server?\nA: The purpose of a stub server is to check if the incoming request matches any of the expected requests in the mappings file and return the mapped response if it matches.\nWA1: The purpose of a stub server is to accurately portray requests and responses.\nWA2: The purpose of a stub server is to update and sync files.\nWA3: The purpose of a stub server is to store PII in mapping files."}
{"prompt": "Context: Table 2-3. ADR Guideline: Integration testing Decision  Should integration testing be added to API testing?    Discussion Points  If your API is integrating with any other service, what level of integration test should you use?  Do you feel confident that you can just mock responses and do not need to perform integration tests?    For creating a stub server to test against, are you able to accurately craft the request and responses or should they be recorded?    Will you be able to keep stub servers up-to-date and recognize if an interaction is incorrect?      If your stubs are incorrect or become out of date, this means it is possible to have tests that pass against your stub server, but when you deploy to production, your service fails to interact with the other API as it has changed.      Recommendations  We do recommend using the generated stub servers from contract tests. However, if this is not available, then having integration testing using recordings of interactions is the next best option. Having integration tests that can be run locally gives confidence that an integration will work, especially when refactoring an integration; it will help to ensure that any changes have not broken anything.      Integration testing is a really useful tool; however, definitions of these interactions have issues. The main issue is that they are point-in-time snapshots. These bespoke setups do not get updated with changes.  We have been using stub servers for the integrations we have\n\nQUESTIONS:", "completion": "Q: Should integration testing be added to API testing?\nA: We do recommend using the generated stub servers from contract tests. However, if this is not available, then having integration testing using recordings of interactions is the next best option.\nWA1: No, integration testing should not be added to API testing.\nWA2: Integration testing is unnecessary and should not be added to API testing.\nWA3: Integration tests should be added but only after deploying to production.\n\nQ: What level of integration test should you use?\nA: Having integration tests that can be run locally gives confidence that an integration will work, especially when refactoring an integration; it will help to ensure that any changes have not broken anything.\nWA1: No integration test should be used.\nWA2: A high level of integration test should be used.\nWA3: Integration tests should always be automated."}
{"prompt": "Context: looked at; however, it is possible to use a real instance of the external service to verify an integration.      Containerizing Test Components: Testcontainers   It is common to build applications as containerized images, which means that many applications that your service will integrate with are also available as containerized solutions. These images can be run on your local machine as part of your testing. Not only does using local containers allow for testing communication with the external services, but also you can run the same image that is run in production.  Testcontainers is a library that integrates with your testing framework to orchestrate containers. Testcontainers will start and stop and generally organize the lifecycle of containers you use with your tests.      Case Study: Applying Testcontainers to Verify Integrations   Let\u2019s take a look at two use cases where this is helpful for the Attendee API. The first case is that the Attendee API service will support a gRPC interface as well as the RESTful interface. The gRPC interface is to be developed after the RESTful interface, but there are eager developers who want to start testing against a gRPC interface. The decision is made to provide a stub server for the gRPC interface, which will be a stub that provides a few canned responses. To achieve this goal, a bare-bones application is made that fulfills this objective. This gRPC stub is then packaged up, containerized, and published. This stub can be now used by\n\nQUESTIONS:", "completion": "Q: What is Testcontainers?\nA: Testcontainers is a library that integrates with your testing framework to orchestrate containers.\nWA1: Testcontainers is a tool to package applications into containers. \nWA2: Testcontainers is a tool to test communication between services.\nWA3: Testcontainers is a tool to verify integrations.\n\nQ: What was the use case for the Attendee API?\nA: The Attendee API service will support a gRPC interface as well as the RESTful interface.\nWA1: The Attendee API service will support only a gRPC interface. \nWA2: The Attendee API service will support only a RESTful interface.\nWA3: The Attendee API service will support only a GraphQL interface."}
{"prompt": "Context: the developers for testing across a boundary; i.e., they can make real calls to this stub server in their tests, and this containerized stub server can run locally on their machine.  The second use case is that the Attendee API service has a connection to an external database, which is an integration to test. The options for testing integration boundaries for a database would be to mock out the database, use an in-memory database (e.g., H2), or run a local version of the database using Testcontainers. Using a real instance of the database in your test provides a lot of value because with mocks you can mock the wrong return value or make an incorrect assumption. With an in-memory DB you are assuming that the implementation matches the real DB. Using a real instance of the dependency and it being the same version that you run in production means that you get reliable testing across a boundary, which ensures that the integration will work when going to production. In Figure 2-7 you see the structure of the test to confirm a successful integration across a boundary with a database.      Figure 2-7. Testcontainers DAO test      Testcontainers is a powerful tool and should be considered when testing boundaries between any external services. Other common external services that benefit from using Testcontainers include Kafka, Redis, and NGINX. Adding this type of solution will increase the time it takes for your tests to run; however, integration tests are usually fewer and the\n\nQUESTIONS:", "completion": "Q: What are the two use cases for the Attendee API service?\nA: The Attendee API service has a connection to an external database, and it can make real calls to a stub server in tests.\nWA1: The Attendee API service can make real calls to a production database.\nWA2: The Attendee API service can be used to mock out the database.\nWA3: The Attendee API service can be used to run a local version of NGINX.\n\nQ: What is the benefit of using a real instance of the database in tests?\nA: Using a real instance of the database in tests provides reliable testing across a boundary, ensuring integration will work when going to production.\nWA1: Using a real instance of the database in tests is more reliable than mocks.\nWA2: Using a real instance of the database in tests is faster than an in-memory DB.\nWA3: Using a real instance of the database in tests prevents incorrect assumptions."}
{"prompt": "Context: additional confidence that is provided more often than not is a worthwhile trade-off for additional time.  Using Testcontainers raises a couple of questions. First, is this type of testing considered integration testing or is it end-to-end testing as a real instance of another service is being tested against? Second, why not just use this instead of contracts?  Using Testcontainers does not make the tests end-to-end if it stays within the integration boundary. We suggest that you use Testcontainers to test integrations; ensuring that the container has the right behavior is not your job (assuming that the owner of the image is outside your domain). For example, if I issue a statement to publish a message to a Kafka broker, I should not then subscribe to the topic to check that the item published was correct. I should trust that Kafka is doing its job and subscribers would be getting the message. If you want to verify this behavior, make it part of your end-to-end tests. That is why the boundary of what you are testing matters, so the case of the DAO to the database is not end-to-end testing, because only the interactions across the boundary are validated.  Testcontainers and integrating with a real service is a real boon and can add a lot of value to your testing, though they are not a replacement for contracts just because you can use a real version of a service. Working with a real instance is nice; however, contracts provide so much more than just a stub server\u2014they provide\n\nQUESTIONS:", "completion": "Q: Is using Testcontainers considered integration testing or end-to-end testing? \nA: Integration testing\nWA1: End-to-end testing\nWA2: Unit testing\nWA3: System testing\n\nQ: Why not use Testcontainers instead of contracts?\nA: Testcontainers cannot replace contracts as they provide more than just a stub server\nWA1: Contracts are more expensive\nWA2: Testcontainers take more time\nWA3: Testcontainers are not reliable"}
{"prompt": "Context: all the testing, integration, and collaboration.      End-to-End Testing   The essence of end-to-end testing is to test services and their dependencies together to verify they work as expected. It is important to validate that when a request is made, and it reaches the front door (i.e., the request reaches your infrastructure), the request flows all the way through and the consumer gets the correct response. This validation gives confidence that all these systems work together as expected. For our case this is testing the legacy conference system, the new Attendee service, and the database all together.      Automating End-to-End Validation   This section concentrates on automated end-to-end tests. Automation is intended to save you time, so we will present automated testing that we believe gives you the best value. You will always need to verify that your systems work together\u2014however, you can do this manually in a testing environment before releasing software into production.      Warning   If you are building an external-facing API and you have multiple third parties that are consuming it, don\u2019t try to copy the third-party UI and replicate how it works. Doing so will mean that you spend huge amounts of time trying to replicate something out of your domain.    For end-to-end testing it is ideal is to have real versions of your services running and interacting together; however, sometimes this is not always feasible. Therefore, it is okay to stub out some entities of a\n\nQUESTIONS:", "completion": "Q: What is end-to-end testing?\nA: End-to-end testing is to test services and their dependencies together to verify they work as expected.\nWA1: End-to-end testing is to manually test services and their dependencies together.\nWA2: End-to-end testing is to automate services and their dependencies together.\nWA3: End-to-end testing is to replicate the third-party UI.\n\nQ: What is the purpose of automating end-to-end validation?\nA: The purpose of automating end-to-end validation is to save time.\nWA1: The purpose of automating end-to-end validation is to replicate the third-party UI.\nWA2: The purpose of automating end-to-end validation is to manually test services and their dependencies.\nWA3: The purpose of automating end-to-end validation is to debug services and their dependencies."}
{"prompt": "Context: system that are outside your organization\u2019s domain and are provided by an external party. A hypothetical case would be if the Attendee service required the use of AWS S3. Relying on an external entity opens up concerns such as network issues or even the external provider being unavailable. Also, if your tests are not going to use an entity, there is no need to make it available for your test. For an end-to-end test of the Attendee service, the database and the Attendee service need to be launched, but this does not require the legacy conference system, as it is superfluous. This is why end-to-end tests sometimes require boundaries. The boundary for this end-to-end test is shown in Figure 2-8.      Figure 2-8. End-to-end test scope      Managing and coordinating multiple systems together is not easy to automate and end-to-end tests can be brittle. However, running end-to-end tests locally is becoming easier. As you have just seen in \u201cContainerizing Test Components: Testcontainers\u201d, containerization allows you to spin up multiple systems locally. Even though this is getting easier, you should still follow the test pyramid guidelines\u2014end-to-end tests are at the top of the test pyramid for a reason.  When writing your end-to-end tests, you should use realistic payloads. We have seen cases where tests use small and concise payloads, then when investigating why APIs are breaking it is found that the consumers are regularly sending very large payloads\u2014larger than the buffers\n\nQUESTIONS:", "completion": "Q: What is the boundary for an end-to-end test of the Attendee service? \nA: The boundary for this end-to-end test is shown in Figure 2-8.\nWA1: The boundary for this end-to-end test is shown in Figure 2-7.\nWA2: The boundary for this end-to-end test is shown in Figure 2-9.\nWA3: The boundary for this end-to-end test is not defined.\n\nQ: What is the purpose of end-to-end tests?\nA: To manage and coordinate multiple systems together.\nWA1: To run tests locally.\nWA2: To test APIs.\nWA3: To use realistic payloads."}
{"prompt": "Context: support. This is why your end-to-end testing needs to be representative of the way that a consumer uses your API.      Types of End-to-End Tests   The end-to-end tests that you write should be driven off the requirements that are most important, as you saw in \u201cTest Quadrant\u201d.  Within Q3 of the test quadrant, you see scenario testing. Scenario tests are a common form of end-to-end testing. They are for testing out typical user journeys and provide confidence that your service is performing correctly. A scenario test can be based around a single action or multiple actions. It is important that you are only testing core user journeys and not testing edge cases or exception testing. To help you write your tests, you can use Behavior Driven Development (BDD). This is a nice way to write your user stories as part of your business-facing tests. An example for the conference system would be that when an attendee is registered for a conference talk, the attendee count should have increased when the conference talk information is retrieved.  The nice thing with scenario tests and validating these core user journeys is that you are not going to be concerned if a component is slower than in production. What is being interrogated is the correct behavior and expected results. However, you need to be more careful when running performance end-to-end tests. Performance testing, Q4 in the testing quadrant, should be deployed to a like-for-like environment of your production environment. If the\n\nQUESTIONS:", "completion": "Q: What is Behavior Driven Development (BDD) used for? \nA: It is used to write user stories as part of business-facing tests.\nWA1: To create tests for core user journeys.\nWA2: To run performance end-to-end tests.\nWA3: To test out typical user journeys.\n\nQ: What type of end-to-end tests are common?\nA: Scenario tests.\nWA1: Exception tests.\nWA2: Edge cases tests.\nWA3: Performance tests.\n\nQ: What kind of environment is needed to run performance end-to-end tests?\nA: A like-for-like environment of the production environment.\nWA1: A testing environment.\nWA2: A development environment.\nWA3: A simulated environment."}
{"prompt": "Context: two differ, you will not get results indicative of how your services are working. This does mean that you are going to need to deploy your services to this representative hardware, and depending on your resources and environment, that can be tricky. You should take this into consideration if it is going to make your tests flaky or is going to cost more development time than the return of confidence. However, this should not put you off because we have seen this type of end-to-end testing be successful.  The performance tests that you write as part of your end-to-end testing should be focused on ensuring that you are still serving requests within your targeted SLOs. You want these performance tests to show you have not introduced any sudden lag to your services (e.g., accidentally adding in some blocking code). If volume is important, you want to verify that your service is able to handle the loads you expect. Some great tools are available for performance testing, such as Gatling, JMeter, Locust, and K6. Even if none of these appeal to you, others are available and in many different languages that you should be familiar with. The performance figures that you want should be driven from your business requirements.  As part of your end-to-end testing, you should also ensure that your security is in place (i.e., TLS is turned on and the appropriate authentication mechanisms are in place). Security should not be turned off for these tests as it does not make it representative of a\n\nQUESTIONS:", "completion": "Q: Why should end-to-end testing be done?\nA: To ensure that services are still serving requests within targeted SLOs and that no sudden lag has been introduced.\nWA1: To make development time easier.\nWA2: To ensure that all security measures are in place.\nWA3: To make sure that hardware is representative.\n\nQ: What tools can be used for performance testing?\nA: Gatling, JMeter, Locust, and K6.\nWA1: Apache Web Server.\nWA2: Microsoft Access.\nWA3: Notepad."}
{"prompt": "Context: user journey, or it misrepresents metrics.  End-to-end testing is more complex than any other type of testing as it takes resources to create and maintain. Though it can be a time saver over doing end-to-end manual testing, it provides confidence in the application and evidence that services were working from a technical standpoint to meet service agreements.      ADR Guideline: End-to-End Testing   Knowing what to include, and whether end-to-end testing is worthwhile for your case, are important considerations. The ADR Guidelines in Table 2-4 should help you make a decision.  Table 2-4. ADR Guideline: End-to-End Testing Decision  As part of your testing setup, should you use automated end-to-end tests?    Discussion Points  Determine how complex your setup is to enable end-to-end testing. Do you have a good idea of end-to-end tests that you require and will provide value? Are there any specific requirements or more advanced end-to-end tests that you should add?      Recommendations  We recommend that you do perform at a minimum end-to-end testing on core user journeys. This is going to give feedback as early as possible in your development cycle that a user could be impacted with the changes that have been made. Ideally you can run these end-to-end tests locally; however, if not, then it should be part of your build pipeline.  End-to-end testing is valuable but must be balanced against the time investment you need to get it running. If it is not possible to do automated end-\n\nQUESTIONS:", "completion": "Q: What is the purpose of end-to-end testing?\nA: To provide confidence in the application and evidence that services were working from a technical standpoint to meet service agreements.\nWA1: To save time over doing manual testing.\nWA2: To determine how complex the setup is to enable end-to-end testing.\nWA3: To give feedback as early as possible in the development cycle.\n\nQ: What are the ADR Guidelines in Table 2-4?\nA: Knowing what to include, and whether end-to-end testing is worthwhile for your case, are important considerations.\nWA1: A decision on whether to use automated end-to-end tests.\nWA2: A discussion on the complexity of the setup and any specific requirements.\nWA3: A recommendation on what type of tests should be included."}
{"prompt": "Context: to-end testing, then you need to have a run book of manual tests that you can use. This run book should be used against a testing environment before a production release. This type of manual testing will considerably slow down your production releases and ability to deliver value to customers.      Summary   In this chapter you have learned about the core types of testing for APIs, including what should be tested and where time should be dedicated. The key takeaways are:  Stick to the fundamentals of testing and make unit testing a core of your API.    Contract testing can help you develop a consistent API and test with other APIs.    Perform service tests on your component and isolate the integrations to validate incoming and outgoing traffic.    Use end-to-end tests to replicate core user journeys to help validate that your APIs all integrate correctly.    Use the ADR Guidelines as a way to work out if you should add different tests to your API.      While we\u2019ve given you lots of information, ideas, and techniques for testing your API, this is by no means an exhaustive list of tools available. We encourage you to do some research on testing frameworks and libraries that you may want to use, to ensure you are making an informed decision.  However, no matter how much testing is done upfront, nothing is as good as seeing how an application actually runs in production. You will learn more about testing in production in Chapter 5. The next chapter will focus on exposing and\n\nQUESTIONS:", "completion": "Q: What should you use to slow down production releases?\nA: Manual testing\nWA1: Automated testing\nWA2: Contract testing\nWA3: End-to-end testing\n\nQ: What should you have to perform manual testing?\nA: A run book\nWA1: A testing environment\nWA2: An ADR guideline\nWA3: A research framework"}
{"prompt": "Context: managing APIs in a production setting using API gateways.      1 Matthew\u2019s friend owns a mouthguard company and was on the receiving end of hearing about the arduous process for testing the integrity of the product. No one wants a mouthguard where the only testing takes place during the match!  2 SLOs and SLIs will be discussed in more detail in Chapter 5.  3 To learn more on agile testing, check out the books Agile Testing (O\u2019Reilly), More Agile Testing (O\u2019Reilly), or the video series Agile Testing Essentials.  4 The typical example of a unit in object-oriented (OO) languages is a class.  5 Test doubles include stubs, which look like real implementations of an external entity, except they return hardcoded responses. Mocks look like real implementations of preprogrammed objects, but instead are used to verify behavior.  6 Kent Beck\u2019s book Test Driven Development: By Example (Addison-Wesley) is a fantastic resource for learning more about TDD.  7 To clarify, it is also possible that a service can be both a producer and consumer.  8 A stub server is a service that can be run locally and will return canned responses.  9 At the time of writing, there are a few projects that are available, though none are actively maintained, so it is difficult to recommend any.  10 Pact does a good job of comparing itself to other contract frameworks.  11 These testing libraries usually have a Domain Specific Language (DSL) and make it easy to analyze responses from the API. RestAssured is one\n\nQUESTIONS:", "completion": "Q: What is an example of a unit in object-oriented (OO) languages?\nA: The typical example of a unit in object-oriented (OO) languages is a class.\nWA1: A variable\nWA2: A method\nWA3: A loop\n\nQ: What type of test doubles are used to verify behavior?\nA: Mocks look like real implementations of preprogrammed objects, but instead are used to verify behavior.\nWA1: Stubs\nWA2: Drivers\nWA3: Monitors\n\nQ: What is Kent Becks book Test Driven Development: By Example (Addison-Wesley) about?\nA: Kent Becks book Test Driven Development: By Example (Addison-Wesley) is a fantastic resource for learning more about TDD.\nWA1: Agile Testing\nWA2: API Gateways\nWA3: Testing the Integrity of Products"}
{"prompt": "Context: such REST testing framework in Java, and the httptest package comes out of the box with Golang. Depending on the language or framework that you use, there should be something available; otherwise, creating a small wrapper around a standard client can make things considerably easier to integrate responses when writing tests.  12 Duplicate value for id and misspelled familyNane (sic).  13 Wiremock is a tool that can be used as a standalone service, making it language agnostic, although since it is written in Java there are some specific Java integrations that you can take advantage of. There are many other tools available that have a similar capability in other languages such as camouflage, which is written in TypeScript.      Part II. API Traffic Management   This section explores how API traffic is managed. This includes both traffic originating externally from end users that is entering (ingressing) into your system and traffic originating internally from services that is traveling across (service-to-service) your system.  In Chapter 3, where we recommend you begin your journey, you will explore using API gateway technology to manage ingress, or north\u2013south traffic.  In Chapter 4, you will learn about managing east\u2013west traffic using the service mesh pattern.      Chapter 3. API Gateways: Ingress Traffic Management   Now that you have a good understanding of defining and testing an API, we can turn our attention to platforms and tooling that are responsible for delivering\n\nQUESTIONS:", "completion": "Q: What is the httptest package?\nA: It comes out of the box with Golang. \nWA1: It is an API gateway technology. \nWA2: It is written in TypeScript. \nWA3: It is a standalone service. \n\nQ: What is the purpose of Chapter 3? \nA: It explores using API gateway technology to manage ingress, or northsouth traffic. \nWA1: It explains how to define and test an API. \nWA2: It provides an understanding of the service mesh pattern. \nWA3: It explains how to manage eastwest traffic."}
{"prompt": "Context: APIs to consumers in production. An API gateway is a critical part of any modern technology stack, sitting at the network \u201cedge\u201d of systems and acting as a management tool that mediates between a consumer and a collection of backend services.  In this chapter you will learn about the \u201cwhy,\u201d \u201cwhat,\u201d and \u201cwhere\u201d of API gateways and explore the history of the API gateway and other edge technologies. You will also explore the taxonomy of API gateways and learn how these fit into the bigger picture of system architecture and deployment models, all while avoiding common pitfalls.  Building on all of these topics, you will conclude the chapter by learning how to select an appropriate API gateway based on your requirements, constraints, and use cases.      Is an API Gateway the Only Solution?   We have frequently been asked, \u201cIs an API gateway the only solution to getting user traffic to backend systems?\u201d The short answer is no. But there is a bit more nuance here.  Many software systems need to route consumer API requests or ingress traffic from an external origin to an internal backend application. With web-based software systems, often the consumer\u2019s API requests originate from an end user interacting with a backend system via a web browser or mobile app. A consumer\u2019s requests may also originate from an external system (often third-party) making requests to an API via an application deployed elsewhere on the internet. In addition to providing a mechanism of routing traffic from a\n\nQUESTIONS:", "completion": "Q: What is an API gateway?\nA: An API gateway is a critical part of any modern technology stack, sitting at the network edge of systems and acting as a management tool that mediates between a consumer and a collection of backend services.\nWA1: An API gateway is a piece of software that allows users to access external systems from a web browser.\nWA2: An API gateway is a tool used to debug and test APIs.\nWA3: An API gateway is a type of authentication used to protect user data.\n\nQ: What topics are covered in this chapter?\nA: In this chapter you will learn about the why, what, and where of API gateways and explore the history of the API gateway and other edge technologies. You will also explore the taxonomy of API gateways and learn how these fit into the bigger picture of system architecture and deployment models, all while avoiding common pitfalls.\nWA1: In this chapter you will learn about design patterns, debugging techniques, and best practices for using APIs.\nWA2: In this chapter you will learn about the fundamentals of API development and how to create secure APIs.\nWA3: In this chapter you will learn about the basics of web development and how to deploy APIs."}
{"prompt": "Context: URL to a backend system, a solution that provides ingress will typically also be required to provide reliability, observability, and security.  As you will learn throughout this chapter, an API gateway isn\u2019t the only technology that can provide these requirements. For example, you can use a simple proxy or load balancer implementation. However, we believe it is the most commonly used solution, particularly within an enterprise context, and as the number of consumers and providers increases, it is often the most scalable, maintainable, and secure option.  As shown in Table 3-1, you will want to match your current requirements to the capabilities of each solution. Don\u2019t worry if you don\u2019t understand all of these requirements, as you will learn more about them throughout the chapter.  Table 3-1. Comparing reverse proxies, load balancers, and API gateways Feature Reverse proxy Load balancer API gateway  Single Backend * * *  TlS/SSL * * *  Multiple Backends * *  Service Discovery * *  API Composition *  Authorization *  Retry Logic *  Rate Limiting *  Logging and Tracing *  Circuit Breaking *      Guideline: Proxy, Load Balancer, or API Gateway   Table 3-2 provides a series of ADR Guidelines to help you decide the best ingress solution for your organization\u2019s system or current project.  Table 3-2. ADR Guideline: Proxy, load balancer, or API gateway Decision  Should you use a proxy, load balancer, or API gateways for routing ingress traffic?    Discussion Points  Do you want\n\nQUESTIONS:", "completion": "Q: What solution is typically required to provide ingress?\nA: An API gateway \nWA1: A proxy server\nWA2: A load balancer\nWA3: A firewall\n\nQ: What technology is the most commonly used solution for providing ingress and scalability?\nA: An API gateway \nWA1: A proxy server\nWA2: A load balancer\nWA3: A firewall\n\nQ: What solution should you use to provide TLS/SSL?\nA: A reverse proxy \nWA1: A load balancer\nWA2: An API gateway\nWA3: A firewall"}
{"prompt": "Context: simple routing, for example, from a single endpoint to a single backend service?  Do you have cross-functional requirements that will require more advanced features, such as authentication, authorization, or rate limiting?  Do you require API management functionality, such as API keys/tokens or monetization/chargeback?  Do you already have a solution in place, or is there an organization-wide mandate that all traffic must be routed through certain components at the edge of your network?    Recommendations  Always use the simplest solution for your requirements, with an eye to the immediate future and known requirements.  If you have advanced cross-functional requirements, an API gateway is typically the best choice.  If your organization is an enterprise, an API gateway that supports API Management (APIM) features is recommended.  Always perform due diligence within your organization for existing mandates, solutions, and components.      Case Study: Exposing the Attendee Service to Consumers   As the conference system has seen considerable uptake since its launch, the owners would like to enable conference attendees to be able to view their details via a new mobile application. This will require that the Attendee service API be exposed externally in order for the mobile app to query this data. As the Attendee service contains personally identifiable information (PII), this will mean that the API must be secure in addition to being reliable and observable. You could simply\n\nQUESTIONS:", "completion": "Q: What should be kept in mind when choosing a solution for routing?\nA: Always use the simplest solution for your requirements, with an eye to the immediate future and known requirements.\nWA1: Use the most complex solution available\nWA2: Use the most expensive solution\nWA3: Choose a solution that does not meet the immediate requirements\n\nQ: What is recommended for enterprises with advanced cross-functional requirements?\nA: An API gateway that supports API Management (APIM) features is recommended.\nWA1: A single endpoint to a single backend service\nWA2: API keys/tokens\nWA3: Monetization/chargeback\n\nQ: What is required to enable conference attendees to view their details via a mobile application?\nA: The Attendee service API must be exposed externally and must be secure in addition to being reliable and observable.\nWA1: Expose the mobile application externally\nWA2: Expose the Attendee service internally\nWA3: Make the Attendee service unreliable"}
{"prompt": "Context: expose the API using a proxy or load balancer and implement any additional requirements using language- or framework-specific features. However, you must ask yourself if this solution would scale, would it be reusable (potentially supporting additional APIs using different languages and frameworks), and have these challenges already been solved within existing technologies or products? In this case study, we know that additional APIs are planned to be exposed in the future, and that additional languages and frameworks may be used in their implementation. It therefore makes sense to implement an API gateway-based solution.  As this chapter develops, you will add an API gateway to the existing conference system case study to expose the Attendee API in a manner that meets all of these requirements listed. Figure 3-1 shows what the conference system architecture will look like with the addition of an API gateway.      Figure 3-1. Using an API gateway to route to the Attendee service running independently from the monolith      What Is an API Gateway?   In a nutshell, an API gateway is a management tool that sits at the edge of a system between a consumer and a collection of backend services and acts as a single point of entry for a defined group of APIs. The consumer can be an end-user application or device, such as a single page web application or a mobile app, or another internal system, or third-party application or system.  An API gateway is implemented with two high-level\n\nQUESTIONS:", "completion": "Q: What is an API gateway?\nA: An API gateway is a management tool that sits at the edge of a system between a consumer and a collection of backend services and acts as a single point of entry for a defined group of APIs.\nWA1: An API gateway is a management tool that sits in the middle of a system between a consumer and a collection of backend services and acts as a single point of entry for a defined group of APIs.\nWA2: An API gateway is a software application that sits at the edge of a system between a consumer and a collection of backend services and acts as a single point of entry for a defined group of APIs.\nWA3: An API gateway is a hardware device that sits at the edge of a system between a consumer and a collection of backend services and acts as a single point of entry for a defined group of APIs.\n\nQ: What is the purpose of an API gateway?\nA: The purpose of an API gateway is to act as a single point of entry for a defined group of APIs.\nWA1: The purpose of an API gateway is to act as a single point of entry for all APIs.\nWA2: The purpose of an API gateway is to act as a single point of entry for a defined group of users.\nWA3: The purpose of an API gateway is to act as a single point of exit for a defined group of APIs."}
{"prompt": "Context: fundamental components: a control plane and data plane. These components can typically be packaged together or deployed separately. The control plane is where operators interact with the gateway and define routes, policies, and required telemetry. The data plane is the location where all of the work specified in the control plane occurs, the network packets are routed, the policies enforced, and telemetry emitted.      What Functionality Does an API Gateway Provide?   At a network level an API gateway typically acts as a reverse proxy to accept all of the API requests from a consumer, calls and aggregates the various application-level backend services (and potentially external services) required to fulfill them, and returns the appropriate result.      What Is a Proxy, Forward Proxy, and Reverse Proxy?   A proxy server, sometimes referred to as a forward proxy, is an intermediary server that forwards requests for content from multiple clients to different servers across the internet. A forward proxy is used to protect clients. For instance, a business may have a proxy that routes and filters employee traffic to the public internet. A reverse proxy server, on the other hand, is a type of proxy server that typically sits behind the firewall in a private network and routes client requests to the appropriate backend server. A reverse proxy is designed to protect servers.      An API gateway provides cross-cutting requirements such as user authentication, request rate limiting,\n\nQUESTIONS:", "completion": "Q: What Functionality Does an API Gateway Provide?\nA: At a network level an API gateway typically acts as a reverse proxy to accept all of the API requests from a consumer, calls and aggregates the various application-level backend services (and potentially external services) required to fulfill them, and returns the appropriate result.\nWA1: A network gateway provides security and encryption services for all traffic.\nWA2: An API gateway provides a platform for developers to create applications.\nWA3: An API gateway is used to create a secure connection between two computers.\n\nQ: What Is a Proxy, Forward Proxy, and Reverse Proxy?\nA: A proxy server, sometimes referred to as a forward proxy, is an intermediary server that forwards requests for content from multiple clients to different servers across the internet. A forward proxy is used to protect clients. For instance, a business may have a proxy that routes and filters employee traffic to the public internet. A reverse proxy server, on the other hand, is a type of proxy server that typically sits behind the firewall in a private network and routes client requests to the appropriate backend server. A reverse proxy is designed to protect servers.\nWA1: A proxy is a type of virtual private network (VPN) that is used to access the internet.\nWA2: A forward proxy is used to route traffic from the server to the client.\nWA3: A reverse proxy is used to filter out malicious traffic."}
{"prompt": "Context: and timeouts/retries, and can provide metrics, logs, and trace data in order to support the implementation of observability within the system. Many API gateways provide additional features that enable developers to manage the lifecycle of an API, assist with the onboarding and management of developers using the APIs (such as providing a developer portal and related account administration and access control), and provide enterprise governance.      Where Is an API Gateway Deployed?   An API gateway is typically deployed at the edge of a system, but the definition of \u201csystem\u201d in this case can be quite flexible. For startups and many small-medium businesses (SMBs), an API gateway will often be deployed at the edge of the data center or the cloud. In these situations there may only be a single API gateway (deployed and running via multiple instances for high availability) that acts as the front door for the entire backend estate, and the API gateway will provide all of the edge functionality discussed in this chapter via this single component.  Figure 3-2 shows how clients interact with an API gateway and backend systems over the internet.      Figure 3-2. A typical startup/SMB API gateway deployment      For large organizations and enterprises, an API gateway will typically be deployed in multiple locations, often as part of the initial edge stack at the perimeter of a data center, and additional gateways may be deployed as part of each product, line of business, or\n\nQUESTIONS:", "completion": "Q: What are some additional features that API gateways provide?\nA: API gateways provide additional features that enable developers to manage the lifecycle of an API, assist with the onboarding and management of developers using the APIs (such as providing a developer portal and related account administration and access control), and provide enterprise governance.\nWA1: API gateways provide a way to manage user accounts.\nWA2: API gateways provide a way to store data.\nWA3: API gateways provide a way to debug errors.\n\nQ: Where is an API gateway typically deployed?\nA: An API gateway is typically deployed at the edge of a system.\nWA1: An API gateway is typically deployed in the center of the system.\nWA2: An API gateway is typically deployed in the cloud.\nWA3: An API gateway is typically deployed on the client's computer."}
{"prompt": "Context: organizational department. In this context these gateways would more typically be separate implementations and may offer differing functionality depending on geographical location (e.g., required governance) or infrastructure capabilities (e.g., running on low-powered edge compute resources).  Figure 3-3 shows how an API gateway often sits between the public internet and the demilitarized zone (DMZ) of a private network.      Figure 3-3. A typical large/enterprise API gateway deployment      As you will learn later in this chapter, the definition and exact functionality offered within an API gateway isn\u2019t always consistent across implementations, and so the preceding diagrams should be thought of as more conceptual rather than an exact implementation.      How Does an API Gateway Integrate with Other Technologies at the Edge?   There are typically many components deployed at the edge of an API-based system. This is where the consumers and users first interact with the backend, and hence many cross-cutting concerns are best addressed here. Therefore, a modern edge technology stack or \u201cedge stack\u201d provides a range of functionality that meets essential cross-functional requirements for API-based applications. In some edge stacks each piece of functionality is provided by a separately deployed and operated component, and in others the functionality and/or components are combined. You will learn more about the individual requirements in the next section of the chapter, but for the\n\nQUESTIONS:", "completion": "Q: What is the role of an API gateway in a typical large/enterprise API gateway deployment? \nA: An API gateway typically sits between the public internet and the demilitarized zone (DMZ) of a private network.\nWA1: An API gateway typically sits between two private networks.\nWA2: An API gateway typically handles API requests and responses.\nWA3: An API gateway typically provides authentication and authorization services.\n\nQ: How does an API gateway integrate with other technologies at the edge?\nA: There are typically many components deployed at the edge of an API-based system.\nWA1: An API gateway is typically the only component deployed at the edge.\nWA2: An API gateway typically provides all the necessary functionality for an API-based application.\nWA3: An API gateway typically provides encryption services."}
